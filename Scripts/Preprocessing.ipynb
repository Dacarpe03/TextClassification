{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRÁCTICA 2: Clasificador de noticias\n",
    "\n",
    "### Nombres:\n",
    "Introduce en esta celda los nombres de los dos integrantes del grupo:\n",
    "- *Alumno 1:* DANIEL CARMONA PEDRAJAS\n",
    "- *Alumno 2:* JOEL PARDO FERRERA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objetivo: Implementar un clasificador usando el conjunto de datos recopilado de varias fuentes de internet como:\n",
    "\n",
    "- Google News que toma noticias de varios repositorios dedicados a la información\n",
    "- Periódicos:\n",
    "    - El País\n",
    "    - ABC\n",
    "    - El Confidencial\n",
    "    - 20minutos\n",
    "    - El Diario\n",
    "\n",
    "Este repositorio incluye tanto las noticias en formato '.txt' donde se almacenan los cuerpos de noticia y sus correspondientes títulos, como un '.csv' donde se contiene un registro de todas las noticias donde se refleja el número de noticias, la clase a la que pertenece (deportes, salud, ciencia y politica), el número de noticia dentro de la clase correspondiente, el título de noticia, la ruta donde está almacenada esa noticia, y por último la URL de donde se ha sacado la noticia. \n",
    "\n",
    "La fechas tanto de publicación como de obtención e datos se ubican en Noviembre de 2022. \n",
    "\n",
    "La clase a predecir es el tipo de noticia (columna 'category' de la base de datos), a partir de los archivos '.txt'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. IMPORTACIÓN DE LIBRERÍAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# git config --global user.email \"jpardo0824@gmail.com\"\n",
    "# git config --global user.name \"JPardo08\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install pandas\n",
    "# !pip3 install spacy\n",
    "# !pip3 install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import unicodedata\n",
    "import os\n",
    "#from spellchecker import SpellChecker \n",
    "#from textblob import TextBlob \n",
    "#import contractions\n",
    "import re\n",
    "import random\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CARGA DE DATOS\n",
    "Cargamos los datos en dos formatos:\n",
    "* DataFrame de pandas\n",
    "* Generador\n",
    "\n",
    "Para cargar los datos utilizamos la librería pandas. \n",
    "La funcion implementada recibe la ruta del archivo .csv que queramos cargar y devuelve los textos en una variable generador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta = \"..\" ## CAMBIAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt(df):\n",
    "    \"\"\" Funcion para coger los documentos '.txt' de las noticias\n",
    "    \n",
    "    Path: Path de la carpeta donde se encuentras los documentos\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    paths = df[\"path\"].tolist()\n",
    "    clases = df[\"category\"].unique().tolist()\n",
    "\n",
    "    documentos = []\n",
    "\n",
    "    for i in paths:\n",
    "        \n",
    "        t = i.replace(\".\", ruta,1) \n",
    "        s = t.replace(\"/\", \"//\")\n",
    "        print(s)\n",
    "\n",
    "        with open(s, \"r\", encoding=\"latin-1\", errors='ignore') as f:\n",
    "            \n",
    "            lineas = f.readlines()\n",
    "            txt1 = ''.join(lineas)\n",
    "            documentos.append(txt1)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    return clases, documentos\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_df = ruta + \"/Datos/urls1.csv\"\n",
    "p_df2 = p_df.replace(\"/\", \"//\")\n",
    "print(p_df2)\n",
    "\n",
    "# df_gen = parse(r'/Users/joelpardo/Desktop/TextClassification/Datos/urls1.csv')\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(p_df2,',')\n",
    "\n",
    "#Visualizamos los datos\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clases, documentos = txt(df)\n",
    "\n",
    "print(len(clases))\n",
    "print(len(documentos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comprobamos los tipos\n",
    "print(type(df))\n",
    "# print(type(df_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documentos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"docs\"] = documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Miramos los nombres\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generamos un duplicado para seleccionar lo que queremos\n",
    "df2 = df.loc[:,['title', 'category','docs']]\n",
    "\n",
    "#Eliminamos los na\n",
    "df2=df2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comprobamos cuantas fila con na hemos eliminado\n",
    "print(df.shape)\n",
    "print(df2.shape)\n",
    "\n",
    "#Ninguna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comprobamos los valores de la columna category\n",
    "# clases = pd.unique(df2['category']).tolist()\n",
    "print(clases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a ver que clases tienen más peso\n",
    "frecuencias = df2.groupby('category').count()\n",
    "frecuencias = frecuencias.sort_values('title',ascending=False)\n",
    "\n",
    "#Visualizamos las clases con mas peso (todas)\n",
    "frecuencias.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, todas las clases tienen el mismo peso, es decir, tienen el mismo numero de noticas. Tenemos un conjunto de datos homogeneo para el cual hacer la prediccion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PRE-PROCESADO\n",
    "Realizaremos el siguiente procesado:\n",
    "- Separar el texto en *tokens*\n",
    "- Eliminar los *tokens* de tipo *stop-word*, signos de puntuación, signos especiales o espacios\n",
    "- Lematizar el texto\n",
    "- Introducimos un espacio después de determinados signos de puntuación (\".\", \"?\", \"%\") para que el tokenizado sea correcto\n",
    "- Filtramos los *tokens* con una longitud de 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download es_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spacy.io/models/es\n",
    "nlp = spacy.load(\"es_core_news_lg\") #Mejor modelo optimizado para la CPU\n",
    "\n",
    "#definimos función de normalizado\n",
    "def normaliza(texto):\n",
    "    texto = re.sub(r\"(\\.)|(\\?)|(\\%)\", r\"\\1 |\\2 |\\3 \", texto) #añadimos un espacio después de \".\" y \"?\"\n",
    "    doc = nlp(texto)\n",
    "    tokens = [t for t in doc if\n",
    "                        len(t.text)>1 and\n",
    "                       not t.is_stop and\n",
    "                       not t.is_space and\n",
    "                       not t.is_punct]#filtramos los tokens que nos interesan\n",
    "    palabras = []\n",
    "    for t in tokens:\n",
    "        palabras.append(t.lemma_) #añadimos lema\n",
    "    salida = ' '.join(palabras)#junta todos los tokens en un string\n",
    "    \n",
    "    return salida\n",
    "\n",
    "#funcion para quitar acentos y caracteres no ASCII:\n",
    "\n",
    "def remove_accents(input_str):     \n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)     \n",
    "    only_ascii = nfkd_form.encode('ASCII', 'ignore')     \n",
    "    return only_ascii.decode(\"utf-8\", 'ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comprobamos que funciona con el elemento 0 de noticia (\"noticia\")\n",
    "print(df2['docs'][0])\n",
    "print(\"-\"*140)\n",
    "normaliza(df2['docs'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comprobamos que funciona con el elemento 2 de noticia (\"docs\")\n",
    "print(df2['docs'][2])\n",
    "print(\"-\"*140)\n",
    "normaliza(df2['docs'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creacion de corpus y labels \n",
    "df2[\"corpus\"] = df2['docs'] + df2['title']\n",
    "corpus = df2[\"corpus\"].tolist()\n",
    "\n",
    "labels_posibles = pd.unique(df2['category']).tolist()\n",
    "labels = df2['category'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediccion_r=ruta+\"/Prediccion\"\n",
    "\n",
    "os.mkdir(prediccion_r)\n",
    "\n",
    "df2.to_csv(prediccion_r+'/training.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vemos cuantas observaciones en total tenemos.\n",
    "print(f'\\nTamaño  TOTAL: {len(corpus)}')\n",
    "print(f'Tamaño de clase TOTAL: {len(labels)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seleccionamos 5000 muestras del cojunto de datos total\n",
    "# corpus_sm, labels_sm = resample(corpus, labels, n_samples=5000, replace=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separacion del conjunto de datos P5\n",
    "train_corpus, test_corpus, train_labels, test_labels = train_test_split(corpus, labels, test_size = 0.30, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETAR\n",
    "print(f'\\nTamaño de TRAIN: {len(train_corpus)}')\n",
    "print(f'Tamaño de clase TRAIN: {len(train_labels)}')\n",
    "\n",
    "print(f'\\nTamaño de TEST: {len(test_corpus)}')\n",
    "print(f'Tamaño de clase TEST: {len(test_labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizamos los docss y los guardamos como una lista en lugar de generator porque \n",
    "#lo tenemos que múltiples veces y no queremos tener que normalizar todo el corpus cada vez.\n",
    "norm_train_corpus = list(map(normaliza, train_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_test_corpus = list(map(normaliza, test_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. EXTRACCION DE CARACTERISTICAS\n",
    "Instanciamos los vectorizadores para obtener las características BoW y TF-IDF.  \n",
    "Usamos el parámetro max_df=0.9 para eliminar los stop-words como las palabras que aparecen al menos en el 90% de los documentos y el parámetro min_df=0.01 para eliminar las palabras que no aparecen al menos en un 1% de los documentos.\\\n",
    "\n",
    "Usamos el modelo `TfidfTransformer` para calcular la matriz TF-IDF a partir del BoW y no tener que repetir todo el entrenamiento.\n",
    "\n",
    "Para calcular los modelos basados en WV usamos el modelo gloVe pre-entrenado en `spaCy`. Calculamos dos modelos basados en word-vectors:  \n",
    "* El vector promedio de los WV de todos los tokens con el mismo peso para todas las palabras.  \n",
    "* Ponderando el WV de cada palabra por el término de frecuencia inversa de documento (IDF).  \n",
    "\n",
    "Definimos las funciones para calcular estas dos matrices de características:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BoW\n",
    "bow_vectorizer = CountVectorizer(min_df=0.01, max_df=0.9)\n",
    "\n",
    "#Tf-idf\n",
    "tfidf_vectorizer = TfidfTransformer()\n",
    "\n",
    "#Funciones de WV.\n",
    "def averaged_word_vectorizer(corpus):\n",
    "    '''Aplica la función de cálculo del WE promedio a todos los\n",
    "    documentos del corpus (cada doc es una lista de tokens)'''\n",
    "    features = [nlp(doc).vector\n",
    "                    for doc in corpus]\n",
    "    return np.array(features)\n",
    "\n",
    "def tfidf_wtd_avg_word_vectors(doc, word_tfidf_map):\n",
    "    '''Aplica la función de cálculo del WE ponderado por TF-IDF\n",
    "    a un documento (como lista de tokens)'''\n",
    "    tokens = doc.split()\n",
    "\n",
    "    feature_vector = np.zeros((nlp.vocab.vectors_length,),dtype=\"float64\")\n",
    "    wts = 0.      \n",
    "    for word in tokens:\n",
    "        if nlp.vocab[word].has_vector and word_tfidf_map.get(word, 0): #sólo considera palabras conocidas\n",
    "            weighted_word_vector = word_tfidf_map[word] * nlp.vocab[word].vector\n",
    "            wts = wts + 1\n",
    "            feature_vector = np.add(feature_vector, weighted_word_vector)\n",
    "    if wts:\n",
    "        feature_vector = np.divide(feature_vector, wts)\n",
    "        \n",
    "    return feature_vector\n",
    "    \n",
    "def tfidf_weighted_averaged_word_vectorizer(corpus, word_tfidf_map):\n",
    "    '''Aplica la función de cálculo del WE ponderado por TF-IDF a todos los\n",
    "    documentos del corpus (cada doc es una lista de tokens)'''                                       \n",
    "    features = [tfidf_wtd_avg_word_vectors(doc, word_tfidf_map)\n",
    "                    for doc in corpus]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# características bag of words\n",
    "bow_train_features = bow_vectorizer.fit_transform(norm_train_corpus)  \n",
    "bow_test_features = bow_vectorizer.transform(norm_test_corpus) \n",
    "\n",
    "# características tfidf (a partir del BoW)\n",
    "tfidf_train_features = tfidf_vectorizer.fit_transform(bow_train_features)\n",
    "tfidf_test_features = tfidf_vectorizer.transform(bow_test_features)    \n",
    "\n",
    "# características averaged word vector\n",
    "avg_wv_train_features = averaged_word_vectorizer(norm_train_corpus)                \n",
    "avg_wv_test_features = averaged_word_vectorizer(norm_test_corpus)      \n",
    "\n",
    "# características tfidf weighted averaged word vector\n",
    "word_tfidf_map = {key:value for (key, value) in zip(bow_vectorizer.get_feature_names_out(), tfidf_vectorizer.idf_)}\n",
    "\n",
    "tfidf_wv_train_features = tfidf_weighted_averaged_word_vectorizer(norm_train_corpus, word_tfidf_map)\n",
    "\n",
    "tfidf_wv_test_features = tfidf_weighted_averaged_word_vectorizer(norm_test_corpus, word_tfidf_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_wv_train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_wv_train_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CLASIFICADOR\n",
    "Aplicamos distintos clasificadores a cada modelo para ver cuál funciona mejor con nuestros datos. Primero definimos unas funciones para entrenar y medir el rendimiento de los clasificadores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(true_labels, predicted_labels):\n",
    "    \"\"\"Calculamos distintas métricas sobre el\n",
    "    rendimiento del modelo. Devuelve un diccionario\n",
    "    con los parámetros medidos\"\"\"\n",
    "    \n",
    "    return {\n",
    "        'Accuracy': np.round(\n",
    "                        metrics.accuracy_score(true_labels, \n",
    "                                               predicted_labels),\n",
    "                        3),\n",
    "        'Precision': np.round(\n",
    "                        metrics.precision_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted',\n",
    "                                               zero_division=0),\n",
    "                        3),\n",
    "    'Recall': np.round(\n",
    "                        metrics.recall_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted',\n",
    "                                               zero_division=0),\n",
    "                        3),\n",
    "    'F1 Score': np.round(\n",
    "                        metrics.f1_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted',\n",
    "                                               zero_division=0),\n",
    "                        3)}\n",
    "                        \n",
    "\n",
    "def train_predict_evaluate_model(classifier, \n",
    "                                 train_features, train_labels, \n",
    "                                 test_features, test_labels):\n",
    "    \"\"\"Función que entrena un modelo de clasificación sobre\n",
    "    un conjunto de entrenamiento, lo aplica sobre un conjunto\n",
    "    de test y devuelve la predicción sobre el conjunto de test\n",
    "    y las métricas de rendimiento\"\"\"\n",
    "    # genera modelo    \n",
    "    classifier.fit(train_features, train_labels)\n",
    "    # predice usando el modelo sobre test\n",
    "    predictions = classifier.predict(test_features) \n",
    "    # evalúa rendimiento de la predicción   \n",
    "    metricas = get_metrics(true_labels=test_labels, \n",
    "                predicted_labels=predictions)\n",
    "    return predictions, metricas     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a entrenar sobre el conjunto de train y evaluamos en el conjunto de test. Guardamos métricas en una lista y resultados en otra para mostrar resumen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLR = LogisticRegression(solver='liblinear')\n",
    "modelNB = GaussianNB() #var_smoothing=1e-9\n",
    "modelSVM = SGDClassifier(loss='hinge', max_iter=1000)\n",
    "modelRBFSVM = SVC(gamma='scale', C=2)\n",
    "\n",
    "modelos = [('Logistic Regression', modelLR),\n",
    "           ('Naive Bayes', modelNB),\n",
    "           ('Linear SVM', modelSVM),\n",
    "           ('Gauss kernel SVM', modelRBFSVM)]\n",
    "\n",
    "metricas = []\n",
    "resultados = []\n",
    "\n",
    "# Modelos con características BoW\n",
    "bow_train_features_ = bow_train_features.toarray()\n",
    "bow_test_features_ = bow_test_features.toarray()\n",
    "for m, clf in modelos:\n",
    "    prediccion, metrica = train_predict_evaluate_model(classifier=clf,\n",
    "                                           train_features=bow_train_features_,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features_,\n",
    "                                           test_labels=test_labels)\n",
    "    metrica['modelo']=f'{m} BoW'\n",
    "    resultados.append(prediccion)\n",
    "    metricas.append(metrica)\n",
    "    \n",
    "# Modelos con características TF-IDF\n",
    "tfidf_train_features_ = tfidf_train_features.toarray()\n",
    "tfidf_test_features_ = tfidf_test_features.toarray()\n",
    "for m, clf in modelos:\n",
    "    prediccion, metrica = train_predict_evaluate_model(classifier=clf,\n",
    "                                           train_features=tfidf_train_features_,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features_,\n",
    "                                           test_labels=test_labels)\n",
    "    metrica['modelo']=f'{m} tfidf'\n",
    "    resultados.append(prediccion)\n",
    "    metricas.append(metrica)\n",
    "\n",
    "# Modelos con características averaged word vectors\n",
    "avg_wv_train_features_ = avg_wv_train_features\n",
    "avg_wv_test_features_ = avg_wv_test_features\n",
    "for m, clf in modelos:\n",
    "    prediccion, metrica = train_predict_evaluate_model(classifier=clf,\n",
    "                                           train_features=avg_wv_train_features_,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=avg_wv_test_features_,\n",
    "                                           test_labels=test_labels)\n",
    "    metrica['modelo']=f'{m} averaged'\n",
    "    resultados.append(prediccion)\n",
    "    metricas.append(metrica)\n",
    "\n",
    "# Modelos con características tfidf weighted averaged word vectors\n",
    "tfidf_wv_train_features_ = tfidf_wv_train_features\n",
    "tfidf_wv_test_features_ = tfidf_wv_test_features\n",
    "for m, clf in modelos:\n",
    "    prediccion, metrica = train_predict_evaluate_model(classifier=clf,\n",
    "                                           train_features=tfidf_wv_train_features_,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_wv_test_features_,\n",
    "                                           test_labels=test_labels)\n",
    "    metrica['modelo']=f'{m} tfidf wv'\n",
    "    resultados.append(prediccion)\n",
    "    metricas.append(metrica)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conviertimos la lista de métricas en un DataFrame para observar sus valores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricas = pd.DataFrame(metricas)\n",
    "metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricas.to_csv(prediccion_r+'/metricas.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordenamos las métricas por `accuracy` y muestra el mejor resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricas = metricas.sort_values('Accuracy',ascending=False)\n",
    "metricas.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mejoramos el `accuracy` a partir del juego de parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelos = metricas['modelo'].tolist()\n",
    "\n",
    "mejor = modelos[0]\n",
    "sep = mejor.split(' ')\n",
    "\n",
    "\n",
    "\n",
    "if sep[len(sep)-1]== \"wv\":\n",
    "    mo = ' '.join(sep[:len(sep)-2])\n",
    "    \n",
    "else:\n",
    "    mo = ' '.join(sep[:len(sep)-1])\n",
    "\n",
    "print(mo)\n",
    "    \n",
    "\n",
    "datos = sep[len(sep)-1]\n",
    "print(datos)\n",
    "\n",
    "metricas2 = []\n",
    "resultados = []\n",
    "\n",
    "\n",
    "\n",
    "# DATOS\n",
    "if datos == 'Bow':\n",
    "    train_features_ = bow_train_features.toarray()\n",
    "    test_features_ = bow_test_features.toarray()\n",
    "    \n",
    "if datos == 'tfidf':\n",
    "    train_features_ = tfidf_train_features.toarray()\n",
    "    test_features_ = tfidf_test_features.toarray()\n",
    "    \n",
    "if datos == 'averaged':\n",
    "    train_features_ = avg_wv_train_features\n",
    "    test_features_ = avg_wv_test_features\n",
    "    \n",
    "else: # tfidf wv\n",
    "    train_features_ = tfidf_wv_train_features\n",
    "    test_features_ = tfidf_wv_test_features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# MODELOS\n",
    "\n",
    "if mo == 'Logistic Regression':\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "    sol = [\"lbfgs\", \"liblinear\", \"newton-cg\", \"newton-cholesky\", \"sag\", \"saga\"]\n",
    "    \n",
    "    for i in sol:\n",
    "        modelLR = LogisticRegression(solver=i)\n",
    "        prediccion, metrica = train_predict_evaluate_model(classifier=modelLR,\n",
    "                                            train_features=train_features_,\n",
    "                                            train_labels=train_labels,\n",
    "                                            test_features=test_features_,\n",
    "                                            test_labels=test_labels)\n",
    "        metrica['modelo']=f'{i} - Logistic Regression ' + datos\n",
    "        resultados.append(prediccion)\n",
    "        metricas2.append(metrica)\n",
    "    \n",
    "if mo == 'Naive Bayes':\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html\n",
    "    var_smo = [1e-1,1e-2,1e-3,1e-4,1e-5,1e-6,1e-7,1e-8,1e-9,1e-10,1e-11,1e-12,1e-13,1e-14,1e-15]\n",
    "    \n",
    "    for i in var_smo:\n",
    "        modelNB = GaussianNB(var_smoothing=i)\n",
    "        prediccion, metrica = train_predict_evaluate_model(classifier=modelLR,\n",
    "                                            train_features=train_features_,\n",
    "                                            train_labels=train_labels,\n",
    "                                            test_features=test_features_,\n",
    "                                            test_labels=test_labels)\n",
    "        metrica['modelo']=f'{i} - Naive Bayes ' + datos\n",
    "        resultados.append(prediccion)\n",
    "        metricas2.append(metrica)\n",
    "    \n",
    "if mo == 'Linear SVM':\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "    loss = [\"hinge\", \"log_loss\", \"log\", \"modified_huber\", \"squared_hinge\", \"perceptron\", \"squared_error\", \"huber\", \"epsilon_insensitive\", \"squared_epsilon_insensitive\"]\n",
    "\n",
    "    for i in loss:\n",
    "        modelSVM = SGDClassifier(loss=i, max_iter=1000)\n",
    "        prediccion, metrica = train_predict_evaluate_model(classifier=modelLR,\n",
    "                                            train_features=train_features_,\n",
    "                                            train_labels=train_labels,\n",
    "                                            test_features=test_features_,\n",
    "                                            test_labels=test_labels)\n",
    "        metrica['modelo']=f'{i} - Linear SVM ' + datos\n",
    "        resultados.append(prediccion)\n",
    "        metricas2.append(metrica)\n",
    "\n",
    "    \n",
    "else: # Gauss kernel SVM \n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "    gamma = [\"scale\", \"auto\"]\n",
    "\n",
    "    for i in gamma:\n",
    "        modelRBFSVM = SVC(gamma=i, C=2)\n",
    "        prediccion, metrica = train_predict_evaluate_model(classifier=modelLR,\n",
    "                                            train_features=train_features_,\n",
    "                                            train_labels=train_labels,\n",
    "                                            test_features=test_features_,\n",
    "                                            test_labels=test_labels)\n",
    "        metrica['modelo']=f'{i} - Gauss kernel SVM ' + datos\n",
    "        resultados.append(prediccion)\n",
    "        metricas2.append(metrica)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conviertimos la lista de métricas en un DataFrame para observar sus valores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricas3 = pd.DataFrame(metricas2)\n",
    "metricas3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordenamos las métricas por `accuracy` y muestra el mejor resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricas3 = metricas3.sort_values('Accuracy',ascending=False)\n",
    "metricas3.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. PRODUCCIÓN [PENDIENTE DE CAMBIAR] \n",
    "Cogemos el modelo que mejor funciona y lo aplicamos de forma manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividimos el conjunto total de muestras (`corpus` y `labels`) en entrenamiento y test (30%) y entrenamos el mejor modelo obtenido, para ver si se mejoran los resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MEJOR MODELO\n",
    "train_corpus, test_corpus, train_labels, test_labels = train_test_split(corpus, labels, test_size = 0.30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizamos\n",
    "norm_train_corpus = list(map(normaliza, train_corpus))\n",
    "norm_test_corpus = list(map(normaliza, test_corpus))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Características averaged word vector\n",
    "avg_wv_train_features = averaged_word_vectorizer(norm_train_corpus)                \n",
    "avg_wv_test_features = averaged_word_vectorizer(norm_test_corpus)      \n",
    "avg_wv_train_features_ = avg_wv_train_features\n",
    "avg_wv_test_features_ = avg_wv_test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos con características\n",
    "modelRBFSVM = SVC(gamma='scale', C=2)\n",
    "prediccion, metrica = train_predict_evaluate_model(classifier=modelRBFSVM,\n",
    "                                           train_features=avg_wv_train_features_,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=avg_wv_test_features_,\n",
    "                                           test_labels=test_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Modelo Gauss kernel SVM averaged con gamma scale: ')\n",
    "for i in metrica:\n",
    "    print('- ',i, ': ', metrica[i])\n",
    "\n",
    "print('\\nPredicciones:')\n",
    "print(prediccion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.unique(prediccion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "a821d265b19b8e474c372894184ad502aefb1f7882947607cd0ea5f074b097d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
