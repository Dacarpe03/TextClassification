{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRÁCTICA 2: Clasificador de noticias\n",
    "\n",
    "### Nombres:\n",
    "Introduce en esta celda los nombres de los dos integrantes del grupo:\\\n",
    "*Alumno 1:* DANIEL CARMONA PEDRAJAS\n",
    "*Alumno 2:* JOEL PARDO FERRERA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objetivo: Implementar un clasificador usando el conjunto de datos recopilado de varias fuentes de internet como:\n",
    "\n",
    "- Google News que toma noticias de varios repositorios dedicados a la información\n",
    "- Periodicos:\n",
    "    - El pais\n",
    "    - ABC\n",
    "    - El confidencial\n",
    "    - 20minutos\n",
    "    - El diario\n",
    "\n",
    "Este repositorio incluye tanto las noticias en formato '.txt' donde se almacenan los cuerpos de noticia y sus correspondientes titulos, como un '.csv' donde se contiene un registro de todas las noticias donde se refleja el número de noticias, la clase a la que pertenece (deportes, salud, ciencia y politica), el número de noticia dentro de la clase correspondiente, el titulo de noticia, la ruta donde esta almacenada esa noticia, y por ultimo la URL correspondiente del repositorio de donde se ha sacado al noticia. \n",
    "\n",
    "La fechas fechas tanto de publicacion como de obtenciond e datos se ubican en Noviembre de 2022. \n",
    "\n",
    "La clase a predecir es el tipo de noticia (columna 'category' de la base de datos), a partir de los archivos '.txt'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. IMPORTACION DE LIBRERIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# git config --global user.email \"jpardo0824@gmail.com\"\n",
    "# git config --global user.name \"JPardo08\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from pandas) (2022.6)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: spacy in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (3.4.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy) (1.10.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy) (65.5.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy) (8.1.5)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (1.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from scikit-learn) (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from scikit-learn) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pandas\n",
    "!pip3 install spacy\n",
    "!pip3 install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import unicodedata\n",
    "import os\n",
    "#from spellchecker import SpellChecker \n",
    "#from textblob import TextBlob \n",
    "#import contractions\n",
    "import re\n",
    "import random\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CARGA DE DATOS\n",
    "Cargamos los datos en dos formatos:\n",
    "* DataFrame de pandas\n",
    "* Generador\n",
    "\n",
    "Para cargar los datos utilizamos la libreria Pandas. En la funcion impleentada, le pasamos unicmanete  la ruta del archivo csv que queramos cargar y se guarda los datos en una variable generador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta = \"/Users/joelpardo/Desktop/TextClassification\" ## CAMBIAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt(df):\n",
    "    \"\"\" Funcion para coger los documentos '.txt' de las noticias\n",
    "    \n",
    "    Path: Path de la carpeta donde se encuentras los documentos\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    paths = df[\"path\"].tolist()\n",
    "    clases = df[\"category\"].unique().tolist()\n",
    "\n",
    "    documentos = []\n",
    "\n",
    "    for i in paths:\n",
    "        \n",
    "        t = i.replace(\".\", ruta,1) \n",
    "        s = t.replace(\"/\", \"//\")\n",
    "        print(s)\n",
    "\n",
    "        with open(s, \"r\", encoding=\"utf-8\", errors='ignore') as f:\n",
    "            \n",
    "            lineas = f.readlines()\n",
    "            txt1 = ''.join(lineas)\n",
    "            documentos.append(txt1)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    return clases, documentos\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//Users//joelpardo//Desktop//TextClassification//Datos//urls1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yq/prfd22yn0k3gnlt89mfz3zb80000gn/T/ipykernel_2913/261014751.py:9: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  df = pd.read_csv(p_df2,',')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>category</th>\n",
       "      <th>n_doc</th>\n",
       "      <th>title</th>\n",
       "      <th>path</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>sports</td>\n",
       "      <td>1</td>\n",
       "      <td>Memphis y Dumfries certifican que el 'soccer' ...</td>\n",
       "      <td>./Datos/Raw_data/sports/1.txt</td>\n",
       "      <td>https://news.google.com/__i/rss/rd/articles/CB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>sports</td>\n",
       "      <td>2</td>\n",
       "      <td>Con el dinero no basta en el fútbol - AS</td>\n",
       "      <td>./Datos/Raw_data/sports/2.txt</td>\n",
       "      <td>https://news.google.com/__i/rss/rd/articles/CB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>sports</td>\n",
       "      <td>3</td>\n",
       "      <td>El fútbol es un cuento - La Voz de Galicia</td>\n",
       "      <td>./Datos/Raw_data/sports/3.txt</td>\n",
       "      <td>https://news.google.com/__i/rss/rd/articles/CB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>sports</td>\n",
       "      <td>4</td>\n",
       "      <td>Cavani y Giménez serán sancionados pero no con...</td>\n",
       "      <td>./Datos/Raw_data/sports/4.txt</td>\n",
       "      <td>https://news.google.com/__i/rss/rd/articles/CB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>sports</td>\n",
       "      <td>5</td>\n",
       "      <td>Suspendido por una tangana a puñetazos el Alme...</td>\n",
       "      <td>./Datos/Raw_data/sports/5.txt</td>\n",
       "      <td>https://news.google.com/__i/rss/rd/articles/CB...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index category  n_doc                                              title  \\\n",
       "0      0   sports      1  Memphis y Dumfries certifican que el 'soccer' ...   \n",
       "1      1   sports      2          Con el dinero no basta en el fútbol - AS    \n",
       "2      2   sports      3         El fútbol es un cuento - La Voz de Galicia   \n",
       "3      3   sports      4  Cavani y Giménez serán sancionados pero no con...   \n",
       "4      4   sports      5  Suspendido por una tangana a puñetazos el Alme...   \n",
       "\n",
       "                            path  \\\n",
       "0  ./Datos/Raw_data/sports/1.txt   \n",
       "1  ./Datos/Raw_data/sports/2.txt   \n",
       "2  ./Datos/Raw_data/sports/3.txt   \n",
       "3  ./Datos/Raw_data/sports/4.txt   \n",
       "4  ./Datos/Raw_data/sports/5.txt   \n",
       "\n",
       "                                                link  \n",
       "0  https://news.google.com/__i/rss/rd/articles/CB...  \n",
       "1  https://news.google.com/__i/rss/rd/articles/CB...  \n",
       "2  https://news.google.com/__i/rss/rd/articles/CB...  \n",
       "3  https://news.google.com/__i/rss/rd/articles/CB...  \n",
       "4  https://news.google.com/__i/rss/rd/articles/CB...  "
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_df = ruta + \"/Datos/urls1.csv\"\n",
    "p_df2 = p_df.replace(\"/\", \"//\")\n",
    "print(p_df2)\n",
    "\n",
    "# df_gen = parse(r'/Users/joelpardo/Desktop/TextClassification/Datos/urls1.csv')\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(p_df2,',')\n",
    "\n",
    "#Visualizamos los datos\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//1.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//2.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//3.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//4.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//5.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//6.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//7.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//8.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//9.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//10.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//11.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//12.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//13.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//14.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//15.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//16.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//17.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//18.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//19.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//20.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//21.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//22.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//23.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//24.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//25.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//26.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//27.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//28.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//29.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//30.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//31.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//32.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//33.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//34.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//35.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//36.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//37.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//38.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//39.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//40.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//41.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//42.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//43.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//44.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//45.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//46.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//47.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//48.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//49.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//50.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//51.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//52.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//53.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//54.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//55.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//56.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//57.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//58.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//59.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//60.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//1.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//2.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//3.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//4.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//5.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//6.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//7.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//8.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//9.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//10.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//11.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//12.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//13.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//14.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//15.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//16.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//17.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//18.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//19.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//20.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//21.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//22.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//23.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//24.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//25.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//26.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//27.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//28.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//29.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//30.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//31.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//32.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//33.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//34.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//35.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//36.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//37.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//38.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//39.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//40.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//41.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//42.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//43.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//44.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//45.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//46.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//47.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//48.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//49.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//50.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//51.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//52.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//53.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//54.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//55.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//56.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//57.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//58.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//59.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//60.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//1.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//2.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//3.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//4.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//5.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//6.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//7.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//8.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//9.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//10.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//11.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//12.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//13.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//14.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//15.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//16.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//17.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//18.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//19.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//20.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//21.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//22.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//23.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//24.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//25.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//26.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//27.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//28.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//29.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//30.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//31.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//32.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//33.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//34.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//35.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//36.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//37.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//38.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//39.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//40.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//41.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//42.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//43.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//44.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//45.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//46.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//47.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//48.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//49.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//50.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//51.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//52.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//53.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//54.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//55.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//56.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//57.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//58.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//59.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//60.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//1.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//2.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//3.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//4.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//5.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//6.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//7.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//8.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//9.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//10.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//11.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//12.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//13.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//14.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//15.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//16.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//17.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//18.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//19.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//20.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//21.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//22.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//23.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//24.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//25.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//26.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//27.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//28.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//29.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//30.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//31.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//32.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//33.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//34.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//35.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//36.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//37.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//38.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//39.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//40.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//41.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//42.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//43.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//44.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//45.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//46.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//47.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//48.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//49.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//50.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//51.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//52.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//53.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//54.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//55.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//56.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//57.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//58.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//59.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//60.txt\n",
      "4\n",
      "240\n"
     ]
    }
   ],
   "source": [
    "clases, documentos = txt(df)\n",
    "\n",
    "print(len(clases))\n",
    "print(len(documentos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "#Comprobamos los tipos\n",
    "print(type(df))\n",
    "# print(type(df_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"docs\"] = documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'category', 'n_doc', 'title', 'path', 'link', 'docs'], dtype='object')"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Miramos los nombres\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generamos un duplicado para seleccionar lo que queremos\n",
    "df2 = df.loc[:,['title', 'category','docs']]\n",
    "\n",
    "#Eliminamos los na\n",
    "df2=df2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 7)\n",
      "(240, 3)\n"
     ]
    }
   ],
   "source": [
    "#Comprobamos cuantas fila con na hemos eliminado\n",
    "print(df.shape)\n",
    "print(df2.shape)\n",
    "\n",
    "#Ninguna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sports', 'health', 'science', 'politics']\n"
     ]
    }
   ],
   "source": [
    "#Comprobamos los valores de la columna category\n",
    "# clases = pd.unique(df2['category']).tolist()\n",
    "print(clases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>docs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>health</th>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>politics</th>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>science</th>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sports</th>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          title  docs\n",
       "category             \n",
       "health       60    60\n",
       "politics     60    60\n",
       "science      60    60\n",
       "sports       60    60"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vamos a ver que clases tienen más peso\n",
    "frecuencias = df2.groupby('category').count()\n",
    "frecuencias = frecuencias.sort_values('title',ascending=False)\n",
    "\n",
    "#Visualizamos las clases con mas peso (todas)\n",
    "frecuencias.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, todas las clases tienen el mismo peso, es decir, tienen el mismo numero de noticas. Tenemos un conjunto de datos homogeneo para el cual hacer la prediccion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PRE-PROCESADO\n",
    "Realizaremos el siguiente procesado:\n",
    "- Separar el texto en *tokens*\n",
    "- Eliminar los *tokens* de tipo *stop-word*, signos de puntuación, signos especiales o espacios\n",
    "- Lematizar el texto\n",
    "- Introducimos un espacio después de determinados signos de puntuación (\".\", \"?\", \"%\") para que el tokenizado sea correcto\n",
    "- Filtramos los *tokens* con una longitud de 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting es-core-news-lg==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_lg-3.4.0/es_core_news_lg-3.4.0-py3-none-any.whl (568.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m568.0/568.0 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from es-core-news-lg==3.4.0) (3.4.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (1.10.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (3.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (2.0.7)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (8.1.5)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (1.0.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (65.5.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (1.23.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (6.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (1.0.9)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (2.28.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (4.64.1)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (0.7.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (3.0.10)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (21.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (2.4.5)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (3.1.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (1.26.13)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (0.0.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_lg')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download es_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spacy.io/models/es\n",
    "nlp = spacy.load(\"es_core_news_lg\") #Mejor modelo optimizado para la CPU\n",
    "\n",
    "#definimos función de normalizado\n",
    "def normaliza(texto):\n",
    "    texto = re.sub(r\"(\\.)|(\\?)|(\\%)\", r\"\\1 |\\2 |\\3 \", texto) #añadimos un espacio después de \".\" y \"?\"\n",
    "    doc = nlp(texto)\n",
    "    tokens = [t for t in doc if\n",
    "                        len(t.text)>1 and\n",
    "                       not t.is_stop and\n",
    "                       not t.is_space and\n",
    "                       not t.is_punct]#filtramos los tokens que nos interesan\n",
    "    palabras = []\n",
    "    for t in tokens:\n",
    "        palabras.append(t.lemma_) #añadimos lema\n",
    "    salida = ' '.join(palabras)#junta todos los tokens en un string\n",
    "    \n",
    "    return salida\n",
    "\n",
    "#funcion para quitar acentos y caracteres no ASCII:\n",
    "\n",
    "def remove_accents(input_str):     \n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)     \n",
    "    only_ascii = nfkd_form.encode('ASCII', 'ignore')     \n",
    "    return only_ascii.decode(\"utf-8\", 'ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El lateral neerlands regal dos asistencias, marc el 1-3... y el delantero azulgrana bail con la defensa estadounidense para clasificar a la Oranje a los cuartos de final del Mundial\n",
      "\"En Pases Bajos aprend mucho, tom muchos conceptos de ftbol\". Gregg Berhalter, seleccionador de Estados Unidos, asegur en la previa del partido que se inspir en el ftbol neerlands para fijar las bases de su ideario como entrenador. Unos conceptos futbolsticos que se dejaron ver en los primeros compases del partido, hasta que Memphis Depay decidi romper el espejismo impuesto por la caballera yankee.\n",
      "Averiguaron la forma en la que tenan que atacar a Pases Bajos para desarmar su entramado defensivo pero Pulisic, que no tuvo su da, no consigui mandar al fondo de la red ni una de las dos ocasiones que tuvo en la primera parte. Una de ellas tardar mucho tiempo en olvidarla. \n",
      "Instaurados en el bloque bajo, los hombres de Louis van Gaal dieron una autntica leccin de cmo un equipo tiene que atacar el espacio con sus contras. El '10' bajaba a recibir, 'liberaba' de sus funciones a los centrocampistas... e incluso era capaz de llegar al rea para generar todo el peligro del conjunto neerlands. Y en la primera que caz dentro del rea, Depay no perdon para certifican que el 'soccer' todava est lejos del modelo de ftbol de Pases Bajos.\n",
      "Dos cabalgadas de Dumfries fueron ms que suficientes para romper a toda la defensa de Estados Unidos: la primera la mand Depay al fondo de la red y en el tercer pase de la muerte del lateral en la primera parte, el segundo lo cort Adams con muchos problemas, apareci Blind para firmar el 2-0 justo antes del descanso. Dos acciones calcadas que ponan a la Oranje con pie y medio en los cuartos de final del Mundial de Qatar 2022.\n",
      "Memphis aprovechaba con su tanto para deshacer el empate con Klass-Jan Huntelaar y convertirse as en el segundo mximo goleador de la seleccin neerlandesa, solo superado por Robin Van Persie (50). \n",
      "Y cuando Pases Bajos disfrutaba de la segunda parte, apareci Wright para ponerle un poco de emocin al partido con una accin de ilusionista. El delantero estadounidense se invent un escorzo de pelcula para mandar el baln al fondo de la red con un taconazo brillante. El 'Capitn Amrica' colgaba el baln en el rea pequea y el '19' tiraba de imaginacin para firmar el 1-2. \n",
      "Sin embargo, la rebelin en la que Estados Unidos pareca buscar la independencia dur tan slo 5 minutos. Dumfries, MVP del partido, apareci primero en su rea sacando un disparo de Wright  en la lnea de gol y sorprenda en la jugada siguiente para finiquitar el partido con el 3-1. Baln colgado de Blind al segundo palo y el lateral, completamente slo, empalaba a la perfeccin el esfrico para cerrar la clasificacin de la la Oranje a los cuartos de final del Mundial. El show termin siendo naranja. \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'lateral neerlands regal asistencia marc 1-3 delantero azulgrana bail defensa estadounidense clasificar Oranje cuarto Mundial Pases Bajos aprend tom concepto ftbol Gregg Berhalter seleccionador Unidos asegur previa partido inspir ftbol neerlands fijar base ideario entrenador concepto futbolstico dejar compás partido Memphis Depay decidi romper espejismo impuesto caballera yankee averiguar forma tenar atacar Pases Bajos desarmar entramado defensivo Pulisic consigui mandar fondo red ocasión tardar tiempo olvidar él Instaurados bloque hombre Louis Gaal autntica leccin cmo equipo atacar espacio contra 10 bajar recibir liberar función centrocampista capaz llegar rea generar peligro conjunto neerlands caz rea Depay perdon certificar soccer todava est lejos modelo ftbol Pases Bajos cabalgada Dumfries ms suficiente romper defensa Unidos mand Depay fondo red tercer pase muerte lateral cort Adams problema apareci Blind firmar 2-0 justo descanso acción calcada ponar Oranje pie cuarto Mundial Qatar 2022 Memphis aprovechar deshacer empate Klass-Jan Huntelaar convertir él os mximo goleador seleccin neerlandés superado Robin Persie 50 Pases Bajos disfrutar apareci Wright poner él emocin partido accin ilusionista delantero estadounidense invent escorzo pelcula mandar baln fondo red taconazo brillante Capitn Amrica colgar baln rea pequea 19 tirar imaginacin firmar 1-2 rebelin Unidos pareca buscar independencia dur slo minuto Dumfries MVP partido aparecer rea sacar disparo Wright lnea gol sorprenda jugada finiquitar partido 3-1 Baln colgado Blind palo lateral completamente slo empalar perfeccin esfrico cerrar clasificacin Oranje cuarto Mundial show termin naranja'"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Comprobamos que funciona con el elemento 0 de noticia (\"noticia\")\n",
    "print(df['docs'][0])\n",
    "print(\"-\"*140)\n",
    "normaliza(df['docs'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El ftbol es un cuento. Ya lo dijo ese Shakespeare futbolstico que es Gary Lineker, buen goleador y estrella de la tele: El ftbol es un juego muy simple. 22 hombres persiguen un baln durante 90 minutos y al final siempre ganan los alemanes. La frase ha envejecido mal. Las mujeres tambin corren detrs de la pelota y arbitran (aunque sea a cuentagotas). Los alemanes ya no le ganan ni a Japn (nosotros tampoco). Y ese juego simple puede ser un tostn. Ms que el ftbol, nos gusta el negocio del ftbol, esa fbrica de millonarios, algunos por lo civil y otros por lo criminal, como dira Luis Aragons. Nos gustan su cultura, sus batallas, el cuento del ftbol.\n",
      "No est tan mal un Mundial en Navidad, al margen de la tropela de la FIFA y Catar. Con las mil y una noches persas y las luces del desierto y de las calles de Vigo. Las jvenes iranes jugando a tirarles el turbante a los ayatols. Los vecinos de la calle Prncipe contando ovejas viendo pasar el carricoche de la montaa rusa por la ventana.\n",
      "Infantino es Gargamel. Cristiano es la madrastra que se mira en el espejo. Luis Enrique es el principito arrogante. Blatter es el Doctor Claw, el malo sin rostro del inspector Gadget que acariciaba su gatito. Messi es Messi. Luis Surez llora por el otro paisito gallego. Y el seleccionador de Camern se ha ido a casa dejando para la posteridad la teora del peligro: Cuando sabes que ests en peligro, en realidad ya no ests en peligro. Cuando no sabes que ests en peligro, es cuando realmente ests en peligro. La Navidad este ao no la ha inaugurado el Black Friday de Caballero, sino el Catar-Ecuador de Doha. Mucho cuento.\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ftbol cuento Shakespeare futbolstico Gary Lineker goleador estrella tele ftbol juego simple 22 hombre perseguir baln 90 minuto ganar alemán frase envejecer mujer tambin correr detrs pelota arbitrar cuentagotas alemán ganar Japn juego simple tostn Ms ftbol gustar negocio ftbol fbrica millonario civil criminal dira Luis Aragons gustar cultura batalla cuento ftbol est Mundial Navidad margen tropela FIFA Catar mil noche persa luz desierto calle Vigo jvenes iranes jugar tirar él turbante ayatols vecino calle Prncipe contar oveja ver pasar carricoche montaa ruso ventana Infantino Gargamel Cristiano madrastra mirar espejo Luis Enrique principito arrogante Blatter Doctor Claw malo rostro inspector Gadget acariciar gatito Messi Messi Luis Surez llorar paisito gallego seleccionador Camern ir casa dejar posteridad teora peligro ests peligro realidad ests peligro ests peligro realmente ests peligro Navidad ao inaugurar Black Friday Caballero Catar-Ecuador Doha cuento'"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Comprobamos que funciona con el elemento 2 de noticia (\"docs\")\n",
    "print(df['docs'][2])\n",
    "print(\"-\"*140)\n",
    "normaliza(df['docs'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creacion de corpus y labels \n",
    "corpus = df['docs'].tolist() + df['title'].tolist()\n",
    "labels_posibles = pd.unique(df2['category']).tolist()\n",
    "labels = df['category'].tolist() + df['category'].tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tamaño  TOTAL: 480\n",
      "Tamaño de clase TOTAL: 480\n"
     ]
    }
   ],
   "source": [
    "#Vemos cuantas observaciones en total tenemos.\n",
    "print(f'\\nTamaño  TOTAL: {len(corpus)}')\n",
    "print(f'Tamaño de clase TOTAL: {len(labels)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seleccionamos 5000 muestras del cojunto de datos total\n",
    "# corpus_sm, labels_sm = resample(corpus, labels, n_samples=5000, replace=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separacion del conjunto de datos P5\n",
    "train_corpus, test_corpus, train_labels, test_labels = train_test_split(corpus, labels, test_size = 0.30, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tamaño de TRAIN: 336\n",
      "Tamaño de clase TRAIN: 336\n",
      "\n",
      "Tamaño de TEST: 144\n",
      "Tamaño de clase TEST: 144\n"
     ]
    }
   ],
   "source": [
    "# COMPLETAR\n",
    "print(f'\\nTamaño de TRAIN: {len(train_corpus)}')\n",
    "print(f'Tamaño de clase TRAIN: {len(train_labels)}')\n",
    "\n",
    "print(f'\\nTamaño de TEST: {len(test_corpus)}')\n",
    "print(f'Tamaño de clase TEST: {len(test_labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizamos los docss y los guardamos como una lista en lugar de generator porque \n",
    "#lo tenemos que múltiples veces y no queremos tener que normalizar todo el corpus cada vez.\n",
    "norm_train_corpus = list(map(normaliza, train_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_test_corpus = list(map(normaliza, test_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. EXTRACCION DE CARACTERISTICAS\n",
    "Instanciamos los vectorizadores para obtener las características BoW y TF-IDF.  \n",
    "Usamos el parámetro max_df=0.9 para eliminar los stop-words como las palabras que aparecen al menos en el 90% de los documentos y el parámetro min_df=0.01 para eliminar las palabras que no aparecen al menos en un 1% de los documentos.\\\n",
    "\n",
    "Usamos el modelo `TfidfTransformer` para calcular la matriz TF-IDF a partir del BoW y no tener que repetir todo el entrenamiento.\n",
    "\n",
    "Para calcular los modelos basados en WV usamos el modelo gloVe pre-entrenado en `spaCy`. Calculamos dos modelos basados en word-vectors:  \n",
    "* El vector promedio de los WV de todos los tokens con el mismo peso para todas las palabras.  \n",
    "* Ponderando el WV de cada palabra por el término de frecuencia inversa de documento (IDF).  \n",
    "\n",
    "Definimos las funciones para calcular estas dos matrices de características:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BoW\n",
    "bow_vectorizer = CountVectorizer(min_df=0.01, max_df=0.9)\n",
    "\n",
    "#Tf-idf\n",
    "tfidf_vectorizer = TfidfTransformer()\n",
    "\n",
    "#Funciones de WV.\n",
    "def averaged_word_vectorizer(corpus):\n",
    "    '''Aplica la función de cálculo del WE promedio a todos los\n",
    "    documentos del corpus (cada doc es una lista de tokens)'''\n",
    "    features = [nlp(doc).vector\n",
    "                    for doc in corpus]\n",
    "    return np.array(features)\n",
    "\n",
    "def tfidf_wtd_avg_word_vectors(doc, word_tfidf_map):\n",
    "    '''Aplica la función de cálculo del WE ponderado por TF-IDF\n",
    "    a un documento (como lista de tokens)'''\n",
    "    tokens = doc.split()\n",
    "\n",
    "    feature_vector = np.zeros((nlp.vocab.vectors_length,),dtype=\"float64\")\n",
    "    wts = 0.      \n",
    "    for word in tokens:\n",
    "        if nlp.vocab[word].has_vector and word_tfidf_map.get(word, 0): #sólo considera palabras conocidas\n",
    "            weighted_word_vector = word_tfidf_map[word] * nlp.vocab[word].vector\n",
    "            wts = wts + 1\n",
    "            feature_vector = np.add(feature_vector, weighted_word_vector)\n",
    "    if wts:\n",
    "        feature_vector = np.divide(feature_vector, wts)\n",
    "        \n",
    "    return feature_vector\n",
    "    \n",
    "def tfidf_weighted_averaged_word_vectorizer(corpus, word_tfidf_map):\n",
    "    '''Aplica la función de cálculo del WE ponderado por TF-IDF a todos los\n",
    "    documentos del corpus (cada doc es una lista de tokens)'''                                       \n",
    "    features = [tfidf_wtd_avg_word_vectors(doc, word_tfidf_map)\n",
    "                    for doc in corpus]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# características bag of words\n",
    "bow_train_features = bow_vectorizer.fit_transform(norm_train_corpus)  \n",
    "bow_test_features = bow_vectorizer.transform(norm_test_corpus) \n",
    "\n",
    "# características tfidf (a partir del BoW)\n",
    "tfidf_train_features = tfidf_vectorizer.fit_transform(bow_train_features)\n",
    "tfidf_test_features = tfidf_vectorizer.transform(bow_test_features)    \n",
    "\n",
    "# características averaged word vector\n",
    "avg_wv_train_features = averaged_word_vectorizer(norm_train_corpus)                \n",
    "avg_wv_test_features = averaged_word_vectorizer(norm_test_corpus)      \n",
    "\n",
    "# características tfidf weighted averaged word vector\n",
    "word_tfidf_map = {key:value for (key, value) in zip(bow_vectorizer.get_feature_names_out(), tfidf_vectorizer.idf_)}\n",
    "\n",
    "tfidf_wv_train_features = tfidf_weighted_averaged_word_vectorizer(norm_train_corpus, word_tfidf_map)\n",
    "\n",
    "tfidf_wv_test_features = tfidf_weighted_averaged_word_vectorizer(norm_test_corpus, word_tfidf_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(336, 3142)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(336, 3142)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(336, 300)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_wv_train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(336, 300)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_wv_train_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CLASIFICADOR\n",
    "Aplicamos distintos clasificadores a cada modelo para ver cuál funciona mejor con nuestros datos. Primero definimos unas funciones para entrenar y medir el rendimiento de los clasificadores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(true_labels, predicted_labels):\n",
    "    \"\"\"Calculamos distintas métricas sobre el\n",
    "    rendimiento del modelo. Devuelve un diccionario\n",
    "    con los parámetros medidos\"\"\"\n",
    "    \n",
    "    return {\n",
    "        'Accuracy': np.round(\n",
    "                        metrics.accuracy_score(true_labels, \n",
    "                                               predicted_labels),\n",
    "                        3),\n",
    "        'Precision': np.round(\n",
    "                        metrics.precision_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted',\n",
    "                                               zero_division=0),\n",
    "                        3),\n",
    "    'Recall': np.round(\n",
    "                        metrics.recall_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted',\n",
    "                                               zero_division=0),\n",
    "                        3),\n",
    "    'F1 Score': np.round(\n",
    "                        metrics.f1_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted',\n",
    "                                               zero_division=0),\n",
    "                        3)}\n",
    "                        \n",
    "\n",
    "def train_predict_evaluate_model(classifier, \n",
    "                                 train_features, train_labels, \n",
    "                                 test_features, test_labels):\n",
    "    \"\"\"Función que entrena un modelo de clasificación sobre\n",
    "    un conjunto de entrenamiento, lo aplica sobre un conjunto\n",
    "    de test y devuelve la predicción sobre el conjunto de test\n",
    "    y las métricas de rendimiento\"\"\"\n",
    "    # genera modelo    \n",
    "    classifier.fit(train_features, train_labels)\n",
    "    # predice usando el modelo sobre test\n",
    "    predictions = classifier.predict(test_features) \n",
    "    # evalúa rendimiento de la predicción   \n",
    "    metricas = get_metrics(true_labels=test_labels, \n",
    "                predicted_labels=predictions)\n",
    "    return predictions, metricas     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a entrenar sobre el conjunto de train y evaluamos en el conjunto de test. Guardamos métricas en una lista y resultados en otra para mostrar resumen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLR = LogisticRegression(solver='liblinear')\n",
    "modelNB = GaussianNB()\n",
    "modelSVM = SGDClassifier(loss='hinge', max_iter=1000)\n",
    "modelRBFSVM = SVC(gamma='scale', C=2)\n",
    "\n",
    "modelos = [('Logistic Regression', modelLR),\n",
    "           ('Naive Bayes', modelNB),\n",
    "           ('Linear SVM', modelSVM),\n",
    "           ('Gauss kernel SVM', modelRBFSVM)]\n",
    "\n",
    "metricas = []\n",
    "resultados = []\n",
    "\n",
    "# Modelos con características BoW\n",
    "bow_train_features_ = bow_train_features.toarray()\n",
    "bow_test_features_ = bow_test_features.toarray()\n",
    "for m, clf in modelos:\n",
    "    prediccion, metrica = train_predict_evaluate_model(classifier=clf,\n",
    "                                           train_features=bow_train_features_,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features_,\n",
    "                                           test_labels=test_labels)\n",
    "    metrica['modelo']=f'{m} BoW'\n",
    "    resultados.append(prediccion)\n",
    "    metricas.append(metrica)\n",
    "    \n",
    "# Modelos con características TF-IDF\n",
    "tfidf_train_features_ = tfidf_train_features.toarray()\n",
    "tfidf_test_features_ = tfidf_test_features.toarray()\n",
    "for m, clf in modelos:\n",
    "    prediccion, metrica = train_predict_evaluate_model(classifier=clf,\n",
    "                                           train_features=tfidf_train_features_,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features_,\n",
    "                                           test_labels=test_labels)\n",
    "    metrica['modelo']=f'{m} tfidf'\n",
    "    resultados.append(prediccion)\n",
    "    metricas.append(metrica)\n",
    "\n",
    "# Modelos con características averaged word vectors\n",
    "avg_wv_train_features_ = avg_wv_train_features\n",
    "avg_wv_test_features_ = avg_wv_test_features\n",
    "for m, clf in modelos:\n",
    "    prediccion, metrica = train_predict_evaluate_model(classifier=clf,\n",
    "                                           train_features=avg_wv_train_features_,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=avg_wv_test_features_,\n",
    "                                           test_labels=test_labels)\n",
    "    metrica['modelo']=f'{m} averaged'\n",
    "    resultados.append(prediccion)\n",
    "    metricas.append(metrica)\n",
    "\n",
    "# Modelos con características tfidf weighted averaged word vectors\n",
    "tfidf_wv_train_features_ = tfidf_wv_train_features\n",
    "tfidf_wv_test_features_ = tfidf_wv_test_features\n",
    "for m, clf in modelos:\n",
    "    prediccion, metrica = train_predict_evaluate_model(classifier=clf,\n",
    "                                           train_features=tfidf_wv_train_features_,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_wv_test_features_,\n",
    "                                           test_labels=test_labels)\n",
    "    metrica['modelo']=f'{m} tfidf wv'\n",
    "    resultados.append(prediccion)\n",
    "    metricas.append(metrica)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conviertimos la lista de métricas en un DataFrame para observar sus valores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>modelo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.097</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.097</td>\n",
       "      <td>Logistic Regression BoW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.639</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.639</td>\n",
       "      <td>0.682</td>\n",
       "      <td>Naive Bayes BoW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.153</td>\n",
       "      <td>0.883</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.113</td>\n",
       "      <td>Linear SVM BoW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>Gauss kernel SVM BoW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.222</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.304</td>\n",
       "      <td>Logistic Regression tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.861</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.899</td>\n",
       "      <td>Naive Bayes tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.299</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.343</td>\n",
       "      <td>Linear SVM tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.285</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.332</td>\n",
       "      <td>Gauss kernel SVM tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.812</td>\n",
       "      <td>0.863</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.826</td>\n",
       "      <td>Logistic Regression averaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.153</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.132</td>\n",
       "      <td>Naive Bayes averaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.771</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.792</td>\n",
       "      <td>Linear SVM averaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.424</td>\n",
       "      <td>0.851</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.465</td>\n",
       "      <td>Gauss kernel SVM averaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.667</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.729</td>\n",
       "      <td>Logistic Regression tfidf wv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.076</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.078</td>\n",
       "      <td>Naive Bayes tfidf wv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.646</td>\n",
       "      <td>0.839</td>\n",
       "      <td>0.646</td>\n",
       "      <td>0.715</td>\n",
       "      <td>Linear SVM tfidf wv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.236</td>\n",
       "      <td>0.914</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.315</td>\n",
       "      <td>Gauss kernel SVM tfidf wv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Accuracy  Precision  Recall  F1 Score                        modelo\n",
       "0      0.097      0.097   0.097     0.097       Logistic Regression BoW\n",
       "1      0.639      0.842   0.639     0.682               Naive Bayes BoW\n",
       "2      0.153      0.883   0.153     0.113                Linear SVM BoW\n",
       "3      0.000      0.000   0.000     0.000          Gauss kernel SVM BoW\n",
       "4      0.222      0.955   0.222     0.304     Logistic Regression tfidf\n",
       "5      0.861      0.951   0.861     0.899             Naive Bayes tfidf\n",
       "6      0.299      0.905   0.299     0.343              Linear SVM tfidf\n",
       "7      0.285      0.929   0.285     0.332        Gauss kernel SVM tfidf\n",
       "8      0.812      0.863   0.812     0.826  Logistic Regression averaged\n",
       "9      0.153      0.475   0.153     0.132          Naive Bayes averaged\n",
       "10     0.771      0.855   0.771     0.792           Linear SVM averaged\n",
       "11     0.424      0.851   0.424     0.465     Gauss kernel SVM averaged\n",
       "12     0.667      0.823   0.667     0.729  Logistic Regression tfidf wv\n",
       "13     0.076      0.080   0.076     0.078          Naive Bayes tfidf wv\n",
       "14     0.646      0.839   0.646     0.715           Linear SVM tfidf wv\n",
       "15     0.236      0.914   0.236     0.315     Gauss kernel SVM tfidf wv"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metricas = pd.DataFrame(metricas)\n",
    "metricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordenamos las métricas por `accuracy` y muestra el mejor resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>modelo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.861</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.899</td>\n",
       "      <td>Naive Bayes tfidf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Precision  Recall  F1 Score             modelo\n",
       "5     0.861      0.951   0.861     0.899  Naive Bayes tfidf"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metricas = metricas.sort_values('Accuracy',ascending=False)\n",
    "metricas.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mejoramos el `accuracy` a partir del juego de parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "#Funcion de gamma\n",
    "gamma = ['scale', 'auto']\n",
    "\n",
    "metricas = []\n",
    "resultados = []\n",
    "\n",
    "\n",
    "for i in gamma:\n",
    "    modelRBFSVM = SVC(gamma=i, C=2)\n",
    "    prediccion, metrica = train_predict_evaluate_model(classifier=modelRBFSVM,\n",
    "                                           train_features=avg_wv_train_features_,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=avg_wv_test_features_,\n",
    "                                           test_labels=test_labels)\n",
    "    metrica['modelo']=f'{i} – Gauss kernel SVM averaged'\n",
    "    resultados.append(prediccion)\n",
    "    metricas.append(metrica)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conviertimos la lista de métricas en un DataFrame para observar sus valores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>modelo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.424</td>\n",
       "      <td>0.851</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.465</td>\n",
       "      <td>scale – Gauss kernel SVM averaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.549</td>\n",
       "      <td>0.869</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.588</td>\n",
       "      <td>auto – Gauss kernel SVM averaged</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Precision  Recall  F1 Score                             modelo\n",
       "0     0.424      0.851   0.424     0.465  scale – Gauss kernel SVM averaged\n",
       "1     0.549      0.869   0.549     0.588   auto – Gauss kernel SVM averaged"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metricas = pd.DataFrame(metricas)\n",
    "metricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordenamos las métricas por `accuracy` y muestra el mejor resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>modelo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.549</td>\n",
       "      <td>0.869</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.588</td>\n",
       "      <td>auto – Gauss kernel SVM averaged</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Precision  Recall  F1 Score                            modelo\n",
       "1     0.549      0.869   0.549     0.588  auto – Gauss kernel SVM averaged"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metricas = metricas.sort_values('Accuracy',ascending=False)\n",
    "metricas.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividimos el conjunto total de muestras (`corpus` y `labels`) en entrenamiento y test (30%) y entrenamos el mejor modelo obtenido, para ver si se mejoran los resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MEJOR MODELO\n",
    "train_corpus, test_corpus, train_labels, test_labels = train_test_split(corpus, labels, test_size = 0.30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizamos\n",
    "norm_train_corpus = list(map(normaliza, train_corpus))\n",
    "norm_test_corpus = list(map(normaliza, test_corpus))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Características averaged word vector\n",
    "avg_wv_train_features = averaged_word_vectorizer(norm_train_corpus)                \n",
    "avg_wv_test_features = averaged_word_vectorizer(norm_test_corpus)      \n",
    "avg_wv_train_features_ = avg_wv_train_features\n",
    "avg_wv_test_features_ = avg_wv_test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos con características\n",
    "modelRBFSVM = SVC(gamma='scale', C=2)\n",
    "prediccion, metrica = train_predict_evaluate_model(classifier=modelRBFSVM,\n",
    "                                           train_features=avg_wv_train_features_,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=avg_wv_test_features_,\n",
    "                                           test_labels=test_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo Gauss kernel SVM averaged con gamma scale: \n",
      "-  Accuracy :  0.896\n",
      "-  Precision :  0.896\n",
      "-  Recall :  0.896\n",
      "-  F1 Score :  0.896\n",
      "\n",
      "Predicciones:\n",
      "['sports' 'politics' 'health' 'science' 'science' 'sports' 'science'\n",
      " 'politics' 'sports' 'science' 'science' 'politics' 'science' 'science'\n",
      " 'health' 'health' 'science' 'politics' 'science' 'sports' 'health'\n",
      " 'science' 'health' 'sports' 'sports' 'health' 'sports' 'health' 'sports'\n",
      " 'science' 'sports' 'politics' 'sports' 'health' 'health' 'health'\n",
      " 'politics' 'science' 'science' 'politics' 'health' 'health' 'politics'\n",
      " 'sports' 'health' 'politics' 'sports' 'sports' 'sports' 'health'\n",
      " 'science' 'politics' 'politics' 'sports' 'science' 'science' 'politics'\n",
      " 'sports' 'politics' 'science' 'sports' 'sports' 'science' 'health'\n",
      " 'health' 'sports' 'health' 'health' 'science' 'health' 'health' 'science'\n",
      " 'health' 'health' 'science' 'politics' 'sports' 'politics' 'science'\n",
      " 'politics' 'politics' 'health' 'politics' 'health' 'science' 'sports'\n",
      " 'sports' 'health' 'science' 'health' 'politics' 'sports' 'politics'\n",
      " 'science' 'politics' 'health' 'politics' 'politics' 'politics' 'health'\n",
      " 'health' 'science' 'sports' 'science' 'sports' 'politics' 'health'\n",
      " 'science' 'politics' 'science' 'science' 'politics' 'politics' 'sports'\n",
      " 'sports' 'science' 'politics' 'health' 'science' 'politics' 'politics'\n",
      " 'politics' 'sports' 'health' 'health' 'sports' 'health' 'politics'\n",
      " 'health' 'health' 'sports' 'science' 'sports' 'sports' 'sports' 'sports'\n",
      " 'science' 'health' 'sports' 'health' 'politics' 'politics' 'politics'\n",
      " 'health']\n"
     ]
    }
   ],
   "source": [
    "print('Modelo Gauss kernel SVM averaged con gamma scale: ')\n",
    "for i in metrica:\n",
    "    print('- ',i, ': ', metrica[i])\n",
    "\n",
    "print('\\nPredicciones:')\n",
    "print(prediccion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sports', 'politics', 'health', 'science'], dtype='<U8')"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.unique(prediccion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Clasificacion_3915",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "a821d265b19b8e474c372894184ad502aefb1f7882947607cd0ea5f074b097d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
