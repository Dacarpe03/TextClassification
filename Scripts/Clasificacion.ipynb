{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRÁCTICA 2: Clasificador de noticias\n",
    "\n",
    "### Nombres:\n",
    "Introduce en esta celda los nombres de los dos integrantes del grupo:\n",
    "- *Alumno 1:* DANIEL CARMONA PEDRAJAS\n",
    "- *Alumno 2:* JOEL PARDO FERRERA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objetivo: Implementar un clasificador usando el conjunto de datos recopilado de varias fuentes de internet como:\n",
    "\n",
    "- Google News que toma noticias de varios repositorios dedicados a la información\n",
    "- Periódicos:\n",
    "    - El País\n",
    "    - ABC\n",
    "    - El Confidencial\n",
    "    - 20minutos\n",
    "    - El Diario\n",
    "\n",
    "Este repositorio incluye tanto las noticias en formato '.txt' donde se almacenan los cuerpos de noticia y sus correspondientes títulos, como un '.csv' donde se contiene un registro de todas las noticias donde se refleja el número de noticias, la clase a la que pertenece (deportes, salud, ciencia y politica), el número de noticia dentro de la clase correspondiente, el título de noticia, la ruta donde está almacenada esa noticia, y por último la URL de donde se ha sacado la noticia. \n",
    "\n",
    "La fechas tanto de publicación como de obtención e datos se ubican en Noviembre de 2022. \n",
    "\n",
    "La clase a predecir es el tipo de noticia (columna 'category' de la base de datos), a partir de los archivos '.txt'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. IMPORTACIÓN DE LIBRERÍAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# git config --global user.email \"jpardo0824@gmail.com\"\n",
    "# git config --global user.name \"JPardo08\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install pandas\n",
    "# !pip3 install spacy\n",
    "# !pip3 install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import unicodedata\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CARGA DE DATOS\n",
    "Cargamos los datos en dos formatos:\n",
    "* DataFrame de pandas\n",
    "* Generador\n",
    "\n",
    "Para cargar los datos utilizamos la libreria Pandas. En la funcion impleentada, le pasamos unicmanete  la ruta del archivo csv que queramos cargar y se guarda los datos en una variable generador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta = \"/Users/joelpardo/Desktop/TextClassification\" ## CAMBIAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt(df):\n",
    "    \"\"\" Funcion para coger los documentos '.txt' de las noticias\n",
    "    \n",
    "    Path: Path de la carpeta donde se encuentras los documentos\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    paths = df[\"path\"].tolist()\n",
    "    clases = df[\"category\"].unique().tolist()\n",
    "\n",
    "    documentos = []\n",
    "\n",
    "    for i in paths:\n",
    "        \n",
    "        t = i.replace(\".\", ruta,1) \n",
    "        s = t.replace(\"/\", \"//\")\n",
    "        print(s)\n",
    "\n",
    "        with open(s, \"r\", encoding=\"latin-1\", errors='ignore') as f:\n",
    "            \n",
    "            lineas = f.readlines()\n",
    "            txt1 = ''.join(lineas)\n",
    "            documentos.append(txt1)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    return clases, documentos\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//Users//joelpardo//Desktop//TextClassification//Datos//urls1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yq/prfd22yn0k3gnlt89mfz3zb80000gn/T/ipykernel_1735/261014751.py:9: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  df = pd.read_csv(p_df2,',')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>category</th>\n",
       "      <th>n_doc</th>\n",
       "      <th>title</th>\n",
       "      <th>path</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>sports</td>\n",
       "      <td>1</td>\n",
       "      <td>Memphis y Dumfries certifican que el 'soccer' ...</td>\n",
       "      <td>./Datos/Raw_data/sports/1.txt</td>\n",
       "      <td>https://news.google.com/__i/rss/rd/articles/CB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>sports</td>\n",
       "      <td>2</td>\n",
       "      <td>Con el dinero no basta en el fútbol - AS</td>\n",
       "      <td>./Datos/Raw_data/sports/2.txt</td>\n",
       "      <td>https://news.google.com/__i/rss/rd/articles/CB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>sports</td>\n",
       "      <td>3</td>\n",
       "      <td>El fútbol es un cuento - La Voz de Galicia</td>\n",
       "      <td>./Datos/Raw_data/sports/3.txt</td>\n",
       "      <td>https://news.google.com/__i/rss/rd/articles/CB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>sports</td>\n",
       "      <td>4</td>\n",
       "      <td>Cavani y Giménez serán sancionados pero no con...</td>\n",
       "      <td>./Datos/Raw_data/sports/4.txt</td>\n",
       "      <td>https://news.google.com/__i/rss/rd/articles/CB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>sports</td>\n",
       "      <td>5</td>\n",
       "      <td>Suspendido por una tangana a puñetazos el Alme...</td>\n",
       "      <td>./Datos/Raw_data/sports/5.txt</td>\n",
       "      <td>https://news.google.com/__i/rss/rd/articles/CB...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index category  n_doc                                              title  \\\n",
       "0      0   sports      1  Memphis y Dumfries certifican que el 'soccer' ...   \n",
       "1      1   sports      2          Con el dinero no basta en el fútbol - AS    \n",
       "2      2   sports      3         El fútbol es un cuento - La Voz de Galicia   \n",
       "3      3   sports      4  Cavani y Giménez serán sancionados pero no con...   \n",
       "4      4   sports      5  Suspendido por una tangana a puñetazos el Alme...   \n",
       "\n",
       "                            path  \\\n",
       "0  ./Datos/Raw_data/sports/1.txt   \n",
       "1  ./Datos/Raw_data/sports/2.txt   \n",
       "2  ./Datos/Raw_data/sports/3.txt   \n",
       "3  ./Datos/Raw_data/sports/4.txt   \n",
       "4  ./Datos/Raw_data/sports/5.txt   \n",
       "\n",
       "                                                link  \n",
       "0  https://news.google.com/__i/rss/rd/articles/CB...  \n",
       "1  https://news.google.com/__i/rss/rd/articles/CB...  \n",
       "2  https://news.google.com/__i/rss/rd/articles/CB...  \n",
       "3  https://news.google.com/__i/rss/rd/articles/CB...  \n",
       "4  https://news.google.com/__i/rss/rd/articles/CB...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_df = ruta + \"/Datos/urls1.csv\"\n",
    "p_df2 = p_df.replace(\"/\", \"//\")\n",
    "print(p_df2)\n",
    "\n",
    "# df_gen = parse(r'/Users/joelpardo/Desktop/TextClassification/Datos/urls1.csv')\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(p_df2,',')\n",
    "\n",
    "#Visualizamos los datos\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//1.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//2.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//3.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//4.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//5.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//6.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//7.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//8.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//9.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//10.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//11.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//12.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//13.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//14.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//15.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//16.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//17.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//18.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//19.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//20.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//21.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//22.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//23.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//24.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//25.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//26.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//27.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//28.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//29.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//30.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//31.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//32.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//33.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//34.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//35.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//36.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//37.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//38.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//39.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//40.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//41.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//42.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//43.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//44.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//45.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//46.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//47.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//48.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//49.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//50.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//51.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//52.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//53.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//54.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//55.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//56.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//57.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//58.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//59.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//sports//60.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//1.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//2.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//3.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//4.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//5.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//6.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//7.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//8.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//9.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//10.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//11.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//12.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//13.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//14.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//15.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//16.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//17.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//18.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//19.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//20.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//21.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//22.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//23.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//24.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//25.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//26.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//27.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//28.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//29.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//30.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//31.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//32.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//33.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//34.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//35.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//36.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//37.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//38.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//39.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//40.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//41.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//42.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//43.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//44.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//45.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//46.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//47.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//48.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//49.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//50.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//51.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//52.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//53.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//54.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//55.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//56.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//57.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//58.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//59.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//health//60.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//1.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//2.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//3.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//4.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//5.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//6.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//7.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//8.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//9.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//10.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//11.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//12.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//13.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//14.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//15.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//16.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//17.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//18.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//19.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//20.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//21.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//22.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//23.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//24.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//25.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//26.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//27.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//28.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//29.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//30.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//31.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//32.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//33.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//34.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//35.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//36.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//37.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//38.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//39.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//40.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//41.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//42.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//43.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//44.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//45.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//46.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//47.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//48.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//49.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//50.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//51.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//52.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//53.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//54.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//55.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//56.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//57.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//58.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//59.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//science//60.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//1.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//2.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//3.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//4.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//5.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//6.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//7.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//8.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//9.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//10.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//11.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//12.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//13.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//14.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//15.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//16.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//17.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//18.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//19.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//20.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//21.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//22.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//23.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//24.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//25.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//26.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//27.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//28.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//29.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//30.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//31.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//32.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//33.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//34.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//35.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//36.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//37.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//38.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//39.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//40.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//41.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//42.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//43.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//44.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//45.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//46.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//47.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//48.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//49.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//50.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//51.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//52.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//53.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//54.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//55.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//56.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//57.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//58.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//59.txt\n",
      "//Users//joelpardo//Desktop//TextClassification//Datos//Raw_data//politics//60.txt\n",
      "4\n",
      "240\n"
     ]
    }
   ],
   "source": [
    "clases, documentos = txt(df)\n",
    "\n",
    "print(len(clases))\n",
    "print(len(documentos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "#Comprobamos los tipos\n",
    "print(type(df))\n",
    "# print(type(df_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El lateral neerlandés regaló dos asistencias, marcó el 1-3... y el delantero azulgrana bailó con la defensa estadounidense para clasificar a la Oranje a los cuartos de final del Mundial\n",
      "\"En Países Bajos aprendí mucho, tomé muchos conceptos de fútbol\". Gregg Berhalter, seleccionador de Estados Unidos, aseguró en la previa del partido que se inspiró en el fútbol neerlandés para fijar las bases de su ideario como entrenador. Unos conceptos futbolísticos que se dejaron ver en los primeros compases del partido, hasta que Memphis Depay decidió romper el espejismo impuesto por la caballería yankee.\n",
      "Averiguaron la forma en la que tenían que atacar a Países Bajos para desarmar su entramado defensivo pero Pulisic, que no tuvo su día, no consiguió mandar al fondo de la red ni una de las dos ocasiones que tuvo en la primera parte. Una de ellas tardará mucho tiempo en olvidarla. \n",
      "Instaurados en el bloque bajo, los hombres de Louis van Gaal dieron una auténtica lección de cómo un equipo tiene que atacar el espacio con sus contras. El '10' bajaba a recibir, 'liberaba' de sus funciones a los centrocampistas... e incluso era capaz de llegar al área para generar todo el peligro del conjunto neerlandés. Y en la primera que cazó dentro del área, Depay no perdonó para certifican que el 'soccer' todavía está lejos del modelo de fútbol de Países Bajos.\n",
      "Dos cabalgadas de Dumfries fueron más que suficientes para romper a toda la defensa de Estados Unidos: la primera la mandó Depay al fondo de la red y en el tercer pase de la muerte del lateral en la primera parte, el segundo lo cortó Adams con muchos problemas, apareció Blind para firmar el 2-0 justo antes del descanso. Dos acciones calcadas que ponían a la Oranje con pie y medio en los cuartos de final del Mundial de Qatar 2022.\n",
      "Memphis aprovechaba con su tanto para deshacer el empate con Klass-Jan Huntelaar y convertirse así en el segundo máximo goleador de la selección neerlandesa, solo superado por Robin Van Persie (50). \n",
      "Y cuando Países Bajos disfrutaba de la segunda parte, apareció Wright para ponerle un poco de emoción al partido con una acción de ilusionista. El delantero estadounidense se inventó un escorzo de película para mandar el balón al fondo de la red con un taconazo brillante. El 'Capitán América' colgaba el balón en el área pequeña y el '19' tiraba de imaginación para firmar el 1-2. \n",
      "Sin embargo, la rebelión en la que Estados Unidos parecía buscar la independencia duró tan sólo 5 minutos. Dumfries, MVP del partido, apareció primero en su área sacando un disparo de Wright  en la línea de gol y sorprendía en la jugada siguiente para finiquitar el partido con el 3-1. Balón colgado de Blind al segundo palo y el lateral, completamente sólo, empalaba a la perfección el esférico para cerrar la clasificación de la la Oranje a los cuartos de final del Mundial. El show terminó siendo naranja. \n"
     ]
    }
   ],
   "source": [
    "print(documentos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"docs\"] = documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'category', 'n_doc', 'title', 'path', 'link', 'docs'], dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Miramos los nombres\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generamos un duplicado para seleccionar lo que queremos\n",
    "df2 = df.loc[:,['title', 'category','docs']]\n",
    "\n",
    "#Eliminamos los na\n",
    "df2=df2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 7)\n",
      "(240, 3)\n"
     ]
    }
   ],
   "source": [
    "#Comprobamos cuantas fila con na hemos eliminado\n",
    "print(df.shape)\n",
    "print(df2.shape)\n",
    "\n",
    "#Ninguna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sports', 'health', 'science', 'politics']\n"
     ]
    }
   ],
   "source": [
    "#Comprobamos los valores de la columna category\n",
    "# clases = pd.unique(df2['category']).tolist()\n",
    "print(clases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>docs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>health</th>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>politics</th>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>science</th>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sports</th>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          title  docs\n",
       "category             \n",
       "health       60    60\n",
       "politics     60    60\n",
       "science      60    60\n",
       "sports       60    60"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vamos a ver que clases tienen más peso\n",
    "frecuencias = df2.groupby('category').count()\n",
    "frecuencias = frecuencias.sort_values('title',ascending=False)\n",
    "\n",
    "#Visualizamos las clases con mas peso (todas)\n",
    "frecuencias.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, todas las clases tienen el mismo peso, es decir, tienen el mismo numero de noticas. Tenemos un conjunto de datos homogeneo para el cual hacer la prediccion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PRE-PROCESADO\n",
    "Realizaremos el siguiente procesado:\n",
    "- Separar el texto en *tokens*\n",
    "- Eliminar los *tokens* de tipo *stop-word*, signos de puntuación, signos especiales o espacios\n",
    "- Lematizar el texto\n",
    "- Introducimos un espacio después de determinados signos de puntuación (\".\", \"?\", \"%\") para que el tokenizado sea correcto\n",
    "- Filtramos los *tokens* con una longitud de 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting es-core-news-lg==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_lg-3.4.0/es_core_news_lg-3.4.0-py3-none-any.whl (568.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m568.0/568.0 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from es-core-news-lg==3.4.0) (3.4.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (2.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (2.0.7)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (2.4.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (1.0.9)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (6.3.0)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (0.7.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (8.1.5)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (1.0.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (2.28.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (1.10.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (3.1.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (3.0.8)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (3.0.10)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (65.5.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (1.26.13)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (0.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_lg')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download es_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spacy.io/models/es\n",
    "nlp = spacy.load(\"es_core_news_lg\") #Mejor modelo optimizado para la CPU\n",
    "\n",
    "#definimos función de normalizado\n",
    "def normaliza(texto):\n",
    "    texto = re.sub(r\"(\\.)|(\\?)|(\\%)\", r\"\\1 |\\2 |\\3 \", texto) #añadimos un espacio después de \".\" y \"?\"\n",
    "    doc = nlp(texto)\n",
    "    tokens = [t for t in doc if\n",
    "                        len(t.text)>1 and\n",
    "                       not t.is_stop and\n",
    "                       not t.is_space and\n",
    "                       not t.is_punct]#filtramos los tokens que nos interesan\n",
    "    palabras = []\n",
    "    for t in tokens:\n",
    "        palabras.append(t.lemma_) #añadimos lema\n",
    "    salida = ' '.join(palabras)#junta todos los tokens en un string\n",
    "    \n",
    "    return salida\n",
    "\n",
    "#funcion para quitar acentos y caracteres no ASCII:\n",
    "\n",
    "def remove_accents(input_str):     \n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)     \n",
    "    only_ascii = nfkd_form.encode('ASCII', 'ignore')     \n",
    "    return only_ascii.decode(\"utf-8\", 'ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El lateral neerlandés regaló dos asistencias, marcó el 1-3... y el delantero azulgrana bailó con la defensa estadounidense para clasificar a la Oranje a los cuartos de final del Mundial\n",
      "\"En Países Bajos aprendí mucho, tomé muchos conceptos de fútbol\". Gregg Berhalter, seleccionador de Estados Unidos, aseguró en la previa del partido que se inspiró en el fútbol neerlandés para fijar las bases de su ideario como entrenador. Unos conceptos futbolísticos que se dejaron ver en los primeros compases del partido, hasta que Memphis Depay decidió romper el espejismo impuesto por la caballería yankee.\n",
      "Averiguaron la forma en la que tenían que atacar a Países Bajos para desarmar su entramado defensivo pero Pulisic, que no tuvo su día, no consiguió mandar al fondo de la red ni una de las dos ocasiones que tuvo en la primera parte. Una de ellas tardará mucho tiempo en olvidarla. \n",
      "Instaurados en el bloque bajo, los hombres de Louis van Gaal dieron una auténtica lección de cómo un equipo tiene que atacar el espacio con sus contras. El '10' bajaba a recibir, 'liberaba' de sus funciones a los centrocampistas... e incluso era capaz de llegar al área para generar todo el peligro del conjunto neerlandés. Y en la primera que cazó dentro del área, Depay no perdonó para certifican que el 'soccer' todavía está lejos del modelo de fútbol de Países Bajos.\n",
      "Dos cabalgadas de Dumfries fueron más que suficientes para romper a toda la defensa de Estados Unidos: la primera la mandó Depay al fondo de la red y en el tercer pase de la muerte del lateral en la primera parte, el segundo lo cortó Adams con muchos problemas, apareció Blind para firmar el 2-0 justo antes del descanso. Dos acciones calcadas que ponían a la Oranje con pie y medio en los cuartos de final del Mundial de Qatar 2022.\n",
      "Memphis aprovechaba con su tanto para deshacer el empate con Klass-Jan Huntelaar y convertirse así en el segundo máximo goleador de la selección neerlandesa, solo superado por Robin Van Persie (50). \n",
      "Y cuando Países Bajos disfrutaba de la segunda parte, apareció Wright para ponerle un poco de emoción al partido con una acción de ilusionista. El delantero estadounidense se inventó un escorzo de película para mandar el balón al fondo de la red con un taconazo brillante. El 'Capitán América' colgaba el balón en el área pequeña y el '19' tiraba de imaginación para firmar el 1-2. \n",
      "Sin embargo, la rebelión en la que Estados Unidos parecía buscar la independencia duró tan sólo 5 minutos. Dumfries, MVP del partido, apareció primero en su área sacando un disparo de Wright  en la línea de gol y sorprendía en la jugada siguiente para finiquitar el partido con el 3-1. Balón colgado de Blind al segundo palo y el lateral, completamente sólo, empalaba a la perfección el esférico para cerrar la clasificación de la la Oranje a los cuartos de final del Mundial. El show terminó siendo naranja. \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'lateral neerlandés regalar asistencia marcar 1-3 delantero azulgrana bailar defensa estadounidense clasificar Oranje cuarto Mundial Países Bajos aprender tomar concepto fútbol Gregg Berhalter seleccionador Unidos previa partido inspirar fútbol neerlandés fijar base ideario entrenador concepto futbolístico dejar compás partido Memphis Depay decidir romper espejismo impuesto caballería yankee averiguar forma tener atacar Países Bajos desarmar entramado defensivo Pulisic conseguir mandar fondo red ocasión tardar tiempo olvidar él Instaurados bloque hombre Louis Gaal auténtico lección equipo atacar espacio contra 10 bajar recibir liberar función centrocampista capaz llegar área generar peligro conjunto neerlandés cazar área Depay perdonar certificar soccer lejos modelo fútbol Países Bajos cabalgada Dumfries suficiente romper defensa Unidos mandar Depay fondo red tercer pase muerte lateral cortar Adams problema aparecer Blind firmar 2-0 justo descanso acción calcada poner Oranje pie cuarto Mundial Qatar 2022 Memphis aprovechar deshacer empate Klass-Jan Huntelaar convertir él máximo goleador selección neerlandés superado Robin Persie 50 Países Bajos disfrutar aparecer Wright poner él emoción partido acción ilusionista delantero estadounidense inventar escorzo película mandar balón fondo red taconazo brillante Capitán América colgar balón área pequeño 19 tirar imaginación firmar 1-2 rebelión Unidos parecer buscar independencia durar minuto Dumfries MVP partido aparecer área sacar disparo Wright línea gol sorprender jugada finiquitar partido 3-1 Balón colgado Blind palo lateral completamente empalar perfección esférico cerrar clasificación Oranje cuarto Mundial show terminar naranja'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Comprobamos que funciona con el elemento 0 de noticia (\"noticia\")\n",
    "print(df2['docs'][0])\n",
    "print(\"-\"*140)\n",
    "normaliza(df2['docs'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El fútbol es un cuento. Ya lo dijo ese Shakespeare futbolístico que es Gary Lineker, buen goleador y estrella de la tele: «El fútbol es un juego muy simple. 22 hombres persiguen un balón durante 90 minutos y al final siempre ganan los alemanes». La frase ha envejecido mal. Las mujeres también corren detrás de la pelota y arbitran (aunque sea a cuentagotas). Los alemanes ya no le ganan ni a Japón (nosotros tampoco). Y ese juego simple puede ser un tostón. Más que el fútbol, nos gusta el negocio del fútbol, esa fábrica de millonarios, algunos por lo civil y otros por lo criminal, como diría Luis Aragonés. Nos gustan su cultura, sus batallas, el cuento del fútbol.\n",
      "No está tan mal un Mundial en Navidad, al margen de la tropelía de la FIFA y Catar. Con las mil y una noches persas y las luces del desierto y de las calles de Vigo. Las jóvenes iraníes jugando a tirarles el turbante a los ayatolás. Los vecinos de la calle Príncipe contando ovejas viendo pasar el carricoche de la montaña rusa por la ventana.\n",
      "Infantino es Gargamel. Cristiano es la madrastra que se mira en el espejo. Luis Enrique es el principito arrogante. Blatter es el Doctor Claw, el malo sin rostro del inspector Gadget que acariciaba su gatito. Messi es Messi. Luis Suárez llora por el otro paisito gallego. Y el seleccionador de Camerún se ha ido a casa dejando para la posteridad la teoría del peligro: «Cuando sabes que estás en peligro, en realidad ya no estás en peligro. Cuando no sabes que estás en peligro, es cuando realmente estás en peligro». La Navidad este año no la ha inaugurado el Black Friday de Caballero, sino el Catar-Ecuador de Doha. Mucho cuento.\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'fútbol cuento Shakespeare futbolístico Gary Lineker goleador estrella tele fútbol juego simple 22 hombre perseguir balón 90 minuto ganar alemán frase envejecer mujer correr pelota arbitrar cuentagotas alemán ganar Japón juego simple tostón fútbol gustar negocio fútbol fábrica millonario civil criminal decir Luis Aragonés gustar cultura batalla cuento fútbol Mundial Navidad margen tropelía FIFA Catar mil noche persa luz desierto calle Vigo joven iraní jugar tirar él turbante ayatolá vecino calle Príncipe contar oveja ver pasar carricoche montaña ruso ventana Infantino Gargamel Cristiano madrastra mirar espejo Luis Enrique principito arrogante Blatter Doctor Claw malo rostro inspector Gadget acariciar gatito Messi Messi Luis Suárez llorar paisito gallego seleccionador Camerún ir casa dejar posteridad teoría peligro estar peligro realidad estar peligro estar peligro realmente estar peligro Navidad año inaugurar Black Friday Caballero Catar-Ecuador Doha cuento'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Comprobamos que funciona con el elemento 2 de noticia (\"docs\")\n",
    "print(df2['docs'][2])\n",
    "print(\"-\"*140)\n",
    "normaliza(df2['docs'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creacion de corpus y labels \n",
    "df2[\"corpus\"] = df2['docs'] + df2['title']\n",
    "corpus = df2[\"corpus\"].tolist()\n",
    "\n",
    "labels_posibles = pd.unique(df2['category']).tolist()\n",
    "labels = df2['category'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediccion_r=ruta+\"/Prediccion\"\n",
    "\n",
    "os.mkdir(prediccion_r)\n",
    "\n",
    "df2.to_csv(prediccion_r+'/training.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tamaño  TOTAL: 240\n",
      "Tamaño de clase TOTAL: 240\n"
     ]
    }
   ],
   "source": [
    "#Vemos cuantas observaciones en total tenemos.\n",
    "print(f'\\nTamaño  TOTAL: {len(corpus)}')\n",
    "print(f'Tamaño de clase TOTAL: {len(labels)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seleccionamos 5000 muestras del cojunto de datos total\n",
    "# corpus_sm, labels_sm = resample(corpus, labels, n_samples=5000, replace=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separacion del conjunto de datos P5\n",
    "train_corpus, test_corpus, train_labels, test_labels = train_test_split(corpus, labels, test_size = 0.30, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tamaño de TRAIN: 168\n",
      "Tamaño de clase TRAIN: 168\n",
      "\n",
      "Tamaño de TEST: 72\n",
      "Tamaño de clase TEST: 72\n"
     ]
    }
   ],
   "source": [
    "# COMPLETAR\n",
    "print(f'\\nTamaño de TRAIN: {len(train_corpus)}')\n",
    "print(f'Tamaño de clase TRAIN: {len(train_labels)}')\n",
    "\n",
    "print(f'\\nTamaño de TEST: {len(test_corpus)}')\n",
    "print(f'Tamaño de clase TEST: {len(test_labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizamos los docss y los guardamos como una lista en lugar de generator porque \n",
    "#lo tenemos que múltiples veces y no queremos tener que normalizar todo el corpus cada vez.\n",
    "norm_train_corpus = list(map(normaliza, train_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_test_corpus = list(map(normaliza, test_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. EXTRACCION DE CARACTERISTICAS\n",
    "Instanciamos los vectorizadores para obtener las características BoW y TF-IDF.  \n",
    "Usamos el parámetro max_df=0.9 para eliminar los stop-words como las palabras que aparecen al menos en el 90% de los documentos y el parámetro min_df=0.01 para eliminar las palabras que no aparecen al menos en un 1% de los documentos.\\\n",
    "\n",
    "Usamos el modelo `TfidfTransformer` para calcular la matriz TF-IDF a partir del BoW y no tener que repetir todo el entrenamiento.\n",
    "\n",
    "Para calcular los modelos basados en WV usamos el modelo gloVe pre-entrenado en `spaCy`. Calculamos dos modelos basados en word-vectors:  \n",
    "* El vector promedio de los WV de todos los tokens con el mismo peso para todas las palabras.  \n",
    "* Ponderando el WV de cada palabra por el término de frecuencia inversa de documento (IDF).  \n",
    "\n",
    "Definimos las funciones para calcular estas dos matrices de características:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BoW\n",
    "bow_vectorizer = CountVectorizer(min_df=0.01, max_df=0.9)\n",
    "\n",
    "#Tf-idf\n",
    "tfidf_vectorizer = TfidfTransformer()\n",
    "\n",
    "#Funciones de WV.\n",
    "def averaged_word_vectorizer(corpus):\n",
    "    '''Aplica la función de cálculo del WE promedio a todos los\n",
    "    documentos del corpus (cada doc es una lista de tokens)'''\n",
    "    features = [nlp(doc).vector\n",
    "                    for doc in corpus]\n",
    "    return np.array(features)\n",
    "\n",
    "def tfidf_wtd_avg_word_vectors(doc, word_tfidf_map):\n",
    "    '''Aplica la función de cálculo del WE ponderado por TF-IDF\n",
    "    a un documento (como lista de tokens)'''\n",
    "    tokens = doc.split()\n",
    "\n",
    "    feature_vector = np.zeros((nlp.vocab.vectors_length,),dtype=\"float64\")\n",
    "    wts = 0.      \n",
    "    for word in tokens:\n",
    "        if nlp.vocab[word].has_vector and word_tfidf_map.get(word, 0): #sólo considera palabras conocidas\n",
    "            weighted_word_vector = word_tfidf_map[word] * nlp.vocab[word].vector\n",
    "            wts = wts + 1\n",
    "            feature_vector = np.add(feature_vector, weighted_word_vector)\n",
    "    if wts:\n",
    "        feature_vector = np.divide(feature_vector, wts)\n",
    "        \n",
    "    return feature_vector\n",
    "    \n",
    "def tfidf_weighted_averaged_word_vectorizer(corpus, word_tfidf_map):\n",
    "    '''Aplica la función de cálculo del WE ponderado por TF-IDF a todos los\n",
    "    documentos del corpus (cada doc es una lista de tokens)'''                                       \n",
    "    features = [tfidf_wtd_avg_word_vectors(doc, word_tfidf_map)\n",
    "                    for doc in corpus]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# características bag of words\n",
    "bow_train_features = bow_vectorizer.fit_transform(norm_train_corpus)  \n",
    "bow_test_features = bow_vectorizer.transform(norm_test_corpus) \n",
    "\n",
    "# características tfidf (a partir del BoW)\n",
    "tfidf_train_features = tfidf_vectorizer.fit_transform(bow_train_features)\n",
    "tfidf_test_features = tfidf_vectorizer.transform(bow_test_features)    \n",
    "\n",
    "# características averaged word vector\n",
    "avg_wv_train_features = averaged_word_vectorizer(norm_train_corpus)                \n",
    "avg_wv_test_features = averaged_word_vectorizer(norm_test_corpus)      \n",
    "\n",
    "# características tfidf weighted averaged word vector\n",
    "word_tfidf_map = {key:value for (key, value) in zip(bow_vectorizer.get_feature_names_out(), tfidf_vectorizer.idf_)}\n",
    "\n",
    "tfidf_wv_train_features = tfidf_weighted_averaged_word_vectorizer(norm_train_corpus, word_tfidf_map)\n",
    "\n",
    "tfidf_wv_test_features = tfidf_weighted_averaged_word_vectorizer(norm_test_corpus, word_tfidf_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168, 4842)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['000', '03', '05', ..., 'únicamente', 'único', 'útil'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168, 4842)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['x0', 'x1', 'x2', ..., 'x4839', 'x4840', 'x4841'], dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168, 300)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_wv_train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168, 300)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_wv_train_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CLASIFICADOR\n",
    "Aplicamos distintos clasificadores a cada modelo para ver cuál funciona mejor con nuestros datos. Primero definimos unas funciones para entrenar y medir el rendimiento de los clasificadores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(true_labels, predicted_labels):\n",
    "    \"\"\"Calculamos distintas métricas sobre el\n",
    "    rendimiento del modelo. Devuelve un diccionario\n",
    "    con los parámetros medidos\"\"\"\n",
    "    \n",
    "    return {\n",
    "        'Accuracy': np.round(\n",
    "                        metrics.accuracy_score(true_labels, \n",
    "                                               predicted_labels),\n",
    "                        3),\n",
    "        'Precision': np.round(\n",
    "                        metrics.precision_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted',\n",
    "                                               zero_division=0),\n",
    "                        3),\n",
    "    'Recall': np.round(\n",
    "                        metrics.recall_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted',\n",
    "                                               zero_division=0),\n",
    "                        3),\n",
    "    'F1 Score': np.round(\n",
    "                        metrics.f1_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted',\n",
    "                                               zero_division=0),\n",
    "                        3)}\n",
    "                        \n",
    "\n",
    "def train_predict_evaluate_model(classifier, \n",
    "                                 train_features, train_labels, \n",
    "                                 test_features, test_labels):\n",
    "    \"\"\"Función que entrena un modelo de clasificación sobre\n",
    "    un conjunto de entrenamiento, lo aplica sobre un conjunto\n",
    "    de test y devuelve la predicción sobre el conjunto de test\n",
    "    y las métricas de rendimiento\"\"\"\n",
    "    # genera modelo    \n",
    "    classifier.fit(train_features, train_labels)\n",
    "    # predice usando el modelo sobre test\n",
    "    predictions = classifier.predict(test_features) \n",
    "    # evalúa rendimiento de la predicción   \n",
    "    metricas = get_metrics(true_labels=test_labels, \n",
    "                predicted_labels=predictions)\n",
    "    return predictions, metricas     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a entrenar sobre el conjunto de train y evaluamos en el conjunto de test. Guardamos métricas en una lista y resultados en otra para mostrar resumen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLR = LogisticRegression(solver='liblinear')\n",
    "modelNB = GaussianNB() #var_smoothing=1e-9\n",
    "modelSVM = SGDClassifier(loss='hinge', max_iter=1000)\n",
    "modelRBFSVM = SVC(gamma='scale', C=2)\n",
    "\n",
    "modelos = [('Logistic Regression', modelLR),\n",
    "           ('Naive Bayes', modelNB),\n",
    "           ('Linear SVM', modelSVM),\n",
    "           ('Gauss kernel SVM', modelRBFSVM)]\n",
    "\n",
    "metricas = []\n",
    "resultados = []\n",
    "\n",
    "# Modelos con características BoW\n",
    "bow_train_features_ = bow_train_features.toarray()\n",
    "bow_test_features_ = bow_test_features.toarray()\n",
    "for m, clf in modelos:\n",
    "    prediccion, metrica = train_predict_evaluate_model(classifier=clf,\n",
    "                                           train_features=bow_train_features_,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features_,\n",
    "                                           test_labels=test_labels)\n",
    "    metrica['modelo']=f'{m} BoW'\n",
    "    resultados.append(prediccion)\n",
    "    metricas.append(metrica)\n",
    "    \n",
    "# Modelos con características TF-IDF\n",
    "tfidf_train_features_ = tfidf_train_features.toarray()\n",
    "tfidf_test_features_ = tfidf_test_features.toarray()\n",
    "for m, clf in modelos:\n",
    "    prediccion, metrica = train_predict_evaluate_model(classifier=clf,\n",
    "                                           train_features=tfidf_train_features_,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features_,\n",
    "                                           test_labels=test_labels)\n",
    "    metrica['modelo']=f'{m} tfidf'\n",
    "    resultados.append(prediccion)\n",
    "    metricas.append(metrica)\n",
    "\n",
    "# Modelos con características averaged word vectors\n",
    "avg_wv_train_features_ = avg_wv_train_features\n",
    "avg_wv_test_features_ = avg_wv_test_features\n",
    "for m, clf in modelos:\n",
    "    prediccion, metrica = train_predict_evaluate_model(classifier=clf,\n",
    "                                           train_features=avg_wv_train_features_,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=avg_wv_test_features_,\n",
    "                                           test_labels=test_labels)\n",
    "    metrica['modelo']=f'{m} averaged'\n",
    "    resultados.append(prediccion)\n",
    "    metricas.append(metrica)\n",
    "\n",
    "# Modelos con características tfidf weighted averaged word vectors\n",
    "tfidf_wv_train_features_ = tfidf_wv_train_features\n",
    "tfidf_wv_test_features_ = tfidf_wv_test_features\n",
    "for m, clf in modelos:\n",
    "    prediccion, metrica = train_predict_evaluate_model(classifier=clf,\n",
    "                                           train_features=tfidf_wv_train_features_,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_wv_test_features_,\n",
    "                                           test_labels=test_labels)\n",
    "    metrica['modelo']=f'{m} tfidf wv'\n",
    "    resultados.append(prediccion)\n",
    "    metricas.append(metrica)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conviertimos la lista de métricas en un DataFrame para observar sus valores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>modelo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.139</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.057</td>\n",
       "      <td>Logistic Regression BoW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.153</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.045</td>\n",
       "      <td>Naive Bayes BoW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.139</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.051</td>\n",
       "      <td>Linear SVM BoW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.125</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.051</td>\n",
       "      <td>Gauss kernel SVM BoW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.153</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.057</td>\n",
       "      <td>Logistic Regression tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.153</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.045</td>\n",
       "      <td>Naive Bayes tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.153</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.050</td>\n",
       "      <td>Linear SVM tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.125</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.049</td>\n",
       "      <td>Gauss kernel SVM tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.139</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.051</td>\n",
       "      <td>Logistic Regression averaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.139</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.090</td>\n",
       "      <td>Naive Bayes averaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.153</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.050</td>\n",
       "      <td>Linear SVM averaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.153</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.051</td>\n",
       "      <td>Gauss kernel SVM averaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.139</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.062</td>\n",
       "      <td>Logistic Regression tfidf wv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.139</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.101</td>\n",
       "      <td>Naive Bayes tfidf wv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.167</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.060</td>\n",
       "      <td>Linear SVM tfidf wv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.125</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.071</td>\n",
       "      <td>Gauss kernel SVM tfidf wv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Accuracy  Precision  Recall  F1 Score                        modelo\n",
       "0      0.139      0.036   0.139     0.057       Logistic Regression BoW\n",
       "1      0.153      0.027   0.153     0.045               Naive Bayes BoW\n",
       "2      0.139      0.031   0.139     0.051                Linear SVM BoW\n",
       "3      0.125      0.032   0.125     0.051          Gauss kernel SVM BoW\n",
       "4      0.153      0.035   0.153     0.057     Logistic Regression tfidf\n",
       "5      0.153      0.027   0.153     0.045             Naive Bayes tfidf\n",
       "6      0.153      0.030   0.153     0.050              Linear SVM tfidf\n",
       "7      0.125      0.031   0.125     0.049        Gauss kernel SVM tfidf\n",
       "8      0.139      0.031   0.139     0.051  Logistic Regression averaged\n",
       "9      0.139      0.067   0.139     0.090          Naive Bayes averaged\n",
       "10     0.153      0.030   0.153     0.050           Linear SVM averaged\n",
       "11     0.153      0.031   0.153     0.051     Gauss kernel SVM averaged\n",
       "12     0.139      0.040   0.139     0.062  Logistic Regression tfidf wv\n",
       "13     0.139      0.079   0.139     0.101          Naive Bayes tfidf wv\n",
       "14     0.167      0.036   0.167     0.060           Linear SVM tfidf wv\n",
       "15     0.125      0.050   0.125     0.071     Gauss kernel SVM tfidf wv"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metricas = pd.DataFrame(metricas)\n",
    "metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricas.to_csv(prediccion_r+'/metricas.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordenamos las métricas por `accuracy` y muestra el mejor resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>modelo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.167</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.06</td>\n",
       "      <td>Linear SVM tfidf wv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Accuracy  Precision  Recall  F1 Score               modelo\n",
       "14     0.167      0.036   0.167      0.06  Linear SVM tfidf wv"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metricas = metricas.sort_values('Accuracy',ascending=False)\n",
    "metricas.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mejoramos el `accuracy` a partir del juego de parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM\n",
      "wv\n"
     ]
    }
   ],
   "source": [
    "modelos = metricas['modelo'].tolist()\n",
    "\n",
    "mejor = modelos[0]\n",
    "sep = mejor.split(' ')\n",
    "\n",
    "\n",
    "\n",
    "if sep[len(sep)-1]== \"wv\":\n",
    "    mo = ' '.join(sep[:len(sep)-2])\n",
    "    \n",
    "else:\n",
    "    mo = ' '.join(sep[:len(sep)-1])\n",
    "\n",
    "print(mo)\n",
    "    \n",
    "\n",
    "datos = sep[len(sep)-1]\n",
    "print(datos)\n",
    "\n",
    "metricas2 = []\n",
    "resultados = []\n",
    "\n",
    "\n",
    "\n",
    "# DATOS\n",
    "if datos == 'Bow':\n",
    "    train_features_ = bow_train_features.toarray()\n",
    "    test_features_ = bow_test_features.toarray()\n",
    "    \n",
    "if datos == 'tfidf':\n",
    "    train_features_ = tfidf_train_features.toarray()\n",
    "    test_features_ = tfidf_test_features.toarray()\n",
    "    \n",
    "if datos == 'averaged':\n",
    "    train_features_ = avg_wv_train_features\n",
    "    test_features_ = avg_wv_test_features\n",
    "    \n",
    "else: # tfidf wv\n",
    "    train_features_ = tfidf_wv_train_features\n",
    "    test_features_ = tfidf_wv_test_features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# MODELOS\n",
    "\n",
    "if mo == 'Logistic Regression':\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "    sol = [\"lbfgs\", \"liblinear\", \"newton-cg\", \"newton-cholesky\", \"sag\", \"saga\"]\n",
    "    \n",
    "    for i in sol:\n",
    "        modelLR = LogisticRegression(solver=i)\n",
    "        prediccion, metrica = train_predict_evaluate_model(classifier=modelLR,\n",
    "                                            train_features=train_features_,\n",
    "                                            train_labels=train_labels,\n",
    "                                            test_features=test_features_,\n",
    "                                            test_labels=test_labels)\n",
    "        metrica['modelo']=f'{i} - Logistic Regression ' + datos\n",
    "        resultados.append(prediccion)\n",
    "        metricas2.append(metrica)\n",
    "    \n",
    "if mo == 'Naive Bayes':\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html\n",
    "    var_smo = [1e-1,1e-2,1e-3,1e-4,1e-5,1e-6,1e-7,1e-8,1e-9,1e-10,1e-11,1e-12,1e-13,1e-14,1e-15]\n",
    "    \n",
    "    for i in var_smo:\n",
    "        modelNB = GaussianNB(var_smoothing=i)\n",
    "        prediccion, metrica = train_predict_evaluate_model(classifier=modelLR,\n",
    "                                            train_features=train_features_,\n",
    "                                            train_labels=train_labels,\n",
    "                                            test_features=test_features_,\n",
    "                                            test_labels=test_labels)\n",
    "        metrica['modelo']=f'{i} - Naive Bayes ' + datos\n",
    "        resultados.append(prediccion)\n",
    "        metricas2.append(metrica)\n",
    "    \n",
    "if mo == 'Linear SVM':\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "    loss = [\"hinge\", \"log_loss\", \"log\", \"modified_huber\", \"squared_hinge\", \"perceptron\", \"squared_error\", \"huber\", \"epsilon_insensitive\", \"squared_epsilon_insensitive\"]\n",
    "\n",
    "    for i in loss:\n",
    "        modelSVM = SGDClassifier(loss=i, max_iter=1000)\n",
    "        prediccion, metrica = train_predict_evaluate_model(classifier=modelLR,\n",
    "                                            train_features=train_features_,\n",
    "                                            train_labels=train_labels,\n",
    "                                            test_features=test_features_,\n",
    "                                            test_labels=test_labels)\n",
    "        metrica['modelo']=f'{i} - Linear SVM ' + datos\n",
    "        resultados.append(prediccion)\n",
    "        metricas2.append(metrica)\n",
    "\n",
    "    \n",
    "else: # Gauss kernel SVM \n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "    gamma = [\"scale\", \"auto\"]\n",
    "\n",
    "    for i in gamma:\n",
    "        modelRBFSVM = SVC(gamma=i, C=2)\n",
    "        prediccion, metrica = train_predict_evaluate_model(classifier=modelLR,\n",
    "                                            train_features=train_features_,\n",
    "                                            train_labels=train_labels,\n",
    "                                            test_features=test_features_,\n",
    "                                            test_labels=test_labels)\n",
    "        metrica['modelo']=f'{i} - Gauss kernel SVM ' + datos\n",
    "        resultados.append(prediccion)\n",
    "        metricas2.append(metrica)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conviertimos la lista de métricas en un DataFrame para observar sus valores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>modelo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.139</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.062</td>\n",
       "      <td>hinge - Linear SVM wv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.139</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.062</td>\n",
       "      <td>log_loss - Linear SVM wv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.139</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.062</td>\n",
       "      <td>log - Linear SVM wv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.139</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.062</td>\n",
       "      <td>modified_huber - Linear SVM wv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.139</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.062</td>\n",
       "      <td>squared_hinge - Linear SVM wv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.139</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.062</td>\n",
       "      <td>perceptron - Linear SVM wv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.139</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.062</td>\n",
       "      <td>squared_error - Linear SVM wv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.139</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.062</td>\n",
       "      <td>huber - Linear SVM wv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.139</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.062</td>\n",
       "      <td>epsilon_insensitive - Linear SVM wv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.139</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.062</td>\n",
       "      <td>squared_epsilon_insensitive - Linear SVM wv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Precision  Recall  F1 Score  \\\n",
       "0     0.139       0.04   0.139     0.062   \n",
       "1     0.139       0.04   0.139     0.062   \n",
       "2     0.139       0.04   0.139     0.062   \n",
       "3     0.139       0.04   0.139     0.062   \n",
       "4     0.139       0.04   0.139     0.062   \n",
       "5     0.139       0.04   0.139     0.062   \n",
       "6     0.139       0.04   0.139     0.062   \n",
       "7     0.139       0.04   0.139     0.062   \n",
       "8     0.139       0.04   0.139     0.062   \n",
       "9     0.139       0.04   0.139     0.062   \n",
       "\n",
       "                                        modelo  \n",
       "0                        hinge - Linear SVM wv  \n",
       "1                     log_loss - Linear SVM wv  \n",
       "2                          log - Linear SVM wv  \n",
       "3               modified_huber - Linear SVM wv  \n",
       "4                squared_hinge - Linear SVM wv  \n",
       "5                   perceptron - Linear SVM wv  \n",
       "6                squared_error - Linear SVM wv  \n",
       "7                        huber - Linear SVM wv  \n",
       "8          epsilon_insensitive - Linear SVM wv  \n",
       "9  squared_epsilon_insensitive - Linear SVM wv  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metricas3 = pd.DataFrame(metricas2)\n",
    "metricas3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordenamos las métricas por `accuracy` y muestra el mejor resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricas3 = metricas3.sort_values('Accuracy',ascending=False)\n",
    "metricas3.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. PRODUCCIÓN [PENDIENTE DE CAMBIAR] \n",
    "Cogemos el modelo que mejor funciona y lo aplicamos de forma manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividimos el conjunto total de muestras (`corpus` y `labels`) en entrenamiento y test (30%) y entrenamos el mejor modelo obtenido, para ver si se mejoran los resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MEJOR MODELO\n",
    "train_corpus, test_corpus, train_labels, test_labels = train_test_split(corpus, labels, test_size = 0.30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizamos\n",
    "norm_train_corpus = list(map(normaliza, train_corpus))\n",
    "norm_test_corpus = list(map(normaliza, test_corpus))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Características averaged word vector\n",
    "avg_wv_train_features = averaged_word_vectorizer(norm_train_corpus)                \n",
    "avg_wv_test_features = averaged_word_vectorizer(norm_test_corpus)      \n",
    "avg_wv_train_features_ = avg_wv_train_features\n",
    "avg_wv_test_features_ = avg_wv_test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos con características\n",
    "modelRBFSVM = SVC(gamma='scale', C=2)\n",
    "prediccion, metrica = train_predict_evaluate_model(classifier=modelRBFSVM,\n",
    "                                           train_features=avg_wv_train_features_,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=avg_wv_test_features_,\n",
    "                                           test_labels=test_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Modelo Gauss kernel SVM averaged con gamma scale: ')\n",
    "for i in metrica:\n",
    "    print('- ',i, ': ', metrica[i])\n",
    "\n",
    "print('\\nPredicciones:')\n",
    "print(prediccion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.unique(prediccion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "a821d265b19b8e474c372894184ad502aefb1f7882947607cd0ea5f074b097d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
