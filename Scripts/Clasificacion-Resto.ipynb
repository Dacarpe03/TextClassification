{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "11239500",
   "metadata": {},
   "source": [
    "## CLASIFICACIÓN TODOS LOS MODELOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "d823c7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.utils import simple_preprocess\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import spacy\n",
    "import unicodedata\n",
    "import os\n",
    "#from spellchecker import SpellChecker \n",
    "#from textblob import TextBlob \n",
    "#import contractions\n",
    "import re\n",
    "import random\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70b0a0f5",
   "metadata": {},
   "source": [
    "# 1. CARGA DE COJUNTOS DE NOTICIAS Y DE GLOSARIOS\n",
    "Cargamos los cojuntos que contienen las noticias tanto para el entrenamiento como para el test. Hay un glosario por cada una de las categorias de noticias que tenemos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb78b1e",
   "metadata": {},
   "source": [
    "Lo primero que necesitamos es cargar nuestros glosarios de términos para crear nuestro diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "074e9305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_glosario(categoria, conjunto):\n",
    "    fname = f\"../Datos/Glosarios/{conjunto}/glosario_{categoria}.txt\"\n",
    "\n",
    "    glosario = []\n",
    "    with open(fname, 'r') as f:\n",
    "        glosario = [termino.rstrip('\\n') for termino in f.readlines()]\n",
    "    return glosario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "99a6e637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ciencia': ['vacao',\n",
      "             'llama',\n",
      "             'captura',\n",
      "             'cola',\n",
      "             'honor',\n",
      "             'gemanidas',\n",
      "             'banyoles',\n",
      "             'neandertal',\n",
      "             'agujero',\n",
      "             'latigo',\n",
      "             'supersa',\n",
      "             'perfecto',\n",
      "             'regalo',\n",
      "             'bankman',\n",
      "             'fried',\n",
      "             'navidad',\n",
      "             'estampido',\n",
      "             'ftx',\n",
      "             'york',\n",
      "             'menta',\n",
      "             'supermasivo',\n",
      "             'barrera',\n",
      "             'segundo',\n",
      "             'ingeniero',\n",
      "             'magic',\n",
      "             'reencuentro',\n",
      "             'leo',\n",
      "             'nif',\n",
      "             'cernan',\n",
      "             'fauci'],\n",
      " 'deportes': ['falso',\n",
      "              'jamas',\n",
      "              'smash',\n",
      "              'mbappe',\n",
      "              'gigante',\n",
      "              'reserva',\n",
      "              'nets',\n",
      "              'carpena',\n",
      "              'djokovic',\n",
      "              'mclaren',\n",
      "              'exencion',\n",
      "              'butler',\n",
      "              'cuento',\n",
      "              'formato',\n",
      "              'campazzo',\n",
      "              'tenerife',\n",
      "              'domenicali',\n",
      "              'booker',\n",
      "              'tatum',\n",
      "              'madrid',\n",
      "              'real',\n",
      "              'premio',\n",
      "              'juventus',\n",
      "              'pagar',\n",
      "              'ronaldo',\n",
      "              'magnussen',\n",
      "              'steiner',\n",
      "              'gavi',\n",
      "              'porra',\n",
      "              'indycar',\n",
      "              'palou',\n",
      "              'gonzalez',\n",
      "              'gonzalo',\n",
      "              'rotacion',\n",
      "              'florentino',\n",
      "              'heredero',\n",
      "              'verdasco',\n",
      "              'enrique',\n",
      "              'luis',\n",
      "              'cristiano',\n",
      "              'cd',\n",
      "              'marbella',\n",
      "              'residencia',\n",
      "              'serbia',\n",
      "              'serbio',\n",
      "              'rosa',\n",
      "              'collins',\n",
      "              'jimmy',\n",
      "              'arabia',\n",
      "              'club'],\n",
      " 'politica': ['rosell',\n",
      "              'vuelo',\n",
      "              'sentir',\n",
      "              'reyes',\n",
      "              'rufian',\n",
      "              'unilateral',\n",
      "              'multilateral',\n",
      "              'ja',\n",
      "              'sinema',\n",
      "              'benito',\n",
      "              'juarez',\n",
      "              'turismo',\n",
      "              'ucraniano',\n",
      "              'nicolas',\n",
      "              'negativo',\n",
      "              'autoritariaa',\n",
      "              'ruido',\n",
      "              'nero',\n",
      "              'registral',\n",
      "              'demarcacia',\n",
      "              'ex',\n",
      "              'consulta',\n",
      "              'patrimonio',\n",
      "              'perao',\n",
      "              'juventud',\n",
      "              'trias',\n",
      "              'dema',\n",
      "              'coraza',\n",
      "              'ciudadano',\n",
      "              'anunciara'],\n",
      " 'salud': ['arterial',\n",
      "           'dash',\n",
      "           'anemia',\n",
      "           'folcodina',\n",
      "           'cafa',\n",
      "           'genital',\n",
      "           'vrs',\n",
      "           'unicef',\n",
      "           'lobo',\n",
      "           'verruga',\n",
      "           'aceite',\n",
      "           'mascarilla',\n",
      "           'pulmonar',\n",
      "           'presion',\n",
      "           'cannabis',\n",
      "           'pet',\n",
      "           'epilepsia',\n",
      "           'zumo',\n",
      "           'quiraorgica',\n",
      "           'afiliado',\n",
      "           'eps',\n",
      "           'ips',\n",
      "           'tc',\n",
      "           'creatividad',\n",
      "           'condiloma',\n",
      "           'marfan',\n",
      "           'manada',\n",
      "           'parasito',\n",
      "           'lavar',\n",
      "           'coosalud']}\n"
     ]
    }
   ],
   "source": [
    "categorias = [\"deportes\", \"salud\", \"ciencia\", \"politica\"]\n",
    "\n",
    "glosarios = {}\n",
    "for categoria in categorias:\n",
    "    glosarios[categoria] = cargar_glosario(categoria, \"train\")\n",
    "\n",
    "pprint(glosarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "dfdcfd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ciencia': ['sal',\n",
      "             'cuantico',\n",
      "             'idioma',\n",
      "             'fraa',\n",
      "             'vacuna',\n",
      "             'isidro',\n",
      "             'santo',\n",
      "             'llnl',\n",
      "             'agujero',\n",
      "             'sonido',\n",
      "             'antageno',\n",
      "             'subtipo',\n",
      "             'quipus',\n",
      "             'congelacia',\n",
      "             'ska',\n",
      "             'ofensivo',\n",
      "             'palabrota',\n",
      "             'canaria',\n",
      "             'relatividad',\n",
      "             'estruendo',\n",
      "             'programador',\n",
      "             'telescopio',\n",
      "             'congelar',\n",
      "             'gripe',\n",
      "             'alineacia',\n",
      "             'almanaque',\n",
      "             'escarcha',\n",
      "             'helada',\n",
      "             'sevilla',\n",
      "             'alphacode'],\n",
      " 'deportes': ['seguidor',\n",
      "              'carlos',\n",
      "              'mans',\n",
      "              'warren',\n",
      "              'doncic',\n",
      "              'suarez',\n",
      "              'juancho',\n",
      "              'boston',\n",
      "              'estabilidad',\n",
      "              'horford',\n",
      "              'hernanga',\n",
      "              'mez',\n",
      "              'ferrero',\n",
      "              'enrique',\n",
      "              'grada',\n",
      "              'mirar',\n",
      "              'marko',\n",
      "              'wiggins',\n",
      "              'stakhovsky',\n",
      "              'raptors',\n",
      "              'booker',\n",
      "              'resistencia',\n",
      "              'pts',\n",
      "              'reb',\n",
      "              'colombia',\n",
      "              'garden',\n",
      "              'brooklyn',\n",
      "              'cancha',\n",
      "              'lebron',\n",
      "              'gasto',\n",
      "              'tristeza',\n",
      "              'celtics',\n",
      "              'gimenez',\n",
      "              'sancionar',\n",
      "              'siebert',\n",
      "              'ktm',\n",
      "              'moto',\n",
      "              'dolares',\n",
      "              'rez',\n",
      "              'golden',\n",
      "              'state',\n",
      "              'warriors',\n",
      "              'lesia',\n",
      "              'leclerc',\n",
      "              'ofensivo',\n",
      "              'suns',\n",
      "              'krack',\n",
      "              'james',\n",
      "              'luis',\n",
      "              'aerodinamico'],\n",
      " 'politica': ['silva',\n",
      "              'refera',\n",
      "              'ilacito',\n",
      "              'rodraguez',\n",
      "              'torra',\n",
      "              'insulto',\n",
      "              'urgente',\n",
      "              'esparza',\n",
      "              'mocia',\n",
      "              'animal',\n",
      "              'castillo',\n",
      "              'almeida',\n",
      "              'smith',\n",
      "              'upn',\n",
      "              'cheque',\n",
      "              'ayres',\n",
      "              'concejala',\n",
      "              'mexicano',\n",
      "              'catalunya',\n",
      "              'isabel',\n",
      "              'ndum',\n",
      "              'nduma',\n",
      "              'ahorro',\n",
      "              'nombramiento',\n",
      "              'obrador',\n",
      "              'adjudicacia',\n",
      "              'gandia',\n",
      "              'procesado',\n",
      "              'navarra',\n",
      "              'auxiliar'],\n",
      " 'salud': ['strep',\n",
      "           'estigma',\n",
      "           'secuela',\n",
      "           'automatico',\n",
      "           'orina',\n",
      "           'antidepresivo',\n",
      "           'aprovechar',\n",
      "           'nuez',\n",
      "           'entrenar',\n",
      "           'bacteria',\n",
      "           'fruto',\n",
      "           'seco',\n",
      "           'cabello',\n",
      "           'atras',\n",
      "           'sida',\n",
      "           'vih',\n",
      "           'esconder',\n",
      "           'temporada',\n",
      "           'receta',\n",
      "           'congelado',\n",
      "           'alga',\n",
      "           'wakame',\n",
      "           'rehabilitacia',\n",
      "           'adolescente',\n",
      "           'gotlib',\n",
      "           'depresion',\n",
      "           'bronquiolitis',\n",
      "           'cuadro',\n",
      "           'niaos',\n",
      "           'morder']}\n"
     ]
    }
   ],
   "source": [
    "glosarios_t = {}\n",
    "for categoria in categorias:\n",
    "    glosarios_t[categoria] = cargar_glosario(categoria, \"test\")\n",
    "\n",
    "pprint(glosarios_t)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d790efb0",
   "metadata": {},
   "source": [
    "Creamos nuestro diccionario de palabras en base a los terminos de todos los glosarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "f0a4cfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary(glosarios):\n",
    "    doc_tokens = [[termino for termino in glosario] for glosario in glosarios.values()]\n",
    "    dictionary = corpora.Dictionary(doc_tokens)\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "962e8472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary_glos(glosario):\n",
    "    dictionary = corpora.Dictionary(glosario)\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "30f46d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in glosarios.keys():\n",
    "    if g == \"deportes\":\n",
    "        deportes_dic = create_dictionary_glos([glosarios[g]])\n",
    "    if g == \"salud\":\n",
    "        salud_dic = create_dictionary_glos([glosarios[g]])\n",
    "    if g == \"ciencia\":\n",
    "        ciencia_dic = create_dictionary_glos([glosarios[g]])\n",
    "    else:\n",
    "        politica_dic = create_dictionary_glos([glosarios[g]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "d842df19",
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in glosarios_t.keys():\n",
    "    if g == \"deportes\":\n",
    "        salud_dic_t = create_dictionary_glos([glosarios_t[g]])\n",
    "    if g == \"salud\":\n",
    "        salud_dic_t = create_dictionary_glos([glosarios_t[g]])\n",
    "    if g == \"ciencia\":\n",
    "        ciencia_dic_t = create_dictionary_glos([glosarios_t[g]])\n",
    "    else:\n",
    "        politica_dic_t = create_dictionary_glos([glosarios_t[g]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "c9eae608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<140 unique tokens: ['arabia', 'booker', 'butler', 'campazzo', 'carpena']...>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Dictionary<139 unique tokens: ['aerodinamico', 'booker', 'boston', 'brooklyn', 'cancha']...>\n"
     ]
    }
   ],
   "source": [
    "glosarios_dict = create_dictionary(glosarios)\n",
    "print(glosarios_dict)\n",
    "\n",
    "print(\"-\"*100)\n",
    "\n",
    "glosarios_dict_t = create_dictionary(glosarios_t)\n",
    "print(glosarios_dict_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fccfd6f",
   "metadata": {},
   "source": [
    "Cargamos nuestras noticias de test y las convertimos a un bag of words utilizando nuestro diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "e1cfad72",
   "metadata": {},
   "outputs": [],
   "source": [
    "noticias_train_dataframe = pd.read_csv(\"../Datos/noticias_train.csv\")\n",
    "noticias_test_dataframe = pd.read_csv(\"../Datos/noticias_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "0564676e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>category</th>\n",
       "      <th>n_doc</th>\n",
       "      <th>title</th>\n",
       "      <th>path</th>\n",
       "      <th>link</th>\n",
       "      <th>docs</th>\n",
       "      <th>corpus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57</td>\n",
       "      <td>sports</td>\n",
       "      <td>58</td>\n",
       "      <td>El 'efecto Alonso' transforma a Aston Martin -...</td>\n",
       "      <td>./Datos/Raw_data/sports/58.txt</td>\n",
       "      <td>https://news.google.com/__i/rss/rd/articles/CB...</td>\n",
       "      <td>Mike Krack, director del cuadro británico, con...</td>\n",
       "      <td>Mike Krack director cuadro britanico confesar ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47</td>\n",
       "      <td>sports</td>\n",
       "      <td>48</td>\n",
       "      <td>\"Es el mismo Alonso que con Minardi hace 20 añ...</td>\n",
       "      <td>./Datos/Raw_data/sports/48.txt</td>\n",
       "      <td>https://news.google.com/__i/rss/rd/articles/CB...</td>\n",
       "      <td>De la Rosa defiende la valía de Fernando pese ...</td>\n",
       "      <td>Rosa defender valia Fernando pese 41 ano astur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>sports</td>\n",
       "      <td>29</td>\n",
       "      <td>Devin Booker y Jayson Tatum nombrados jugadore...</td>\n",
       "      <td>./Datos/Raw_data/sports/29.txt</td>\n",
       "      <td>https://news.google.com/__i/rss/rd/articles/CB...</td>\n",
       "      <td>La NBA acaba de publicar sus tradicionales pre...</td>\n",
       "      <td>NBA acabar publicar tradicional premio mensual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>sports</td>\n",
       "      <td>41</td>\n",
       "      <td>Ferrero: “Trabajar con Zverev no fue fácil par...</td>\n",
       "      <td>./Datos/Raw_data/sports/41.txt</td>\n",
       "      <td>https://news.google.com/__i/rss/rd/articles/CB...</td>\n",
       "      <td>Carlos Alcaraz afrontará la temporada 2023 des...</td>\n",
       "      <td>Carlos Alcaraz afrontar temporada 2023 cima te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>sports</td>\n",
       "      <td>19</td>\n",
       "      <td>Golden State acaba con Houston al ritmo de And...</td>\n",
       "      <td>./Datos/Raw_data/sports/19.txt</td>\n",
       "      <td>https://news.google.com/__i/rss/rd/articles/CB...</td>\n",
       "      <td>QuÃ© fue de aquel Andrew Wiggins que llegÃ³ a ...</td>\n",
       "      <td>QuA Andrew Wiggins llega3 objeto burla remonta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>224</td>\n",
       "      <td>politics</td>\n",
       "      <td>45</td>\n",
       "      <td>El Ministerio de Igualdad pacta conMoncloa una...</td>\n",
       "      <td>./Datos/Raw_data/politics/45.txt</td>\n",
       "      <td>https://elpais.com/espana/catalunya/2022-12-12...</td>\n",
       "      <td>Xavier Trias, que fue alcalde de Barcelona ent...</td>\n",
       "      <td>Xavier Trias alcalde Barcelona 2011 2015 Conve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>203</td>\n",
       "      <td>politics</td>\n",
       "      <td>24</td>\n",
       "      <td>El Poder Judicial propone mantener laobligació...</td>\n",
       "      <td>./Datos/Raw_data/politics/24.txt</td>\n",
       "      <td>https://www.eldiario.es/politica/pp-vox-anunci...</td>\n",
       "      <td>Las dos derechas, PP y Vox, han reaccionado de...</td>\n",
       "      <td>derecha PP Vox reaccionar forma airado viernes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>228</td>\n",
       "      <td>politics</td>\n",
       "      <td>49</td>\n",
       "      <td>Sandro Rosell anunciará tras Reyes si sepresen...</td>\n",
       "      <td>./Datos/Raw_data/politics/49.txt</td>\n",
       "      <td>https://elpais.com/espana/madrid/2022-12-12/al...</td>\n",
       "      <td>Javier Ortega Smith lleva un tatuaje en un bra...</td>\n",
       "      <td>Javier Ortega Smith tatuaje brazo poner COE 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>208</td>\n",
       "      <td>politics</td>\n",
       "      <td>29</td>\n",
       "      <td>España y el bloque de países por un límite baj...</td>\n",
       "      <td>./Datos/Raw_data/politics/29.txt</td>\n",
       "      <td>https://www.eldiario.es/politica/fraude-fiscal...</td>\n",
       "      <td>SegÃºn Larsen, las amenazas de âPaul Bonâ ...</td>\n",
       "      <td>segaon Larsen amenaza aPaul Bona mantener mayo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>210</td>\n",
       "      <td>politics</td>\n",
       "      <td>31</td>\n",
       "      <td>El Código Penal incluirá el delito deenriqueci...</td>\n",
       "      <td>./Datos/Raw_data/politics/31.txt</td>\n",
       "      <td>https://www.20minutos.es/noticia/5084149/0/pso...</td>\n",
       "      <td>e El PSOE pedÃ­a exigir un aval judicial para ...</td>\n",
       "      <td>PSOE pedaar exigir aval judicial cambio regist...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index  category  n_doc  \\\n",
       "0       57    sports     58   \n",
       "1       47    sports     48   \n",
       "2       28    sports     29   \n",
       "3       40    sports     41   \n",
       "4       18    sports     19   \n",
       "..     ...       ...    ...   \n",
       "115    224  politics     45   \n",
       "116    203  politics     24   \n",
       "117    228  politics     49   \n",
       "118    208  politics     29   \n",
       "119    210  politics     31   \n",
       "\n",
       "                                                 title  \\\n",
       "0    El 'efecto Alonso' transforma a Aston Martin -...   \n",
       "1    \"Es el mismo Alonso que con Minardi hace 20 añ...   \n",
       "2    Devin Booker y Jayson Tatum nombrados jugadore...   \n",
       "3    Ferrero: “Trabajar con Zverev no fue fácil par...   \n",
       "4    Golden State acaba con Houston al ritmo de And...   \n",
       "..                                                 ...   \n",
       "115  El Ministerio de Igualdad pacta conMoncloa una...   \n",
       "116  El Poder Judicial propone mantener laobligació...   \n",
       "117  Sandro Rosell anunciará tras Reyes si sepresen...   \n",
       "118  España y el bloque de países por un límite baj...   \n",
       "119  El Código Penal incluirá el delito deenriqueci...   \n",
       "\n",
       "                                 path  \\\n",
       "0      ./Datos/Raw_data/sports/58.txt   \n",
       "1      ./Datos/Raw_data/sports/48.txt   \n",
       "2      ./Datos/Raw_data/sports/29.txt   \n",
       "3      ./Datos/Raw_data/sports/41.txt   \n",
       "4      ./Datos/Raw_data/sports/19.txt   \n",
       "..                                ...   \n",
       "115  ./Datos/Raw_data/politics/45.txt   \n",
       "116  ./Datos/Raw_data/politics/24.txt   \n",
       "117  ./Datos/Raw_data/politics/49.txt   \n",
       "118  ./Datos/Raw_data/politics/29.txt   \n",
       "119  ./Datos/Raw_data/politics/31.txt   \n",
       "\n",
       "                                                  link  \\\n",
       "0    https://news.google.com/__i/rss/rd/articles/CB...   \n",
       "1    https://news.google.com/__i/rss/rd/articles/CB...   \n",
       "2    https://news.google.com/__i/rss/rd/articles/CB...   \n",
       "3    https://news.google.com/__i/rss/rd/articles/CB...   \n",
       "4    https://news.google.com/__i/rss/rd/articles/CB...   \n",
       "..                                                 ...   \n",
       "115  https://elpais.com/espana/catalunya/2022-12-12...   \n",
       "116  https://www.eldiario.es/politica/pp-vox-anunci...   \n",
       "117  https://elpais.com/espana/madrid/2022-12-12/al...   \n",
       "118  https://www.eldiario.es/politica/fraude-fiscal...   \n",
       "119  https://www.20minutos.es/noticia/5084149/0/pso...   \n",
       "\n",
       "                                                  docs  \\\n",
       "0    Mike Krack, director del cuadro británico, con...   \n",
       "1    De la Rosa defiende la valía de Fernando pese ...   \n",
       "2    La NBA acaba de publicar sus tradicionales pre...   \n",
       "3    Carlos Alcaraz afrontará la temporada 2023 des...   \n",
       "4    QuÃ© fue de aquel Andrew Wiggins que llegÃ³ a ...   \n",
       "..                                                 ...   \n",
       "115  Xavier Trias, que fue alcalde de Barcelona ent...   \n",
       "116  Las dos derechas, PP y Vox, han reaccionado de...   \n",
       "117  Javier Ortega Smith lleva un tatuaje en un bra...   \n",
       "118  SegÃºn Larsen, las amenazas de âPaul Bonâ ...   \n",
       "119  e El PSOE pedÃ­a exigir un aval judicial para ...   \n",
       "\n",
       "                                                corpus  \n",
       "0    Mike Krack director cuadro britanico confesar ...  \n",
       "1    Rosa defender valia Fernando pese 41 ano astur...  \n",
       "2    NBA acabar publicar tradicional premio mensual...  \n",
       "3    Carlos Alcaraz afrontar temporada 2023 cima te...  \n",
       "4    QuA Andrew Wiggins llega3 objeto burla remonta...  \n",
       "..                                                 ...  \n",
       "115  Xavier Trias alcalde Barcelona 2011 2015 Conve...  \n",
       "116  derecha PP Vox reaccionar forma airado viernes...  \n",
       "117  Javier Ortega Smith tatuaje brazo poner COE 13...  \n",
       "118  segaon Larsen amenaza aPaul Bona mantener mayo...  \n",
       "119  PSOE pedaar exigir aval judicial cambio regist...  \n",
       "\n",
       "[120 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(noticias_train_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "55ee8109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>category</th>\n",
       "      <th>n_doc</th>\n",
       "      <th>title</th>\n",
       "      <th>path</th>\n",
       "      <th>link</th>\n",
       "      <th>docs</th>\n",
       "      <th>corpus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21</td>\n",
       "      <td>sports</td>\n",
       "      <td>22</td>\n",
       "      <td>Houston da la sorpresa remontando y derrotando...</td>\n",
       "      <td>./Datos/Raw_data/sports/22.txt</td>\n",
       "      <td>https://news.google.com/__i/rss/rd/articles/CB...</td>\n",
       "      <td>Jalen Green y los jóvenes Rockets sonrieron an...</td>\n",
       "      <td>Jalen Green joven Rockets sonreir anoche terri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>sports</td>\n",
       "      <td>6</td>\n",
       "      <td>Cristiano Ronaldo exige a la Juventus 19,9 mil...</td>\n",
       "      <td>./Datos/Raw_data/sports/6.txt</td>\n",
       "      <td>https://news.google.com/__i/rss/rd/articles/CB...</td>\n",
       "      <td>El portugués reclama al club italiano los sala...</td>\n",
       "      <td>portugues reclamar club italiano salario pacta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48</td>\n",
       "      <td>sports</td>\n",
       "      <td>49</td>\n",
       "      <td>Alpine comenzó 2022 “en ruinas” - AS</td>\n",
       "      <td>./Datos/Raw_data/sports/49.txt</td>\n",
       "      <td>https://news.google.com/__i/rss/rd/articles/CB...</td>\n",
       "      <td>Alpine llegaba con grandes esperanzas a 2022. ...</td>\n",
       "      <td>Alpine llegar esperanza 2022 expectacion maxim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>sports</td>\n",
       "      <td>14</td>\n",
       "      <td>Boicot de Mbappé al alcohol - AS</td>\n",
       "      <td>./Datos/Raw_data/sports/14.txt</td>\n",
       "      <td>https://news.google.com/__i/rss/rd/articles/CB...</td>\n",
       "      <td>Desde que comenzó el Mundial, Kylian Mbappé ha...</td>\n",
       "      <td>comenzar Mundial Kylian Mbappe seguir patron c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46</td>\n",
       "      <td>sports</td>\n",
       "      <td>47</td>\n",
       "      <td>KTM recibe ayuda de la F1 - AS</td>\n",
       "      <td>./Datos/Raw_data/sports/47.txt</td>\n",
       "      <td>https://news.google.com/__i/rss/rd/articles/CB...</td>\n",
       "      <td>El techo presupuestario de la F1, tan polémico...</td>\n",
       "      <td>techo presupuestario F1 polemico temporada exc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>198</td>\n",
       "      <td>politics</td>\n",
       "      <td>19</td>\n",
       "      <td>El PSOE se queda solo y no consigue limitar la...</td>\n",
       "      <td>./Datos/Raw_data/politics/19.txt</td>\n",
       "      <td>https://elpais.com/america-colombia/2022-12-08...</td>\n",
       "      <td>Castillo durÃ³ apenas un aÃ±o y medio en el po...</td>\n",
       "      <td>Castillo dura3 aao inestabilidad constante cen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>216</td>\n",
       "      <td>politics</td>\n",
       "      <td>37</td>\n",
       "      <td>Petro asegura que Pedro Castillo “se dejó llev...</td>\n",
       "      <td>./Datos/Raw_data/politics/37.txt</td>\n",
       "      <td>https://www.20minutos.es/noticia/5084030/0/rib...</td>\n",
       "      <td>Traducido al mensaje que tambiÃ©n la ComisiÃ³n...</td>\n",
       "      <td>traducido mensaje tambiA ComisiA3n Europea -qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>221</td>\n",
       "      <td>politics</td>\n",
       "      <td>42</td>\n",
       "      <td>Meloni cancela su viaje a Alicante por unagrip...</td>\n",
       "      <td>./Datos/Raw_data/politics/42.txt</td>\n",
       "      <td>https://elpais.com/espana/2022-12-13/bolanos-s...</td>\n",
       "      <td>Por otro lado, el presidente del grupo parlame...</td>\n",
       "      <td>presidente grupo parlamentario Unidas Jaume As...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>195</td>\n",
       "      <td>politics</td>\n",
       "      <td>16</td>\n",
       "      <td>El número de hogares acogidos a la tarifaregul...</td>\n",
       "      <td>./Datos/Raw_data/politics/16.txt</td>\n",
       "      <td>https://elpais.com/ideas/2022-12-09/la-subjeti...</td>\n",
       "      <td>Porque ambos exabruptos tienen defensa subjeti...</td>\n",
       "      <td>exabrupto defensa subjetivo partidario cultura...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>211</td>\n",
       "      <td>politics</td>\n",
       "      <td>32</td>\n",
       "      <td>El Gobierno cambia la ley para desbloquear lar...</td>\n",
       "      <td>./Datos/Raw_data/politics/32.txt</td>\n",
       "      <td>https://www.20minutos.es/noticia/5084166/0/erc...</td>\n",
       "      <td>ERC no se aclara en relaciÃ³n a la posibilidad...</td>\n",
       "      <td>ERC aclarar relacia3n posibilidad organizar re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index  category  n_doc  \\\n",
       "0       21    sports     22   \n",
       "1        5    sports      6   \n",
       "2       48    sports     49   \n",
       "3       13    sports     14   \n",
       "4       46    sports     47   \n",
       "..     ...       ...    ...   \n",
       "115    198  politics     19   \n",
       "116    216  politics     37   \n",
       "117    221  politics     42   \n",
       "118    195  politics     16   \n",
       "119    211  politics     32   \n",
       "\n",
       "                                                 title  \\\n",
       "0    Houston da la sorpresa remontando y derrotando...   \n",
       "1    Cristiano Ronaldo exige a la Juventus 19,9 mil...   \n",
       "2                Alpine comenzó 2022 “en ruinas” - AS    \n",
       "3                    Boicot de Mbappé al alcohol - AS    \n",
       "4                      KTM recibe ayuda de la F1 - AS    \n",
       "..                                                 ...   \n",
       "115  El PSOE se queda solo y no consigue limitar la...   \n",
       "116  Petro asegura que Pedro Castillo “se dejó llev...   \n",
       "117  Meloni cancela su viaje a Alicante por unagrip...   \n",
       "118  El número de hogares acogidos a la tarifaregul...   \n",
       "119  El Gobierno cambia la ley para desbloquear lar...   \n",
       "\n",
       "                                 path  \\\n",
       "0      ./Datos/Raw_data/sports/22.txt   \n",
       "1       ./Datos/Raw_data/sports/6.txt   \n",
       "2      ./Datos/Raw_data/sports/49.txt   \n",
       "3      ./Datos/Raw_data/sports/14.txt   \n",
       "4      ./Datos/Raw_data/sports/47.txt   \n",
       "..                                ...   \n",
       "115  ./Datos/Raw_data/politics/19.txt   \n",
       "116  ./Datos/Raw_data/politics/37.txt   \n",
       "117  ./Datos/Raw_data/politics/42.txt   \n",
       "118  ./Datos/Raw_data/politics/16.txt   \n",
       "119  ./Datos/Raw_data/politics/32.txt   \n",
       "\n",
       "                                                  link  \\\n",
       "0    https://news.google.com/__i/rss/rd/articles/CB...   \n",
       "1    https://news.google.com/__i/rss/rd/articles/CB...   \n",
       "2    https://news.google.com/__i/rss/rd/articles/CB...   \n",
       "3    https://news.google.com/__i/rss/rd/articles/CB...   \n",
       "4    https://news.google.com/__i/rss/rd/articles/CB...   \n",
       "..                                                 ...   \n",
       "115  https://elpais.com/america-colombia/2022-12-08...   \n",
       "116  https://www.20minutos.es/noticia/5084030/0/rib...   \n",
       "117  https://elpais.com/espana/2022-12-13/bolanos-s...   \n",
       "118  https://elpais.com/ideas/2022-12-09/la-subjeti...   \n",
       "119  https://www.20minutos.es/noticia/5084166/0/erc...   \n",
       "\n",
       "                                                  docs  \\\n",
       "0    Jalen Green y los jóvenes Rockets sonrieron an...   \n",
       "1    El portugués reclama al club italiano los sala...   \n",
       "2    Alpine llegaba con grandes esperanzas a 2022. ...   \n",
       "3    Desde que comenzó el Mundial, Kylian Mbappé ha...   \n",
       "4    El techo presupuestario de la F1, tan polémico...   \n",
       "..                                                 ...   \n",
       "115  Castillo durÃ³ apenas un aÃ±o y medio en el po...   \n",
       "116  Traducido al mensaje que tambiÃ©n la ComisiÃ³n...   \n",
       "117  Por otro lado, el presidente del grupo parlame...   \n",
       "118  Porque ambos exabruptos tienen defensa subjeti...   \n",
       "119  ERC no se aclara en relaciÃ³n a la posibilidad...   \n",
       "\n",
       "                                                corpus  \n",
       "0    Jalen Green joven Rockets sonreir anoche terri...  \n",
       "1    portugues reclamar club italiano salario pacta...  \n",
       "2    Alpine llegar esperanza 2022 expectacion maxim...  \n",
       "3    comenzar Mundial Kylian Mbappe seguir patron c...  \n",
       "4    techo presupuestario F1 polemico temporada exc...  \n",
       "..                                                 ...  \n",
       "115  Castillo dura3 aao inestabilidad constante cen...  \n",
       "116  traducido mensaje tambiA ComisiA3n Europea -qu...  \n",
       "117  presidente grupo parlamentario Unidas Jaume As...  \n",
       "118  exabrupto defensa subjetivo partidario cultura...  \n",
       "119  ERC aclarar relacia3n posibilidad organizar re...  \n",
       "\n",
       "[120 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(noticias_test_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "468e5aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_train_corpus = noticias_train_dataframe[\"corpus\"]\n",
    "norm_test_corpus = noticias_test_dataframe[\"corpus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "3854731f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = noticias_train_dataframe[\"category\"]\n",
    "test_labels = noticias_test_dataframe[\"category\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07d7e208",
   "metadata": {},
   "source": [
    "# 2. BAG OF WORDS (BOW) Y VOCABULARIO\n",
    "Obtenemos las bolsas de palabras y el vocabulario (glosario) que emplea en los cojuntos de test y train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "91b58db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bag_of_words(docs_list, dictionary):\n",
    "    doc_tokens = [simple_preprocess(corpus) for corpus in docs_list]\n",
    "    docs_bow = [dictionary.doc2bow(doc) for doc in doc_tokens]\n",
    "    return docs_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "f3dfdb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_words(bow, dictionary):\n",
    "    \"\"\" Funcion para obtener un bow con palabras\n",
    "    - bow: docs_bow\n",
    "    - dictionary: glosario\n",
    "    \"\"\"\n",
    "\n",
    "    tok = []\n",
    "    counts = []\n",
    "    \n",
    "    for i in range(len(bow)-1):\n",
    "        \n",
    "        for j in bow[i]:\n",
    "            \n",
    "            token_ids = j[0]\n",
    "            token_count = j[1]\n",
    "\n",
    "            palabra = dictionary[token_ids]\n",
    "            \n",
    "            if palabra not in tok:\n",
    "                tok.append(palabra)\n",
    "                counts.append(token_count)\n",
    "    \n",
    "\n",
    "\n",
    "    d = {t:count for t, count in zip(tok, counts)}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "d4963bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_vocab(bow, dictionary):\n",
    "    \"\"\" Funcion para obtener un bow con palabras\n",
    "    - bow: docs_bow\n",
    "    - dictionary: glosario\n",
    "    \"\"\"\n",
    "\n",
    "    tok = []\n",
    "    ids = []\n",
    "    \n",
    "    for i in range(len(bow)-1):\n",
    "        \n",
    "        for j in bow[i]:\n",
    "            \n",
    "            token_ids = j[0]\n",
    "\n",
    "            palabra = dictionary[token_ids]\n",
    "            \n",
    "            if palabra not in tok:\n",
    "                tok.append(palabra)\n",
    "                ids.append(token_ids)\n",
    "    \n",
    "\n",
    "\n",
    "    d = {count:t for t, count in zip(tok, ids)}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "7fd6505d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def voca(dictionary):\n",
    "    \"\"\" Funcion para obtener un bow con palabras\n",
    "    - bow: docs_bow\n",
    "    - dictionary: glosario\n",
    "    \"\"\"\n",
    "\n",
    "    tok = []\n",
    "    ids = []\n",
    "    \n",
    "    for i in dictionary.keys():\n",
    "\n",
    "        palabra = dictionary[i]\n",
    "        \n",
    "        if palabra not in tok:\n",
    "            tok.append(palabra)\n",
    "            ids.append(i)\n",
    "    \n",
    "\n",
    "\n",
    "    d = {t:count for t, count in zip(tok, ids)}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325a645a",
   "metadata": {},
   "source": [
    "Ahora es momento de crear una bag of words para cada noticia en base a nuestro diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "bc449e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arabia': 0,\n",
       " 'booker': 1,\n",
       " 'butler': 2,\n",
       " 'campazzo': 3,\n",
       " 'carpena': 4,\n",
       " 'cd': 5,\n",
       " 'club': 6,\n",
       " 'collins': 7,\n",
       " 'cristiano': 8,\n",
       " 'cuento': 9,\n",
       " 'djokovic': 10,\n",
       " 'domenicali': 11,\n",
       " 'enrique': 12,\n",
       " 'exencion': 13,\n",
       " 'falso': 14,\n",
       " 'florentino': 15,\n",
       " 'formato': 16,\n",
       " 'gavi': 17,\n",
       " 'gigante': 18,\n",
       " 'gonzalez': 19,\n",
       " 'gonzalo': 20,\n",
       " 'heredero': 21,\n",
       " 'indycar': 22,\n",
       " 'jamas': 23,\n",
       " 'jimmy': 24,\n",
       " 'juventus': 25,\n",
       " 'luis': 26,\n",
       " 'madrid': 27,\n",
       " 'magnussen': 28,\n",
       " 'marbella': 29,\n",
       " 'mbappe': 30,\n",
       " 'mclaren': 31,\n",
       " 'nets': 32,\n",
       " 'pagar': 33,\n",
       " 'palou': 34,\n",
       " 'porra': 35,\n",
       " 'premio': 36,\n",
       " 'real': 37,\n",
       " 'reserva': 38,\n",
       " 'residencia': 39,\n",
       " 'ronaldo': 40,\n",
       " 'rosa': 41,\n",
       " 'rotacion': 42,\n",
       " 'serbia': 43,\n",
       " 'serbio': 44,\n",
       " 'smash': 45,\n",
       " 'steiner': 46,\n",
       " 'tatum': 47,\n",
       " 'tenerife': 48,\n",
       " 'verdasco': 49,\n",
       " 'aceite': 50,\n",
       " 'afiliado': 51,\n",
       " 'anemia': 52,\n",
       " 'arterial': 53,\n",
       " 'cafa': 54,\n",
       " 'cannabis': 55,\n",
       " 'condiloma': 56,\n",
       " 'coosalud': 57,\n",
       " 'creatividad': 58,\n",
       " 'dash': 59,\n",
       " 'epilepsia': 60,\n",
       " 'eps': 61,\n",
       " 'folcodina': 62,\n",
       " 'genital': 63,\n",
       " 'ips': 64,\n",
       " 'lavar': 65,\n",
       " 'lobo': 66,\n",
       " 'manada': 67,\n",
       " 'marfan': 68,\n",
       " 'mascarilla': 69,\n",
       " 'parasito': 70,\n",
       " 'pet': 71,\n",
       " 'presion': 72,\n",
       " 'pulmonar': 73,\n",
       " 'quiraorgica': 74,\n",
       " 'tc': 75,\n",
       " 'unicef': 76,\n",
       " 'verruga': 77,\n",
       " 'vrs': 78,\n",
       " 'zumo': 79,\n",
       " 'agujero': 80,\n",
       " 'bankman': 81,\n",
       " 'banyoles': 82,\n",
       " 'barrera': 83,\n",
       " 'captura': 84,\n",
       " 'cernan': 85,\n",
       " 'cola': 86,\n",
       " 'estampido': 87,\n",
       " 'fauci': 88,\n",
       " 'fried': 89,\n",
       " 'ftx': 90,\n",
       " 'gemanidas': 91,\n",
       " 'honor': 92,\n",
       " 'ingeniero': 93,\n",
       " 'latigo': 94,\n",
       " 'leo': 95,\n",
       " 'llama': 96,\n",
       " 'magic': 97,\n",
       " 'menta': 98,\n",
       " 'navidad': 99,\n",
       " 'neandertal': 100,\n",
       " 'nif': 101,\n",
       " 'perfecto': 102,\n",
       " 'reencuentro': 103,\n",
       " 'regalo': 104,\n",
       " 'segundo': 105,\n",
       " 'supermasivo': 106,\n",
       " 'supersa': 107,\n",
       " 'vacao': 108,\n",
       " 'york': 109,\n",
       " 'anunciara': 110,\n",
       " 'autoritariaa': 111,\n",
       " 'benito': 112,\n",
       " 'ciudadano': 113,\n",
       " 'consulta': 114,\n",
       " 'coraza': 115,\n",
       " 'dema': 116,\n",
       " 'demarcacia': 117,\n",
       " 'ex': 118,\n",
       " 'ja': 119,\n",
       " 'juarez': 120,\n",
       " 'juventud': 121,\n",
       " 'multilateral': 122,\n",
       " 'negativo': 123,\n",
       " 'nero': 124,\n",
       " 'nicolas': 125,\n",
       " 'patrimonio': 126,\n",
       " 'perao': 127,\n",
       " 'registral': 128,\n",
       " 'reyes': 129,\n",
       " 'rosell': 130,\n",
       " 'rufian': 131,\n",
       " 'ruido': 132,\n",
       " 'sentir': 133,\n",
       " 'sinema': 134,\n",
       " 'trias': 135,\n",
       " 'turismo': 136,\n",
       " 'ucraniano': 137,\n",
       " 'unilateral': 138,\n",
       " 'vuelo': 139}"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulario_train = voca(glosarios_dict)\n",
    "vocabulario_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "fcd89c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aerodinamico': 0,\n",
       " 'booker': 1,\n",
       " 'boston': 2,\n",
       " 'brooklyn': 3,\n",
       " 'cancha': 4,\n",
       " 'carlos': 5,\n",
       " 'celtics': 6,\n",
       " 'colombia': 7,\n",
       " 'dolares': 8,\n",
       " 'doncic': 9,\n",
       " 'enrique': 10,\n",
       " 'estabilidad': 11,\n",
       " 'ferrero': 12,\n",
       " 'garden': 13,\n",
       " 'gasto': 14,\n",
       " 'gimenez': 15,\n",
       " 'golden': 16,\n",
       " 'grada': 17,\n",
       " 'hernanga': 18,\n",
       " 'horford': 19,\n",
       " 'james': 20,\n",
       " 'juancho': 21,\n",
       " 'krack': 22,\n",
       " 'ktm': 23,\n",
       " 'lebron': 24,\n",
       " 'leclerc': 25,\n",
       " 'lesia': 26,\n",
       " 'luis': 27,\n",
       " 'mans': 28,\n",
       " 'marko': 29,\n",
       " 'mez': 30,\n",
       " 'mirar': 31,\n",
       " 'moto': 32,\n",
       " 'ofensivo': 33,\n",
       " 'pts': 34,\n",
       " 'raptors': 35,\n",
       " 'reb': 36,\n",
       " 'resistencia': 37,\n",
       " 'rez': 38,\n",
       " 'sancionar': 39,\n",
       " 'seguidor': 40,\n",
       " 'siebert': 41,\n",
       " 'stakhovsky': 42,\n",
       " 'state': 43,\n",
       " 'suarez': 44,\n",
       " 'suns': 45,\n",
       " 'tristeza': 46,\n",
       " 'warren': 47,\n",
       " 'warriors': 48,\n",
       " 'wiggins': 49,\n",
       " 'adolescente': 50,\n",
       " 'alga': 51,\n",
       " 'antidepresivo': 52,\n",
       " 'aprovechar': 53,\n",
       " 'atras': 54,\n",
       " 'automatico': 55,\n",
       " 'bacteria': 56,\n",
       " 'bronquiolitis': 57,\n",
       " 'cabello': 58,\n",
       " 'congelado': 59,\n",
       " 'cuadro': 60,\n",
       " 'depresion': 61,\n",
       " 'entrenar': 62,\n",
       " 'esconder': 63,\n",
       " 'estigma': 64,\n",
       " 'fruto': 65,\n",
       " 'gotlib': 66,\n",
       " 'morder': 67,\n",
       " 'niaos': 68,\n",
       " 'nuez': 69,\n",
       " 'orina': 70,\n",
       " 'receta': 71,\n",
       " 'rehabilitacia': 72,\n",
       " 'seco': 73,\n",
       " 'secuela': 74,\n",
       " 'sida': 75,\n",
       " 'strep': 76,\n",
       " 'temporada': 77,\n",
       " 'vih': 78,\n",
       " 'wakame': 79,\n",
       " 'agujero': 80,\n",
       " 'alineacia': 81,\n",
       " 'almanaque': 82,\n",
       " 'alphacode': 83,\n",
       " 'antageno': 84,\n",
       " 'canaria': 85,\n",
       " 'congelacia': 86,\n",
       " 'congelar': 87,\n",
       " 'cuantico': 88,\n",
       " 'escarcha': 89,\n",
       " 'estruendo': 90,\n",
       " 'fraa': 91,\n",
       " 'gripe': 92,\n",
       " 'helada': 93,\n",
       " 'idioma': 94,\n",
       " 'isidro': 95,\n",
       " 'llnl': 96,\n",
       " 'palabrota': 97,\n",
       " 'programador': 98,\n",
       " 'quipus': 99,\n",
       " 'relatividad': 100,\n",
       " 'sal': 101,\n",
       " 'santo': 102,\n",
       " 'sevilla': 103,\n",
       " 'ska': 104,\n",
       " 'sonido': 105,\n",
       " 'subtipo': 106,\n",
       " 'telescopio': 107,\n",
       " 'vacuna': 108,\n",
       " 'adjudicacia': 109,\n",
       " 'ahorro': 110,\n",
       " 'almeida': 111,\n",
       " 'animal': 112,\n",
       " 'auxiliar': 113,\n",
       " 'ayres': 114,\n",
       " 'castillo': 115,\n",
       " 'catalunya': 116,\n",
       " 'cheque': 117,\n",
       " 'concejala': 118,\n",
       " 'esparza': 119,\n",
       " 'gandia': 120,\n",
       " 'ilacito': 121,\n",
       " 'insulto': 122,\n",
       " 'isabel': 123,\n",
       " 'mexicano': 124,\n",
       " 'mocia': 125,\n",
       " 'navarra': 126,\n",
       " 'ndum': 127,\n",
       " 'nduma': 128,\n",
       " 'nombramiento': 129,\n",
       " 'obrador': 130,\n",
       " 'procesado': 131,\n",
       " 'refera': 132,\n",
       " 'rodraguez': 133,\n",
       " 'silva': 134,\n",
       " 'smith': 135,\n",
       " 'torra': 136,\n",
       " 'upn': 137,\n",
       " 'urgente': 138}"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulario_test = voca(glosarios_dict_t)\n",
    "vocabulario_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "f1026623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[(39, 1), (77, 2)]\n"
     ]
    }
   ],
   "source": [
    "#https://radimrehurek.com/gensim/corpora/dictionary.html\n",
    "\n",
    "docs_bow = create_bag_of_words(noticias_train_dataframe[\"corpus\"].values, glosarios_dict)\n",
    "print(docs_bow[19])\n",
    "# [(token_id, token_count)]\n",
    "\n",
    "print(\"-\"*100)\n",
    "\n",
    "docs_bow_t = create_bag_of_words(noticias_test_dataframe[\"corpus\"].values, glosarios_dict_t)\n",
    "print(docs_bow_t[19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "c212a4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{36: 'premio', 41: 'rosa', 133: 'sentir', 1: 'booker', 47: 'tatum', 84: 'captura', 10: 'djokovic', 27: 'madrid', 44: 'serbio', 8: 'cristiano', 9: 'cuento', 12: 'enrique', 26: 'luis', 99: 'navidad', 22: 'indycar', 31: 'mclaren', 34: 'palou', 38: 'reserva', 102: 'perfecto', 19: 'gonzalez', 20: 'gonzalo', 72: 'presion', 97: 'magic', 103: 'reencuentro', 105: 'segundo', 114: 'consulta', 93: 'ingeniero', 125: 'nicolas', 3: 'campazzo', 6: 'club', 37: 'real', 118: 'ex', 0: 'arabia', 16: 'formato', 17: 'gavi', 35: 'porra', 95: 'leo', 5: 'cd', 48: 'tenerife', 92: 'honor', 28: 'magnussen', 46: 'steiner', 42: 'rotacion', 11: 'domenicali', 2: 'butler', 24: 'jimmy', 109: 'york', 139: 'vuelo', 123: 'negativo', 14: 'falso', 18: 'gigante', 23: 'jamas', 45: 'smash', 119: 'ja', 76: 'unicef', 33: 'pagar', 86: 'cola', 129: 'reyes', 132: 'ruido', 121: 'juventud', 52: 'anemia', 116: 'dema', 79: 'zumo', 56: 'condiloma', 63: 'genital', 77: 'verruga', 65: 'lavar', 78: 'vrs', 51: 'afiliado', 57: 'coosalud', 61: 'eps', 64: 'ips', 55: 'cannabis', 58: 'creatividad', 50: 'aceite', 73: 'pulmonar', 71: 'pet', 75: 'tc', 59: 'dash', 81: 'bankman', 89: 'fried', 90: 'ftx', 108: 'vacao', 110: 'anunciara', 66: 'lobo', 124: 'nero', 107: 'supersa', 113: 'ciudadano', 80: 'agujero', 83: 'barrera', 70: 'parasito', 85: 'cernan', 101: 'nif', 96: 'llama', 82: 'banyoles', 98: 'menta', 100: 'neandertal', 112: 'benito', 120: 'juarez', 126: 'patrimonio', 117: 'demarcacia', 13: 'exencion', 127: 'perao', 135: 'trias', 136: 'turismo', 138: 'unilateral', 29: 'marbella', 130: 'rosell', 134: 'sinema', 131: 'rufian', 74: 'quiraorgica', 115: 'coraza', 128: 'registral', 111: 'autoritariaa', 104: 'regalo'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{1: 'booker', 33: 'ofensivo', 45: 'suns', 53: 'aprovechar', 135: 'smith', 77: 'temporada', 63: 'esconder', 0: 'aerodinamico', 23: 'ktm', 32: 'moto', 25: 'leclerc', 29: 'marko', 31: 'mirar', 2: 'boston', 4: 'cancha', 65: 'fruto', 3: 'brooklyn', 47: 'warren', 6: 'celtics', 8: 'dolares', 11: 'estabilidad', 19: 'horford', 20: 'james', 28: 'mans', 37: 'resistencia', 10: 'enrique', 17: 'grada', 27: 'luis', 40: 'seguidor', 124: 'mexicano', 15: 'gimenez', 39: 'sancionar', 41: 'siebert', 46: 'tristeza', 7: 'colombia', 14: 'gasto', 42: 'stakhovsky', 24: 'lebron', 34: 'pts', 36: 'reb', 9: 'doncic', 13: 'garden', 60: 'cuadro', 38: 'rez', 75: 'sida', 78: 'vih', 62: 'entrenar', 92: 'gripe', 69: 'nuez', 71: 'receta', 73: 'seco', 101: 'sal', 68: 'niaos', 58: 'cabello', 57: 'bronquiolitis', 52: 'antidepresivo', 61: 'depresion', 44: 'suarez', 64: 'estigma', 94: 'idioma', 115: 'castillo', 50: 'adolescente', 66: 'gotlib', 59: 'congelado', 112: 'animal', 56: 'bacteria', 76: 'strep', 74: 'secuela', 123: 'isabel', 54: 'atras', 81: 'alineacia', 82: 'almanaque', 89: 'escarcha', 91: 'fraa', 93: 'helada', 100: 'relatividad', 85: 'canaria', 90: 'estruendo', 88: 'cuantico', 103: 'sevilla', 80: 'agujero', 26: 'lesia', 87: 'congelar', 133: 'rodraguez', 105: 'sonido', 99: 'quipus', 5: 'carlos', 84: 'antageno', 106: 'subtipo', 108: 'vacuna', 55: 'automatico', 107: 'telescopio', 116: 'catalunya', 129: 'nombramiento', 86: 'congelacia', 117: 'cheque', 113: 'auxiliar', 121: 'ilacito', 127: 'ndum', 132: 'refera', 138: 'urgente', 111: 'almeida', 130: 'obrador', 110: 'ahorro', 125: 'mocia', 122: 'insulto'}\n"
     ]
    }
   ],
   "source": [
    "vocab_train = obtain_vocab(docs_bow, glosarios_dict)\n",
    "print(vocab_train)\n",
    "# [(token_id, token_count)]\n",
    "\n",
    "print(\"-\"*100)\n",
    "\n",
    "vocab_test = obtain_vocab(docs_bow_t, glosarios_dict_t)\n",
    "print(vocab_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "47533c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'premio': 1, 'rosa': 3, 'sentir': 5, 'booker': 3, 'tatum': 3, 'captura': 1, 'djokovic': 7, 'madrid': 1, 'serbio': 1, 'cristiano': 1, 'cuento': 4, 'enrique': 1, 'luis': 3, 'navidad': 2, 'indycar': 3, 'mclaren': 7, 'palou': 3, 'reserva': 5, 'perfecto': 2, 'gonzalez': 5, 'gonzalo': 5, 'presion': 2, 'magic': 2, 'reencuentro': 1, 'segundo': 1, 'consulta': 2, 'ingeniero': 1, 'nicolas': 1, 'campazzo': 5, 'club': 1, 'real': 7, 'ex': 1, 'arabia': 3, 'formato': 5, 'gavi': 4, 'porra': 4, 'leo': 1, 'cd': 4, 'tenerife': 6, 'honor': 1, 'magnussen': 6, 'steiner': 6, 'rotacion': 3, 'domenicali': 4, 'butler': 3, 'jimmy': 2, 'york': 1, 'vuelo': 1, 'negativo': 1, 'falso': 3, 'gigante': 2, 'jamas': 3, 'smash': 3, 'ja': 1, 'unicef': 3, 'pagar': 2, 'cola': 1, 'reyes': 2, 'ruido': 1, 'juventud': 1, 'anemia': 7, 'dema': 1, 'zumo': 14, 'condiloma': 8, 'genital': 15, 'verruga': 13, 'lavar': 3, 'vrs': 5, 'afiliado': 6, 'coosalud': 5, 'eps': 6, 'ips': 6, 'cannabis': 20, 'creatividad': 15, 'aceite': 2, 'pulmonar': 5, 'pet': 6, 'tc': 5, 'dash': 6, 'bankman': 7, 'fried': 7, 'ftx': 6, 'vacao': 27, 'anunciara': 2, 'lobo': 1, 'nero': 6, 'supersa': 3, 'ciudadano': 1, 'agujero': 2, 'barrera': 1, 'parasito': 7, 'cernan': 6, 'nif': 5, 'llama': 11, 'banyoles': 6, 'menta': 4, 'neandertal': 6, 'benito': 1, 'juarez': 1, 'patrimonio': 2, 'demarcacia': 5, 'exencion': 2, 'perao': 2, 'trias': 1, 'turismo': 1, 'unilateral': 1, 'marbella': 2, 'rosell': 3, 'sinema': 11, 'rufian': 7, 'quiraorgica': 1, 'coraza': 1, 'registral': 4, 'autoritariaa': 4, 'regalo': 1}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'booker': 4, 'ofensivo': 3, 'suns': 3, 'aprovechar': 1, 'smith': 1, 'temporada': 7, 'esconder': 1, 'aerodinamico': 5, 'ktm': 6, 'moto': 6, 'leclerc': 1, 'marko': 1, 'mirar': 1, 'boston': 1, 'cancha': 2, 'fruto': 1, 'brooklyn': 3, 'warren': 5, 'celtics': 3, 'dolares': 2, 'estabilidad': 3, 'horford': 3, 'james': 1, 'mans': 5, 'resistencia': 1, 'enrique': 2, 'grada': 2, 'luis': 2, 'seguidor': 3, 'mexicano': 1, 'gimenez': 3, 'sancionar': 3, 'siebert': 3, 'tristeza': 1, 'colombia': 4, 'gasto': 7, 'stakhovsky': 9, 'lebron': 2, 'pts': 5, 'reb': 5, 'doncic': 7, 'garden': 4, 'cuadro': 1, 'rez': 4, 'sida': 3, 'vih': 17, 'entrenar': 7, 'gripe': 1, 'nuez': 9, 'receta': 4, 'seco': 5, 'sal': 1, 'niaos': 2, 'cabello': 16, 'bronquiolitis': 3, 'antidepresivo': 11, 'depresion': 8, 'suarez': 2, 'estigma': 1, 'idioma': 1, 'castillo': 2, 'adolescente': 5, 'gotlib': 5, 'congelado': 4, 'animal': 1, 'bacteria': 6, 'strep': 5, 'secuela': 1, 'isabel': 1, 'atras': 1, 'alineacia': 2, 'almanaque': 2, 'escarcha': 2, 'fraa': 3, 'helada': 2, 'relatividad': 2, 'canaria': 1, 'estruendo': 1, 'cuantico': 1, 'sevilla': 3, 'agujero': 12, 'lesia': 1, 'congelar': 1, 'rodraguez': 3, 'sonido': 4, 'quipus': 10, 'carlos': 1, 'antageno': 6, 'subtipo': 6, 'vacuna': 16, 'automatico': 1, 'telescopio': 1, 'catalunya': 2, 'nombramiento': 5, 'congelacia': 3, 'cheque': 5, 'auxiliar': 4, 'ilacito': 2, 'ndum': 1, 'refera': 1, 'urgente': 1, 'almeida': 1, 'obrador': 4, 'ahorro': 1, 'mocia': 1, 'insulto': 6}\n"
     ]
    }
   ],
   "source": [
    "bow_train = obtain_words(docs_bow, glosarios_dict)\n",
    "print(bow_train)\n",
    "# [(token_id, token_count)]\n",
    "\n",
    "print(\"-\"*100)\n",
    "\n",
    "bow_test = obtain_words(docs_bow_t, glosarios_dict_t)\n",
    "print(bow_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9ae7b70",
   "metadata": {},
   "source": [
    "# 3. EXTRACCIÓN DE CARACTERISTICAS\n",
    "Instanciamos los vectorizadores para obtener las características BoW y TF-IDF.  \n",
    "Usamos el parámetro max_df=0.9 para eliminar los stop-words como las palabras que aparecen al menos en el 90% de los documentos y el parámetro min_df=0.01 para eliminar las palabras que no aparecen al menos en un 1% de los documentos.\\\n",
    "\n",
    "Usamos el modelo `TfidfTransformer` para calcular la matriz TF-IDF a partir del BoW y no tener que repetir todo el entrenamiento.\n",
    "\n",
    "Para calcular los modelos basados en WV usamos el modelo gloVe pre-entrenado en `spaCy`. Calculamos dos modelos basados en word-vectors:  \n",
    "* El vector promedio de los WV de todos los tokens con el mismo peso para todas las palabras.  \n",
    "* Ponderando el WV de cada palabra por el término de frecuencia inversa de documento (IDF).  \n",
    "\n",
    "Definimos las funciones para calcular estas dos matrices de características:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f2faaee",
   "metadata": {},
   "source": [
    "Sacamos las caracteristicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "83a98679",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download es_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "36c453fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"es_core_news_lg\") #Mejor modelo optimizado para la CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "debd5901",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BoW\n",
    "bow_vectorizer = CountVectorizer(min_df=0.01, max_df=0.9, vocabulary=vocabulario_train)\n",
    "# bow_vectorizer_t = CountVectorizer(min_df=0.01, max_df=0.9, vocabulary=vocabulario_test)\n",
    "\n",
    "\n",
    "#Tf-idf\n",
    "tfidf_vectorizer = TfidfTransformer()\n",
    "\n",
    "#Funciones de WV.\n",
    "def averaged_word_vectorizer(corpus):\n",
    "    '''Aplica la función de cálculo del WE promedio a todos los\n",
    "    documentos del corpus (cada doc es una lista de tokens)'''\n",
    "    features = [nlp(doc).vector\n",
    "                    for doc in corpus]\n",
    "    return np.array(features)\n",
    "\n",
    "def tfidf_wtd_avg_word_vectors(doc, word_tfidf_map):\n",
    "    '''Aplica la función de cálculo del WE ponderado por TF-IDF\n",
    "    a un documento (como lista de tokens)'''\n",
    "    tokens = doc.split()\n",
    "\n",
    "    feature_vector = np.zeros((nlp.vocab.vectors_length,),dtype=\"float64\")\n",
    "    wts = 0.      \n",
    "    for word in tokens:\n",
    "        if nlp.vocab[word].has_vector and word_tfidf_map.get(word, 0): #sólo considera palabras conocidas\n",
    "            weighted_word_vector = word_tfidf_map[word] * nlp.vocab[word].vector\n",
    "            wts = wts + 1\n",
    "            feature_vector = np.add(feature_vector, weighted_word_vector)\n",
    "    if wts:\n",
    "        feature_vector = np.divide(feature_vector, wts)\n",
    "        \n",
    "    return feature_vector\n",
    "    \n",
    "def tfidf_weighted_averaged_word_vectorizer(corpus, word_tfidf_map):\n",
    "    '''Aplica la función de cálculo del WE ponderado por TF-IDF a todos los\n",
    "    documentos del corpus (cada doc es una lista de tokens)'''                                       \n",
    "    features = [tfidf_wtd_avg_word_vectors(doc, word_tfidf_map)\n",
    "                    for doc in corpus]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "a0ac59c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# características bag of words\n",
    "bow_train_features = bow_vectorizer.fit_transform(norm_train_corpus)  \n",
    "bow_test_features = bow_vectorizer.transform(norm_test_corpus) \n",
    "\n",
    "# características tfidf (a partir del BoW)\n",
    "tfidf_train_features = tfidf_vectorizer.fit_transform(bow_train_features)\n",
    "tfidf_test_features = tfidf_vectorizer.transform(bow_test_features)    \n",
    "\n",
    "# características averaged word vector\n",
    "avg_wv_train_features = averaged_word_vectorizer(norm_train_corpus)                \n",
    "avg_wv_test_features = averaged_word_vectorizer(norm_test_corpus)      \n",
    "\n",
    "# características tfidf weighted averaged word vector\n",
    "word_tfidf_map = {key:value for (key, value) in zip(bow_vectorizer.get_feature_names_out(), tfidf_vectorizer.idf_)}\n",
    "\n",
    "tfidf_wv_train_features = tfidf_weighted_averaged_word_vectorizer(norm_train_corpus, word_tfidf_map)\n",
    "\n",
    "tfidf_wv_test_features = tfidf_weighted_averaged_word_vectorizer(norm_test_corpus, word_tfidf_map) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "bf60bbf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 140)"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "ded6c62b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['arabia', 'booker', 'butler', 'campazzo', 'carpena', 'cd', 'club',\n",
       "       'collins', 'cristiano', 'cuento', 'djokovic', 'domenicali',\n",
       "       'enrique', 'exencion', 'falso', 'florentino', 'formato', 'gavi',\n",
       "       'gigante', 'gonzalez', 'gonzalo', 'heredero', 'indycar', 'jamas',\n",
       "       'jimmy', 'juventus', 'luis', 'madrid', 'magnussen', 'marbella',\n",
       "       'mbappe', 'mclaren', 'nets', 'pagar', 'palou', 'porra', 'premio',\n",
       "       'real', 'reserva', 'residencia', 'ronaldo', 'rosa', 'rotacion',\n",
       "       'serbia', 'serbio', 'smash', 'steiner', 'tatum', 'tenerife',\n",
       "       'verdasco', 'aceite', 'afiliado', 'anemia', 'arterial', 'cafa',\n",
       "       'cannabis', 'condiloma', 'coosalud', 'creatividad', 'dash',\n",
       "       'epilepsia', 'eps', 'folcodina', 'genital', 'ips', 'lavar', 'lobo',\n",
       "       'manada', 'marfan', 'mascarilla', 'parasito', 'pet', 'presion',\n",
       "       'pulmonar', 'quiraorgica', 'tc', 'unicef', 'verruga', 'vrs',\n",
       "       'zumo', 'agujero', 'bankman', 'banyoles', 'barrera', 'captura',\n",
       "       'cernan', 'cola', 'estampido', 'fauci', 'fried', 'ftx',\n",
       "       'gemanidas', 'honor', 'ingeniero', 'latigo', 'leo', 'llama',\n",
       "       'magic', 'menta', 'navidad', 'neandertal', 'nif', 'perfecto',\n",
       "       'reencuentro', 'regalo', 'segundo', 'supermasivo', 'supersa',\n",
       "       'vacao', 'york', 'anunciara', 'autoritariaa', 'benito',\n",
       "       'ciudadano', 'consulta', 'coraza', 'dema', 'demarcacia', 'ex',\n",
       "       'ja', 'juarez', 'juventud', 'multilateral', 'negativo', 'nero',\n",
       "       'nicolas', 'patrimonio', 'perao', 'registral', 'reyes', 'rosell',\n",
       "       'rufian', 'ruido', 'sentir', 'sinema', 'trias', 'turismo',\n",
       "       'ucraniano', 'unilateral', 'vuelo'], dtype=object)"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "6035b93d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 140)"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "59f2fccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['x0', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10',\n",
       "       'x11', 'x12', 'x13', 'x14', 'x15', 'x16', 'x17', 'x18', 'x19',\n",
       "       'x20', 'x21', 'x22', 'x23', 'x24', 'x25', 'x26', 'x27', 'x28',\n",
       "       'x29', 'x30', 'x31', 'x32', 'x33', 'x34', 'x35', 'x36', 'x37',\n",
       "       'x38', 'x39', 'x40', 'x41', 'x42', 'x43', 'x44', 'x45', 'x46',\n",
       "       'x47', 'x48', 'x49', 'x50', 'x51', 'x52', 'x53', 'x54', 'x55',\n",
       "       'x56', 'x57', 'x58', 'x59', 'x60', 'x61', 'x62', 'x63', 'x64',\n",
       "       'x65', 'x66', 'x67', 'x68', 'x69', 'x70', 'x71', 'x72', 'x73',\n",
       "       'x74', 'x75', 'x76', 'x77', 'x78', 'x79', 'x80', 'x81', 'x82',\n",
       "       'x83', 'x84', 'x85', 'x86', 'x87', 'x88', 'x89', 'x90', 'x91',\n",
       "       'x92', 'x93', 'x94', 'x95', 'x96', 'x97', 'x98', 'x99', 'x100',\n",
       "       'x101', 'x102', 'x103', 'x104', 'x105', 'x106', 'x107', 'x108',\n",
       "       'x109', 'x110', 'x111', 'x112', 'x113', 'x114', 'x115', 'x116',\n",
       "       'x117', 'x118', 'x119', 'x120', 'x121', 'x122', 'x123', 'x124',\n",
       "       'x125', 'x126', 'x127', 'x128', 'x129', 'x130', 'x131', 'x132',\n",
       "       'x133', 'x134', 'x135', 'x136', 'x137', 'x138', 'x139'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "b8847fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 300)"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_wv_train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "03d18c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 300)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_wv_train_features.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e01af62e",
   "metadata": {},
   "source": [
    "## 4. CLASIFICADORES\n",
    "Aplicamos distintos clasificadores a cada modelo para ver cuál funciona mejor con nuestros datos. Primero definimos unas funciones para entrenar y medir el rendimiento de los clasificadores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "72dd004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(true_labels, predicted_labels):\n",
    "    \"\"\"Calculamos distintas métricas sobre el\n",
    "    rendimiento del modelo. Devuelve un diccionario\n",
    "    con los parámetros medidos\"\"\"\n",
    "    \n",
    "    return {\n",
    "        'Accuracy': np.round(\n",
    "                        metrics.accuracy_score(true_labels, \n",
    "                                               predicted_labels),\n",
    "                        3),\n",
    "        'Precision': np.round(\n",
    "                        metrics.precision_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted',\n",
    "                                               zero_division=0),\n",
    "                        3),\n",
    "    'Recall': np.round(\n",
    "                        metrics.recall_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted',\n",
    "                                               zero_division=0),\n",
    "                        3),\n",
    "    'F1 Score': np.round(\n",
    "                        metrics.f1_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted',\n",
    "                                               zero_division=0),\n",
    "                        3)}\n",
    "                        \n",
    "\n",
    "def train_predict_evaluate_model(classifier, \n",
    "                                 train_features, train_labels, \n",
    "                                 test_features, test_labels):\n",
    "    \"\"\"Función que entrena un modelo de clasificación sobre\n",
    "    un conjunto de entrenamiento, lo aplica sobre un conjunto\n",
    "    de test y devuelve la predicción sobre el conjunto de test\n",
    "    y las métricas de rendimiento\"\"\"\n",
    "    # genera modelo    \n",
    "    classifier.fit(train_features, train_labels)\n",
    "    # predice usando el modelo sobre test\n",
    "    predictions = classifier.predict(test_features) \n",
    "    # evalúa rendimiento de la predicción   \n",
    "    metricas = get_metrics(true_labels=test_labels, \n",
    "                predicted_labels=predictions)\n",
    "    return predictions, metricas     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68ccaa4",
   "metadata": {},
   "source": [
    "Vamos a entrenar sobre el conjunto de train y evaluamos en el conjunto de test. Guardamos métricas en una lista y resultados en otra para mostrar resumen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "e72512d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLR = LogisticRegression(solver='liblinear')\n",
    "modelNB = GaussianNB() #var_smoothing=1e-9\n",
    "modelSVM = SGDClassifier(loss='hinge', max_iter=1000)\n",
    "modelRBFSVM = SVC(gamma='scale', C=2)\n",
    "\n",
    "modelos = [('Logistic Regression', modelLR),\n",
    "           ('Naive Bayes', modelNB),\n",
    "           ('Linear SVM', modelSVM),\n",
    "           ('Gauss kernel SVM', modelRBFSVM)]\n",
    "\n",
    "metricas = []\n",
    "resultados = []\n",
    "\n",
    "# Modelos con características BoW\n",
    "bow_train_features_ = bow_train_features.toarray()\n",
    "bow_test_features_ = bow_test_features.toarray()\n",
    "for m, clf in modelos:\n",
    "    prediccion, metrica = train_predict_evaluate_model(classifier=clf,\n",
    "                                           train_features=bow_train_features_,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features_,\n",
    "                                           test_labels=test_labels)\n",
    "    metrica['modelo']=f'{m} BoW'\n",
    "    resultados.append(prediccion)\n",
    "    metricas.append(metrica)\n",
    "    \n",
    "# Modelos con características TF-IDF\n",
    "tfidf_train_features_ = tfidf_train_features.toarray()\n",
    "tfidf_test_features_ = tfidf_test_features.toarray()\n",
    "for m, clf in modelos:\n",
    "    prediccion, metrica = train_predict_evaluate_model(classifier=clf,\n",
    "                                           train_features=tfidf_train_features_,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features_,\n",
    "                                           test_labels=test_labels)\n",
    "    metrica['modelo']=f'{m} tfidf'\n",
    "    resultados.append(prediccion)\n",
    "    metricas.append(metrica)\n",
    "\n",
    "# Modelos con características averaged word vectors\n",
    "avg_wv_train_features_ = avg_wv_train_features\n",
    "avg_wv_test_features_ = avg_wv_test_features\n",
    "for m, clf in modelos:\n",
    "    prediccion, metrica = train_predict_evaluate_model(classifier=clf,\n",
    "                                           train_features=avg_wv_train_features_,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=avg_wv_test_features_,\n",
    "                                           test_labels=test_labels)\n",
    "    metrica['modelo']=f'{m} averaged'\n",
    "    resultados.append(prediccion)\n",
    "    metricas.append(metrica)\n",
    "\n",
    "# Modelos con características tfidf weighted averaged word vectors\n",
    "tfidf_wv_train_features_ = tfidf_wv_train_features\n",
    "tfidf_wv_test_features_ = tfidf_wv_test_features\n",
    "for m, clf in modelos:\n",
    "    prediccion, metrica = train_predict_evaluate_model(classifier=clf,\n",
    "                                           train_features=tfidf_wv_train_features_,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_wv_test_features_,\n",
    "                                           test_labels=test_labels)\n",
    "    metrica['modelo']=f'{m} tfidf wv'\n",
    "    resultados.append(prediccion)\n",
    "    metricas.append(metrica)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b9d3ed",
   "metadata": {},
   "source": [
    "Conviertimos la lista de métricas en un DataFrame para observar sus valores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "7f451f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>modelo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.458</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.453</td>\n",
       "      <td>Logistic Regression BoW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.458</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.454</td>\n",
       "      <td>Naive Bayes BoW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.433</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.432</td>\n",
       "      <td>Linear SVM BoW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.342</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.312</td>\n",
       "      <td>Gauss kernel SVM BoW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.467</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.465</td>\n",
       "      <td>Logistic Regression tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.417</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.417</td>\n",
       "      <td>Naive Bayes tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.450</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.426</td>\n",
       "      <td>Linear SVM tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.467</td>\n",
       "      <td>0.471</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.466</td>\n",
       "      <td>Gauss kernel SVM tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.875</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.877</td>\n",
       "      <td>Logistic Regression averaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.850</td>\n",
       "      <td>0.865</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.854</td>\n",
       "      <td>Naive Bayes averaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.883</td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.883</td>\n",
       "      <td>0.882</td>\n",
       "      <td>Linear SVM averaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.842</td>\n",
       "      <td>0.851</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.845</td>\n",
       "      <td>Gauss kernel SVM averaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.450</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.449</td>\n",
       "      <td>Logistic Regression tfidf wv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.375</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.379</td>\n",
       "      <td>Naive Bayes tfidf wv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.400</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.387</td>\n",
       "      <td>Linear SVM tfidf wv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.425</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.429</td>\n",
       "      <td>Gauss kernel SVM tfidf wv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Accuracy  Precision  Recall  F1 Score                        modelo\n",
       "0      0.458      0.490   0.458     0.453       Logistic Regression BoW\n",
       "1      0.458      0.530   0.458     0.454               Naive Bayes BoW\n",
       "2      0.433      0.461   0.433     0.432                Linear SVM BoW\n",
       "3      0.342      0.450   0.342     0.312          Gauss kernel SVM BoW\n",
       "4      0.467      0.494   0.467     0.465     Logistic Regression tfidf\n",
       "5      0.417      0.468   0.417     0.417             Naive Bayes tfidf\n",
       "6      0.450      0.522   0.450     0.426              Linear SVM tfidf\n",
       "7      0.467      0.471   0.467     0.466        Gauss kernel SVM tfidf\n",
       "8      0.875      0.882   0.875     0.877  Logistic Regression averaged\n",
       "9      0.850      0.865   0.850     0.854          Naive Bayes averaged\n",
       "10     0.883      0.893   0.883     0.882           Linear SVM averaged\n",
       "11     0.842      0.851   0.842     0.845     Gauss kernel SVM averaged\n",
       "12     0.450      0.494   0.450     0.449  Logistic Regression tfidf wv\n",
       "13     0.375      0.415   0.375     0.379          Naive Bayes tfidf wv\n",
       "14     0.400      0.415   0.400     0.387           Linear SVM tfidf wv\n",
       "15     0.425      0.480   0.425     0.429     Gauss kernel SVM tfidf wv"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metricas = pd.DataFrame(metricas)\n",
    "metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "eb74a885",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediccion_r=\"../Prediccion\"\n",
    "metricas.to_csv(prediccion_r+'/metricas.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d74d64",
   "metadata": {},
   "source": [
    "Ordenamos las métricas por `accuracy` y muestra el mejor resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "a29f3f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>modelo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.883</td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.883</td>\n",
       "      <td>0.882</td>\n",
       "      <td>Linear SVM averaged</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Accuracy  Precision  Recall  F1 Score               modelo\n",
       "10     0.883      0.893   0.883     0.882  Linear SVM averaged"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metricas = metricas.sort_values('Accuracy',ascending=False)\n",
    "metricas.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db59f31d",
   "metadata": {},
   "source": [
    "Mejoramos el `accuracy` a partir del juego de parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "bf594a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM\n",
      "averaged\n"
     ]
    }
   ],
   "source": [
    "modelos = metricas['modelo'].tolist()\n",
    "\n",
    "mejor = modelos[0]\n",
    "sep = mejor.split(' ')\n",
    "\n",
    "\n",
    "\n",
    "if sep[len(sep)-1]== \"wv\":\n",
    "    mo = ' '.join(sep[:len(sep)-2])\n",
    "    \n",
    "else:\n",
    "    mo = ' '.join(sep[:len(sep)-1])\n",
    "\n",
    "print(mo)\n",
    "    \n",
    "\n",
    "datos = sep[len(sep)-1]\n",
    "print(datos)\n",
    "\n",
    "metricas2 = []\n",
    "resultados = []\n",
    "\n",
    "\n",
    "\n",
    "# DATOS\n",
    "if datos == 'Bow':\n",
    "    train_features_ = bow_train_features.toarray()\n",
    "    test_features_ = bow_test_features.toarray()\n",
    "    \n",
    "if datos == 'tfidf':\n",
    "    train_features_ = tfidf_train_features.toarray()\n",
    "    test_features_ = tfidf_test_features.toarray()\n",
    "    \n",
    "if datos == 'averaged':\n",
    "    train_features_ = avg_wv_train_features\n",
    "    test_features_ = avg_wv_test_features\n",
    "    \n",
    "else: # tfidf wv\n",
    "    train_features_ = tfidf_wv_train_features\n",
    "    test_features_ = tfidf_wv_test_features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# MODELOS\n",
    "\n",
    "if mo == 'Logistic Regression':\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "    sol = [\"lbfgs\", \"liblinear\", \"newton-cg\", \"newton-cholesky\", \"sag\", \"saga\"]\n",
    "    \n",
    "    for i in sol:\n",
    "        modelLR = LogisticRegression(solver=i)\n",
    "        prediccion, metrica = train_predict_evaluate_model(classifier=modelLR,\n",
    "                                            train_features=train_features_,\n",
    "                                            train_labels=train_labels,\n",
    "                                            test_features=test_features_,\n",
    "                                            test_labels=test_labels)\n",
    "        metrica['modelo']=f'{i} - Logistic Regression ' + datos\n",
    "        resultados.append(prediccion)\n",
    "        metricas2.append(metrica)\n",
    "    \n",
    "if mo == 'Naive Bayes':\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html\n",
    "    var_smo = [1e-1,1e-2,1e-3,1e-4,1e-5,1e-6,1e-7,1e-8,1e-9,1e-10,1e-11,1e-12,1e-13,1e-14,1e-15]\n",
    "    \n",
    "    for i in var_smo:\n",
    "        modelNB = GaussianNB(var_smoothing=i)\n",
    "        prediccion, metrica = train_predict_evaluate_model(classifier=modelLR,\n",
    "                                            train_features=train_features_,\n",
    "                                            train_labels=train_labels,\n",
    "                                            test_features=test_features_,\n",
    "                                            test_labels=test_labels)\n",
    "        metrica['modelo']=f'{i} - Naive Bayes ' + datos\n",
    "        resultados.append(prediccion)\n",
    "        metricas2.append(metrica)\n",
    "    \n",
    "if mo == 'Linear SVM':\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "    loss = [\"hinge\", \"log_loss\", \"log\", \"modified_huber\", \"squared_hinge\", \"perceptron\", \"squared_error\", \"huber\", \"epsilon_insensitive\", \"squared_epsilon_insensitive\"]\n",
    "\n",
    "    for i in loss:\n",
    "        modelSVM = SGDClassifier(loss=i, max_iter=1000)\n",
    "        prediccion, metrica = train_predict_evaluate_model(classifier=modelLR,\n",
    "                                            train_features=train_features_,\n",
    "                                            train_labels=train_labels,\n",
    "                                            test_features=test_features_,\n",
    "                                            test_labels=test_labels)\n",
    "        metrica['modelo']=f'{i} - Linear SVM ' + datos\n",
    "        resultados.append(prediccion)\n",
    "        metricas2.append(metrica)\n",
    "\n",
    "    \n",
    "else: # Gauss kernel SVM \n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "    gamma = [\"scale\", \"auto\"]\n",
    "\n",
    "    for i in gamma:\n",
    "        modelRBFSVM = SVC(gamma=i, C=2)\n",
    "        prediccion, metrica = train_predict_evaluate_model(classifier=modelLR,\n",
    "                                            train_features=train_features_,\n",
    "                                            train_labels=train_labels,\n",
    "                                            test_features=test_features_,\n",
    "                                            test_labels=test_labels)\n",
    "        metrica['modelo']=f'{i} - Gauss kernel SVM ' + datos\n",
    "        resultados.append(prediccion)\n",
    "        metricas2.append(metrica)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebb112d",
   "metadata": {},
   "source": [
    "Conviertimos la lista de métricas en un DataFrame para observar sus valores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "c76349cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>modelo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.875</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.877</td>\n",
       "      <td>hinge - Linear SVM averaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.875</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.877</td>\n",
       "      <td>log_loss - Linear SVM averaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.875</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.877</td>\n",
       "      <td>log - Linear SVM averaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.875</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.877</td>\n",
       "      <td>modified_huber - Linear SVM averaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.875</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.877</td>\n",
       "      <td>squared_hinge - Linear SVM averaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.875</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.877</td>\n",
       "      <td>perceptron - Linear SVM averaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.875</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.877</td>\n",
       "      <td>squared_error - Linear SVM averaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.875</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.877</td>\n",
       "      <td>huber - Linear SVM averaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.875</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.877</td>\n",
       "      <td>epsilon_insensitive - Linear SVM averaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.875</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.877</td>\n",
       "      <td>squared_epsilon_insensitive - Linear SVM averaged</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Precision  Recall  F1 Score  \\\n",
       "0     0.875      0.882   0.875     0.877   \n",
       "1     0.875      0.882   0.875     0.877   \n",
       "2     0.875      0.882   0.875     0.877   \n",
       "3     0.875      0.882   0.875     0.877   \n",
       "4     0.875      0.882   0.875     0.877   \n",
       "5     0.875      0.882   0.875     0.877   \n",
       "6     0.875      0.882   0.875     0.877   \n",
       "7     0.875      0.882   0.875     0.877   \n",
       "8     0.875      0.882   0.875     0.877   \n",
       "9     0.875      0.882   0.875     0.877   \n",
       "\n",
       "                                              modelo  \n",
       "0                        hinge - Linear SVM averaged  \n",
       "1                     log_loss - Linear SVM averaged  \n",
       "2                          log - Linear SVM averaged  \n",
       "3               modified_huber - Linear SVM averaged  \n",
       "4                squared_hinge - Linear SVM averaged  \n",
       "5                   perceptron - Linear SVM averaged  \n",
       "6                squared_error - Linear SVM averaged  \n",
       "7                        huber - Linear SVM averaged  \n",
       "8          epsilon_insensitive - Linear SVM averaged  \n",
       "9  squared_epsilon_insensitive - Linear SVM averaged  "
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metricas3 = pd.DataFrame(metricas2)\n",
    "metricas3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "3befe52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metricas3.to_csv(prediccion_r+'/metricas_mejora.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a82fc9",
   "metadata": {},
   "source": [
    "Ordenamos las métricas por `accuracy` y muestra el mejor resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "71574182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>modelo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.875</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.877</td>\n",
       "      <td>hinge - Linear SVM averaged</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Precision  Recall  F1 Score                       modelo\n",
       "0     0.875      0.882   0.875     0.877  hinge - Linear SVM averaged"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metricas3 = metricas3.sort_values('Accuracy',ascending=False)\n",
    "metricas3.head(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ace99cf",
   "metadata": {},
   "source": [
    "# 6. PRODUCCIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "fae42563",
   "metadata": {},
   "outputs": [],
   "source": [
    "deportes_vocabulario_train = voca(deportes_dic)\n",
    "deportes_vocabulario_test = voca(deportes_dic_t)\n",
    "\n",
    "# bow\n",
    "deportes_bow_vectorizer = CountVectorizer(min_df=0.01, max_df=0.9, vocabulary=deportes_vocabulario_train)\n",
    "# deportes_bow_vectorizer_t = CountVectorizer(min_df=0.01, max_df=0.9, vocabulary=deportes_vocabulario_test)\n",
    "\n",
    "# tf-idf\n",
    "deportes_tfidf_vectorizer = TfidfTransformer()\n",
    "\n",
    "# características bag of words\n",
    "bow_train_features = deportes_bow_vectorizer.fit_transform(norm_train_corpus)  \n",
    "bow_test_features = deportes_bow_vectorizer.transform(norm_test_corpus) \n",
    "\n",
    "# características tfidf (a partir del BoW)\n",
    "tfidf_train_features = deportes_tfidf_vectorizer.fit_transform(bow_train_features)\n",
    "tfidf_test_features = deportes_tfidf_vectorizer.transform(bow_test_features)    \n",
    "\n",
    "# características averaged word vector\n",
    "avg_wv_train_features = averaged_word_vectorizer(norm_train_corpus)                \n",
    "avg_wv_test_features = averaged_word_vectorizer(norm_test_corpus)      \n",
    "\n",
    "# características tfidf weighted averaged word vector\n",
    "word_tfidf_map = {key:value for (key, value) in zip(deportes_bow_vectorizer.get_feature_names_out(), deportes_tfidf_vectorizer.idf_)}\n",
    "\n",
    "tfidf_wv_train_features = tfidf_weighted_averaged_word_vectorizer(norm_train_corpus, word_tfidf_map)\n",
    "\n",
    "tfidf_wv_test_features = tfidf_weighted_averaged_word_vectorizer(norm_test_corpus, word_tfidf_map) \n",
    "\n",
    "\"deportes\", \"salud\", \"ciencia\", \"politica\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3edd1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "deportes_vocabulario_train = voca(deportes_dic)\n",
    "deportes_vocabulario_test = voca(deportes_dic_t)\n",
    "\n",
    "# bow\n",
    "deportes_bow_vectorizer = CountVectorizer(min_df=0.01, max_df=0.9, vocabulary=deportes_vocabulario_train)\n",
    "# deportes_bow_vectorizer_t = CountVectorizer(min_df=0.01, max_df=0.9, vocabulary=deportes_vocabulario_test)\n",
    "\n",
    "# tf-idf\n",
    "deportes_tfidf_vectorizer = TfidfTransformer()\n",
    "\n",
    "# características bag of words\n",
    "bow_train_features = deportes_bow_vectorizer.fit_transform(norm_train_corpus)  \n",
    "bow_test_features = deportes_bow_vectorizer.transform(norm_test_corpus) \n",
    "\n",
    "# características tfidf (a partir del BoW)\n",
    "tfidf_train_features = deportes_tfidf_vectorizer.fit_transform(bow_train_features)\n",
    "tfidf_test_features = deportes_tfidf_vectorizer.transform(bow_test_features)    \n",
    "\n",
    "# características averaged word vector\n",
    "avg_wv_train_features = averaged_word_vectorizer(norm_train_corpus)                \n",
    "avg_wv_test_features = averaged_word_vectorizer(norm_test_corpus)      \n",
    "\n",
    "# características tfidf weighted averaged word vector\n",
    "word_tfidf_map = {key:value for (key, value) in zip(deportes_bow_vectorizer.get_feature_names_out(), salud_tfidf_vectorizer.idf_)}\n",
    "\n",
    "tfidf_wv_train_features = tfidf_weighted_averaged_word_vectorizer(norm_train_corpus, word_tfidf_map)\n",
    "\n",
    "tfidf_wv_test_features = tfidf_weighted_averaged_word_vectorizer(norm_test_corpus, word_tfidf_map) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f3f10f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d81a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Clasificacion_3915",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "a821d265b19b8e474c372894184ad502aefb1f7882947607cd0ea5f074b097d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
