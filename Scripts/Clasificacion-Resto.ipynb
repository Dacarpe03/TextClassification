{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "11239500",
   "metadata": {},
   "source": [
    "## Clasificación TODOS LOS MODELOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d823c7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.utils import simple_preprocess\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import spacy\n",
    "import unicodedata\n",
    "import os\n",
    "#from spellchecker import SpellChecker \n",
    "#from textblob import TextBlob \n",
    "#import contractions\n",
    "import re\n",
    "import random\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb78b1e",
   "metadata": {},
   "source": [
    "Lo primero que necesitamos es cargar nuestros glosarios de términos para crear nuestro diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "074e9305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_glosario(categoria, conjunto):\n",
    "    fname = f\"../Datos/Glosarios/{conjunto}/glosario_{categoria}.txt\"\n",
    "\n",
    "    glosario = []\n",
    "    with open(fname, 'r') as f:\n",
    "        glosario = [termino.rstrip('\\n') for termino in f.readlines()]\n",
    "    return glosario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99a6e637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ciencia': ['vacao',\n",
      "             'llama',\n",
      "             'captura',\n",
      "             'cola',\n",
      "             'honor',\n",
      "             'gemanidas',\n",
      "             'banyoles',\n",
      "             'neandertal',\n",
      "             'agujero',\n",
      "             'latigo',\n",
      "             'supersa',\n",
      "             'perfecto',\n",
      "             'regalo',\n",
      "             'bankman',\n",
      "             'fried',\n",
      "             'navidad',\n",
      "             'estampido',\n",
      "             'ftx',\n",
      "             'york',\n",
      "             'menta',\n",
      "             'supermasivo',\n",
      "             'barrera',\n",
      "             'segundo',\n",
      "             'ingeniero',\n",
      "             'magic',\n",
      "             'reencuentro',\n",
      "             'leo',\n",
      "             'nif',\n",
      "             'cernan',\n",
      "             'fauci'],\n",
      " 'deportes': ['falso',\n",
      "              'jamas',\n",
      "              'smash',\n",
      "              'mbappe',\n",
      "              'gigante',\n",
      "              'reserva',\n",
      "              'nets',\n",
      "              'carpena',\n",
      "              'djokovic',\n",
      "              'mclaren',\n",
      "              'exencion',\n",
      "              'butler',\n",
      "              'cuento',\n",
      "              'formato',\n",
      "              'campazzo',\n",
      "              'tenerife',\n",
      "              'domenicali',\n",
      "              'booker',\n",
      "              'tatum',\n",
      "              'madrid',\n",
      "              'real',\n",
      "              'premio',\n",
      "              'juventus',\n",
      "              'pagar',\n",
      "              'ronaldo',\n",
      "              'magnussen',\n",
      "              'steiner',\n",
      "              'gavi',\n",
      "              'porra',\n",
      "              'indycar',\n",
      "              'palou',\n",
      "              'gonzalez',\n",
      "              'gonzalo',\n",
      "              'rotacion',\n",
      "              'florentino',\n",
      "              'heredero',\n",
      "              'verdasco',\n",
      "              'enrique',\n",
      "              'luis',\n",
      "              'cristiano',\n",
      "              'cd',\n",
      "              'marbella',\n",
      "              'residencia',\n",
      "              'serbia',\n",
      "              'serbio',\n",
      "              'rosa',\n",
      "              'collins',\n",
      "              'jimmy',\n",
      "              'arabia',\n",
      "              'club'],\n",
      " 'politica': ['rosell',\n",
      "              'vuelo',\n",
      "              'sentir',\n",
      "              'reyes',\n",
      "              'rufian',\n",
      "              'unilateral',\n",
      "              'multilateral',\n",
      "              'ja',\n",
      "              'sinema',\n",
      "              'benito',\n",
      "              'juarez',\n",
      "              'turismo',\n",
      "              'ucraniano',\n",
      "              'nicolas',\n",
      "              'negativo',\n",
      "              'autoritariaa',\n",
      "              'ruido',\n",
      "              'nero',\n",
      "              'registral',\n",
      "              'demarcacia',\n",
      "              'ex',\n",
      "              'consulta',\n",
      "              'patrimonio',\n",
      "              'perao',\n",
      "              'juventud',\n",
      "              'trias',\n",
      "              'dema',\n",
      "              'coraza',\n",
      "              'ciudadano',\n",
      "              'anunciara'],\n",
      " 'salud': ['arterial',\n",
      "           'dash',\n",
      "           'anemia',\n",
      "           'folcodina',\n",
      "           'cafa',\n",
      "           'genital',\n",
      "           'vrs',\n",
      "           'unicef',\n",
      "           'lobo',\n",
      "           'verruga',\n",
      "           'aceite',\n",
      "           'mascarilla',\n",
      "           'pulmonar',\n",
      "           'presion',\n",
      "           'cannabis',\n",
      "           'pet',\n",
      "           'epilepsia',\n",
      "           'zumo',\n",
      "           'quiraorgica',\n",
      "           'afiliado',\n",
      "           'eps',\n",
      "           'ips',\n",
      "           'tc',\n",
      "           'creatividad',\n",
      "           'condiloma',\n",
      "           'marfan',\n",
      "           'manada',\n",
      "           'parasito',\n",
      "           'lavar',\n",
      "           'coosalud']}\n"
     ]
    }
   ],
   "source": [
    "categorias = [\"deportes\", \"salud\", \"ciencia\", \"politica\"]\n",
    "\n",
    "glosarios = {}\n",
    "for categoria in categorias:\n",
    "    glosarios[categoria] = cargar_glosario(categoria, \"train\")\n",
    "\n",
    "pprint(glosarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfdcfd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ciencia': ['sal',\n",
      "             'cuantico',\n",
      "             'idioma',\n",
      "             'fraa',\n",
      "             'vacuna',\n",
      "             'isidro',\n",
      "             'santo',\n",
      "             'llnl',\n",
      "             'agujero',\n",
      "             'sonido',\n",
      "             'antageno',\n",
      "             'subtipo',\n",
      "             'quipus',\n",
      "             'congelacia',\n",
      "             'ska',\n",
      "             'ofensivo',\n",
      "             'palabrota',\n",
      "             'canaria',\n",
      "             'relatividad',\n",
      "             'estruendo',\n",
      "             'programador',\n",
      "             'telescopio',\n",
      "             'congelar',\n",
      "             'gripe',\n",
      "             'alineacia',\n",
      "             'almanaque',\n",
      "             'escarcha',\n",
      "             'helada',\n",
      "             'sevilla',\n",
      "             'alphacode'],\n",
      " 'deportes': ['seguidor',\n",
      "              'carlos',\n",
      "              'mans',\n",
      "              'warren',\n",
      "              'doncic',\n",
      "              'suarez',\n",
      "              'juancho',\n",
      "              'boston',\n",
      "              'estabilidad',\n",
      "              'horford',\n",
      "              'hernanga',\n",
      "              'mez',\n",
      "              'ferrero',\n",
      "              'enrique',\n",
      "              'grada',\n",
      "              'mirar',\n",
      "              'marko',\n",
      "              'wiggins',\n",
      "              'stakhovsky',\n",
      "              'raptors',\n",
      "              'booker',\n",
      "              'resistencia',\n",
      "              'pts',\n",
      "              'reb',\n",
      "              'colombia',\n",
      "              'garden',\n",
      "              'brooklyn',\n",
      "              'cancha',\n",
      "              'lebron',\n",
      "              'gasto',\n",
      "              'tristeza',\n",
      "              'celtics',\n",
      "              'gimenez',\n",
      "              'sancionar',\n",
      "              'siebert',\n",
      "              'ktm',\n",
      "              'moto',\n",
      "              'dolares',\n",
      "              'rez',\n",
      "              'golden',\n",
      "              'state',\n",
      "              'warriors',\n",
      "              'lesia',\n",
      "              'leclerc',\n",
      "              'ofensivo',\n",
      "              'suns',\n",
      "              'krack',\n",
      "              'james',\n",
      "              'luis',\n",
      "              'aerodinamico'],\n",
      " 'politica': ['silva',\n",
      "              'refera',\n",
      "              'ilacito',\n",
      "              'rodraguez',\n",
      "              'torra',\n",
      "              'insulto',\n",
      "              'urgente',\n",
      "              'esparza',\n",
      "              'mocia',\n",
      "              'animal',\n",
      "              'castillo',\n",
      "              'almeida',\n",
      "              'smith',\n",
      "              'upn',\n",
      "              'cheque',\n",
      "              'ayres',\n",
      "              'concejala',\n",
      "              'mexicano',\n",
      "              'catalunya',\n",
      "              'isabel',\n",
      "              'ndum',\n",
      "              'nduma',\n",
      "              'ahorro',\n",
      "              'nombramiento',\n",
      "              'obrador',\n",
      "              'adjudicacia',\n",
      "              'gandia',\n",
      "              'procesado',\n",
      "              'navarra',\n",
      "              'auxiliar'],\n",
      " 'salud': ['strep',\n",
      "           'estigma',\n",
      "           'secuela',\n",
      "           'automatico',\n",
      "           'orina',\n",
      "           'antidepresivo',\n",
      "           'aprovechar',\n",
      "           'nuez',\n",
      "           'entrenar',\n",
      "           'bacteria',\n",
      "           'fruto',\n",
      "           'seco',\n",
      "           'cabello',\n",
      "           'atras',\n",
      "           'sida',\n",
      "           'vih',\n",
      "           'esconder',\n",
      "           'temporada',\n",
      "           'receta',\n",
      "           'congelado',\n",
      "           'alga',\n",
      "           'wakame',\n",
      "           'rehabilitacia',\n",
      "           'adolescente',\n",
      "           'gotlib',\n",
      "           'depresion',\n",
      "           'bronquiolitis',\n",
      "           'cuadro',\n",
      "           'niaos',\n",
      "           'morder']}\n"
     ]
    }
   ],
   "source": [
    "glosarios_t = {}\n",
    "for categoria in categorias:\n",
    "    glosarios_t[categoria] = cargar_glosario(categoria, \"test\")\n",
    "\n",
    "pprint(glosarios_t)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d790efb0",
   "metadata": {},
   "source": [
    "Creamos nuestro diccionario de palabras en base a los terminos de todos los glosarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0a4cfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary(glosarios):\n",
    "    doc_tokens = [[termino for termino in glosario] for glosario in glosarios.values()]\n",
    "    dictionary = corpora.Dictionary(doc_tokens)\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9eae608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<140 unique tokens: ['arabia', 'booker', 'butler', 'campazzo', 'carpena']...>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Dictionary<139 unique tokens: ['aerodinamico', 'booker', 'boston', 'brooklyn', 'cancha']...>\n"
     ]
    }
   ],
   "source": [
    "glosarios_dict = create_dictionary(glosarios)\n",
    "print(glosarios_dict)\n",
    "\n",
    "print(\"-\"*100)\n",
    "\n",
    "glosarios_dict_t = create_dictionary(glosarios_t)\n",
    "print(glosarios_dict_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fccfd6f",
   "metadata": {},
   "source": [
    "Cargamos nuestras noticias de test y las convertimos a un bag of words utilizando nuestro diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1cfad72",
   "metadata": {},
   "outputs": [],
   "source": [
    "noticias_test_dataframe = pd.read_csv(\"../Datos/noticias_train.csv\")\n",
    "noticias_test_dataframe_t = pd.read_csv(\"../Datos/noticias_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91b58db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bag_of_words(docs_list, dictionary):\n",
    "    doc_tokens = [simple_preprocess(corpus) for corpus in docs_list]\n",
    "    docs_bow = [dictionary.doc2bow(doc) for doc in doc_tokens]\n",
    "    return docs_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325a645a",
   "metadata": {},
   "source": [
    "Ahora es momento de crear una bag of words para cada noticia en base a nuestro diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1026623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 3), (36, 4), (47, 3), (84, 1)]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[(0, 5), (23, 6), (32, 6), (77, 3)]\n"
     ]
    }
   ],
   "source": [
    "docs_bow = create_bag_of_words(noticias_test_dataframe[\"corpus\"].values, glosarios_dict)\n",
    "print(docs_bow[19])\n",
    "\n",
    "print(\"-\"*100)\n",
    "\n",
    "docs_bow_t = create_bag_of_words(noticias_test_dataframe_t[\"corpus\"].values, glosarios_dict_t)\n",
    "print(docs_bow_t[19])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f2faaee",
   "metadata": {},
   "source": [
    "Sacamos las caracteristicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "debd5901",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tf-idf\n",
    "tfidf_vectorizer = TfidfTransformer()\n",
    "\n",
    "#Funciones de WV.\n",
    "def averaged_word_vectorizer(corpus):\n",
    "    '''Aplica la función de cálculo del WE promedio a todos los\n",
    "    documentos del corpus (cada doc es una lista de tokens)'''\n",
    "    features = [nlp(doc).vector\n",
    "                    for doc in corpus]\n",
    "    return np.array(features)\n",
    "\n",
    "def tfidf_wtd_avg_word_vectors(doc, word_tfidf_map):\n",
    "    '''Aplica la función de cálculo del WE ponderado por TF-IDF\n",
    "    a un documento (como lista de tokens)'''\n",
    "    tokens = doc.split()\n",
    "\n",
    "    feature_vector = np.zeros((nlp.vocab.vectors_length,),dtype=\"float64\")\n",
    "    wts = 0.      \n",
    "    for word in tokens:\n",
    "        if nlp.vocab[word].has_vector and word_tfidf_map.get(word, 0): #sólo considera palabras conocidas\n",
    "            weighted_word_vector = word_tfidf_map[word] * nlp.vocab[word].vector\n",
    "            wts = wts + 1\n",
    "            feature_vector = np.add(feature_vector, weighted_word_vector)\n",
    "    if wts:\n",
    "        feature_vector = np.divide(feature_vector, wts)\n",
    "        \n",
    "    return feature_vector\n",
    "    \n",
    "def tfidf_weighted_averaged_word_vectorizer(corpus, word_tfidf_map):\n",
    "    '''Aplica la función de cálculo del WE ponderado por TF-IDF a todos los\n",
    "    documentos del corpus (cada doc es una lista de tokens)'''                                       \n",
    "    features = [tfidf_wtd_avg_word_vectors(doc, word_tfidf_map)\n",
    "                    for doc in corpus]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0ac59c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages/sklearn/utils/_array_api.py:185: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  array = numpy.asarray(array, order=order, dtype=dtype)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[list([(8, 1), (9, 4), (12, 1), (26, 3), (99, 2)])\n list([(31, 1), (105, 1), (133, 2)]) list([])\n list([(97, 2), (102, 1), (103, 1), (105, 1), (114, 2)])\n list([(14, 3), (18, 2), (23, 3), (45, 3)]) list([(93, 1), (105, 1)])\n list([(5, 4), (6, 1), (48, 6), (92, 1)]) list([(16, 5), (133, 1)])\n list([]) list([(19, 5), (20, 5), (72, 2), (133, 2)])\n list([(28, 6), (36, 1), (46, 6), (92, 1)]) list([(30, 5)]) list([])\n list([]) list([(42, 3), (133, 1)]) list([])\n list([(15, 3), (21, 3), (27, 2), (37, 2)]) list([(7, 3), (32, 6)])\n list([(3, 5), (6, 1), (27, 7), (37, 7), (118, 1)])\n list([(1, 3), (36, 4), (47, 3), (84, 1)])\n list([(22, 3), (31, 7), (34, 3), (38, 5)])\n list([(93, 1), (125, 1), (133, 1)])\n list([(6, 5), (8, 4), (25, 3), (33, 3), (40, 3)]) list([(0, 3)])\n list([(12, 6), (17, 4), (26, 6), (35, 4), (95, 1)])\n list([(2, 3), (24, 2), (109, 1), (139, 1)])\n list([(4, 3), (10, 3), (29, 2), (39, 2), (43, 2), (44, 2)])\n list([(11, 4), (105, 1)]) list([(13, 8), (49, 5), (133, 2)])\n list([(41, 3), (133, 5)]) list([(56, 8), (63, 15), (77, 13), (119, 1)])\n list([(37, 1), (73, 5)]) list([(19, 2), (27, 2), (114, 1), (129, 2)])\n list([(55, 20), (58, 15), (133, 1)]) list([]) list([(69, 5), (74, 4)])\n list([(76, 3)]) list([(54, 22)]) list([(50, 21), (133, 1)])\n list([(109, 1)]) list([]) list([(65, 3), (78, 5), (105, 1)])\n list([(19, 1)]) list([(12, 1), (133, 1)]) list([(62, 12)])\n list([(66, 7), (67, 4), (70, 4)]) list([(52, 7), (114, 1), (116, 1)])\n list([(123, 1)]) list([(14, 2), (79, 5), (102, 1), (109, 1), (123, 4)])\n list([(59, 6), (72, 1), (105, 1)]) list([(79, 14), (105, 1)]) list([])\n list([(51, 6), (57, 5), (61, 6), (64, 6), (114, 1)]) list([(123, 1)])\n list([(6, 1), (19, 1), (27, 2), (60, 26)])\n list([(68, 9), (93, 1), (125, 2)]) list([(53, 21), (72, 17)]) list([])\n list([(71, 6), (75, 5)]) list([(26, 1), (133, 3)])\n list([(80, 2), (105, 7), (116, 1)])\n list([(14, 1), (18, 1), (33, 6), (37, 1), (132, 1), (139, 1)])\n list([(85, 1)]) list([(27, 1)]) list([(93, 3)]) list([(124, 1)])\n list([(88, 6), (116, 3)]) list([(37, 1), (114, 2), (119, 2), (133, 1)])\n list([(81, 7), (89, 7), (90, 6), (109, 6)]) list([(26, 1)])\n list([(91, 5)]) list([(37, 1), (83, 10), (119, 1)])\n list([(33, 1), (38, 1), (43, 1), (95, 3), (137, 1)])\n list([(72, 1), (101, 1), (110, 1)])\n list([(26, 1), (82, 6), (98, 4), (100, 6)]) list([(18, 1)])\n list([(108, 27)]) list([(27, 1), (101, 5), (110, 1)])\n list([(113, 1), (116, 1)]) list([(84, 5), (86, 1)])\n list([(14, 1), (85, 6)]) list([(27, 2), (136, 1)])\n list([(18, 2), (67, 1)]) list([(36, 1), (96, 11), (99, 1)])\n list([(23, 1), (92, 8), (97, 4), (99, 10), (102, 6), (103, 4), (104, 6)])\n list([(18, 1), (86, 12), (87, 4), (94, 5), (107, 5)]) list([(110, 1)])\n list([]) list([(18, 2), (80, 21), (95, 10), (106, 6)]) list([])\n list([(26, 1), (27, 1), (83, 2), (132, 1), (133, 1), (137, 11)])\n list([(75, 4), (111, 4), (127, 1)]) list([(0, 2), (37, 3), (116, 2)])\n list([(72, 1)]) list([(33, 1), (96, 1)]) list([(26, 1)])\n list([(19, 1), (27, 3), (39, 1), (54, 1), (115, 1), (133, 1)])\n list([(33, 1)]) list([(14, 1), (27, 4), (37, 1), (125, 5)])\n list([(27, 1), (118, 3), (135, 4)]) list([(27, 2), (33, 1), (96, 2)])\n list([(27, 2), (29, 1), (102, 1)]) list([(113, 1), (119, 4), (121, 3)])\n list([(26, 1), (112, 6), (117, 5), (120, 6)])\n list([(14, 1), (72, 1), (116, 15), (134, 11)])\n list([(99, 1), (110, 2), (118, 14), (129, 2), (130, 5), (132, 2), (135, 13)])\n list([(114, 2), (131, 7)]) list([(105, 1)]) list([(26, 1)])\n list([(110, 2), (113, 2), (115, 2), (129, 3), (130, 5)])\n list([(124, 1), (127, 8)]) list([(75, 1), (132, 1)])\n list([(12, 1), (27, 1), (127, 8), (136, 4), (139, 8)]) list([])\n list([(138, 7)]) list([(27, 1), (33, 1), (114, 1)])\n list([(14, 3), (126, 3)]) list([(123, 2), (132, 5), (133, 6)])\n list([(114, 5), (122, 3), (131, 5), (138, 5)]) list([(124, 7), (128, 4)])].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m bow_test_features \u001b[39m=\u001b[39m docs_bow_t\n\u001b[1;32m      7\u001b[0m \u001b[39m# características tfidf (a partir del BoW)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m tfidf_train_features \u001b[39m=\u001b[39m tfidf_vectorizer\u001b[39m.\u001b[39;49mfit_transform(bow_train_features)\n\u001b[1;32m      9\u001b[0m tfidf_test_features \u001b[39m=\u001b[39m tfidf_vectorizer\u001b[39m.\u001b[39mtransform(bow_test_features)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages/sklearn/utils/_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 142\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    144\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    145\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    147\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    148\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages/sklearn/base.py:848\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    845\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[1;32m    846\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    847\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 848\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[1;32m    849\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    851\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1653\u001b[0m, in \u001b[0;36mTfidfTransformer.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1650\u001b[0m \u001b[39m# large sparse data is not supported for 32bit platforms because\u001b[39;00m\n\u001b[1;32m   1651\u001b[0m \u001b[39m# _document_frequency uses np.bincount which works on arrays of\u001b[39;00m\n\u001b[1;32m   1652\u001b[0m \u001b[39m# dtype NPY_INTP which is int32 for 32bit platforms. See #20923\u001b[39;00m\n\u001b[0;32m-> 1653\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m   1654\u001b[0m     X, accept_sparse\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m), accept_large_sparse\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m _IS_32BIT\n\u001b[1;32m   1655\u001b[0m )\n\u001b[1;32m   1656\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sp\u001b[39m.\u001b[39missparse(X):\n\u001b[1;32m   1657\u001b[0m     X \u001b[39m=\u001b[39m sp\u001b[39m.\u001b[39mcsr_matrix(X)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages/sklearn/base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    534\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 535\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    536\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[1;32m    537\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Clasificacion_3915/lib/python3.9/site-packages/sklearn/utils/validation.py:900\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[39m# If input is 1D raise error\u001b[39;00m\n\u001b[1;32m    899\u001b[0m     \u001b[39mif\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 900\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    901\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExpected 2D array, got 1D array instead:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39marray=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    902\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    903\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    904\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mif it contains a single sample.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[1;32m    905\u001b[0m         )\n\u001b[1;32m    907\u001b[0m \u001b[39mif\u001b[39;00m dtype_numeric \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39min\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mUSV\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    908\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    909\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdtype=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnumeric\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    910\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    911\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[list([(8, 1), (9, 4), (12, 1), (26, 3), (99, 2)])\n list([(31, 1), (105, 1), (133, 2)]) list([])\n list([(97, 2), (102, 1), (103, 1), (105, 1), (114, 2)])\n list([(14, 3), (18, 2), (23, 3), (45, 3)]) list([(93, 1), (105, 1)])\n list([(5, 4), (6, 1), (48, 6), (92, 1)]) list([(16, 5), (133, 1)])\n list([]) list([(19, 5), (20, 5), (72, 2), (133, 2)])\n list([(28, 6), (36, 1), (46, 6), (92, 1)]) list([(30, 5)]) list([])\n list([]) list([(42, 3), (133, 1)]) list([])\n list([(15, 3), (21, 3), (27, 2), (37, 2)]) list([(7, 3), (32, 6)])\n list([(3, 5), (6, 1), (27, 7), (37, 7), (118, 1)])\n list([(1, 3), (36, 4), (47, 3), (84, 1)])\n list([(22, 3), (31, 7), (34, 3), (38, 5)])\n list([(93, 1), (125, 1), (133, 1)])\n list([(6, 5), (8, 4), (25, 3), (33, 3), (40, 3)]) list([(0, 3)])\n list([(12, 6), (17, 4), (26, 6), (35, 4), (95, 1)])\n list([(2, 3), (24, 2), (109, 1), (139, 1)])\n list([(4, 3), (10, 3), (29, 2), (39, 2), (43, 2), (44, 2)])\n list([(11, 4), (105, 1)]) list([(13, 8), (49, 5), (133, 2)])\n list([(41, 3), (133, 5)]) list([(56, 8), (63, 15), (77, 13), (119, 1)])\n list([(37, 1), (73, 5)]) list([(19, 2), (27, 2), (114, 1), (129, 2)])\n list([(55, 20), (58, 15), (133, 1)]) list([]) list([(69, 5), (74, 4)])\n list([(76, 3)]) list([(54, 22)]) list([(50, 21), (133, 1)])\n list([(109, 1)]) list([]) list([(65, 3), (78, 5), (105, 1)])\n list([(19, 1)]) list([(12, 1), (133, 1)]) list([(62, 12)])\n list([(66, 7), (67, 4), (70, 4)]) list([(52, 7), (114, 1), (116, 1)])\n list([(123, 1)]) list([(14, 2), (79, 5), (102, 1), (109, 1), (123, 4)])\n list([(59, 6), (72, 1), (105, 1)]) list([(79, 14), (105, 1)]) list([])\n list([(51, 6), (57, 5), (61, 6), (64, 6), (114, 1)]) list([(123, 1)])\n list([(6, 1), (19, 1), (27, 2), (60, 26)])\n list([(68, 9), (93, 1), (125, 2)]) list([(53, 21), (72, 17)]) list([])\n list([(71, 6), (75, 5)]) list([(26, 1), (133, 3)])\n list([(80, 2), (105, 7), (116, 1)])\n list([(14, 1), (18, 1), (33, 6), (37, 1), (132, 1), (139, 1)])\n list([(85, 1)]) list([(27, 1)]) list([(93, 3)]) list([(124, 1)])\n list([(88, 6), (116, 3)]) list([(37, 1), (114, 2), (119, 2), (133, 1)])\n list([(81, 7), (89, 7), (90, 6), (109, 6)]) list([(26, 1)])\n list([(91, 5)]) list([(37, 1), (83, 10), (119, 1)])\n list([(33, 1), (38, 1), (43, 1), (95, 3), (137, 1)])\n list([(72, 1), (101, 1), (110, 1)])\n list([(26, 1), (82, 6), (98, 4), (100, 6)]) list([(18, 1)])\n list([(108, 27)]) list([(27, 1), (101, 5), (110, 1)])\n list([(113, 1), (116, 1)]) list([(84, 5), (86, 1)])\n list([(14, 1), (85, 6)]) list([(27, 2), (136, 1)])\n list([(18, 2), (67, 1)]) list([(36, 1), (96, 11), (99, 1)])\n list([(23, 1), (92, 8), (97, 4), (99, 10), (102, 6), (103, 4), (104, 6)])\n list([(18, 1), (86, 12), (87, 4), (94, 5), (107, 5)]) list([(110, 1)])\n list([]) list([(18, 2), (80, 21), (95, 10), (106, 6)]) list([])\n list([(26, 1), (27, 1), (83, 2), (132, 1), (133, 1), (137, 11)])\n list([(75, 4), (111, 4), (127, 1)]) list([(0, 2), (37, 3), (116, 2)])\n list([(72, 1)]) list([(33, 1), (96, 1)]) list([(26, 1)])\n list([(19, 1), (27, 3), (39, 1), (54, 1), (115, 1), (133, 1)])\n list([(33, 1)]) list([(14, 1), (27, 4), (37, 1), (125, 5)])\n list([(27, 1), (118, 3), (135, 4)]) list([(27, 2), (33, 1), (96, 2)])\n list([(27, 2), (29, 1), (102, 1)]) list([(113, 1), (119, 4), (121, 3)])\n list([(26, 1), (112, 6), (117, 5), (120, 6)])\n list([(14, 1), (72, 1), (116, 15), (134, 11)])\n list([(99, 1), (110, 2), (118, 14), (129, 2), (130, 5), (132, 2), (135, 13)])\n list([(114, 2), (131, 7)]) list([(105, 1)]) list([(26, 1)])\n list([(110, 2), (113, 2), (115, 2), (129, 3), (130, 5)])\n list([(124, 1), (127, 8)]) list([(75, 1), (132, 1)])\n list([(12, 1), (27, 1), (127, 8), (136, 4), (139, 8)]) list([])\n list([(138, 7)]) list([(27, 1), (33, 1), (114, 1)])\n list([(14, 3), (126, 3)]) list([(123, 2), (132, 5), (133, 6)])\n list([(114, 5), (122, 3), (131, 5), (138, 5)]) list([(124, 7), (128, 4)])].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# características bag of words\n",
    "bow_train_features = docs_bow\n",
    "bow_test_features = docs_bow_t\n",
    "\n",
    "\n",
    "# características tfidf (a partir del BoW)\n",
    "tfidf_train_features = tfidf_vectorizer.fit_transform(bow_train_features)\n",
    "tfidf_test_features = tfidf_vectorizer.transform(bow_test_features)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3d86bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Clasificacion_3915",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "a821d265b19b8e474c372894184ad502aefb1f7882947607cd0ea5f074b097d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
